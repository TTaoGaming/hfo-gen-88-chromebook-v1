{
  "query": "MediaPipe hand tracking identity stability patterns minimizing latency in hand identity assignment alternatives to bipartite matching for multi-hand tracking simple temporal hand tracking algorithm",
  "tavily": {
    "query": "MediaPipe hand tracking identity stability patterns minimizing latency in hand identity assignment alternatives to bipartite matching for multi-hand tracking simple temporal hand tracking algorithm",
    "follow_up_questions": null,
    "answer": null,
    "images": [],
    "results": [
      {
        "url": "https://www.emergentmind.com/topics/mediapipe-hands",
        "title": "MediaPipe Hands: Real-Time Tracking - Emergent Mind",
        "content": "MediaPipe Hands is an open-source, real-time hand and finger tracking solution originating from Google\u2019s MediaPipe framework, built to deliver precise, markerless multi-hand localization and articulated pose estimation in unconstrained scenarios. It is extensively used in computer vision, augmented reality, gesture-based interaction, sign language translation, and robotics. The system is distinguished by its lightweight pipeline architecture, combining fast palm detectors, regression-based keypoint estimators, and temporal filtering, suitable for large-scale deployment across mobile platforms.\n\n## 1. Architecture and Key Algorithms\n\nMediaPipe Hands employs a cascaded pipeline optimizing for both robustness and low-latency: [...] 2000 character limit reached\n\n# MediaPipe Hands: Real-Time Tracking\n\nUpdated 1 November 2025\n\n MediaPipe Hands is an open-source, real-time hand tracking system that uses a palm-centric detection pipeline and CNN-based landmark regression.\n The methodology integrates fast palm detection, regression-based keypoint estimation, and temporal filtering to ensure robust multi-hand tracking in dynamic environments.\n Practical applications include gesture control, AR/VR interactions, robotic teleoperation, and sign language translation on mobile and web platforms. [...] 3. Temporal Tracking and Filtering: For video-based processing, intersection-over-union (IoU) matching, Kalman filters, or exponential smoothing can be used to maintain ID continuity and suppress outliers during rapid articulation and occlusion. Block feature refinement, as in 3DCPN (Li et al., 2021), enhances discriminative robustness, although MediaPipe Hands typically opts for lightweight, residual-based filtering for speed.\n4. Multihand Pipeline: The system runs multiple palm/landmark pipelines in parallel, supporting up to sixteen hands per frame in theory, although practical device limitations often cap performance at two hands in real time.",
        "score": 0.609455,
        "raw_content": null
      },
      {
        "url": "https://arxiv.org/html/2512.14746v1",
        "title": "BlindSpot: Enabling Bystander-Controlled Privacy Signaling for ...",
        "content": "Hand Detection and Tracking. To recognize a dynamic gesture, the system must first establish a stable identity for a hand across multiple frames. The pipeline begins by processing the video stream with the MediaPipe Hand Landmarker [mediapipe\\_hand], which provides the 21 normalized 3D landmarks for each detected hand. These raw detections are then passed to our HandTracker component, which assigns a persistent integer ID to each hand by implementing a proximity-based matching algorithm. For each existing track, it calculates the Euclidean distance between its wrist landmark from the previous frame and the wrist landmarks of all new detections in the current frame. A match is confirmed if the distance is below an empirically determined threshold (maxDistance). New IDs are assigned to any",
        "score": 0.54521364,
        "raw_content": null
      },
      {
        "url": "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html",
        "title": "MediaPipe Hands - Read the Docs",
        "content": "## Solution APIs\u00b6\n\n### Configuration Options\u00b6\n\nNaming style and availability may differ slightly across platforms/languages.\n\n#### static\\_image\\_mode\u00b6\n\nIf set to `false`, the solution treats the input images as a video stream. It will try to detect hands in the first input images, and upon a successful detection further localizes the hand landmarks. In subsequent images, once all max\\_num\\_hands hands are detected and the corresponding hand landmarks are localized, it simply tracks those landmarks without invoking another detection until it loses track of any of the hands. This reduces latency and is ideal for processing video frames. If set to `true`, hand detection runs on every input image, ideal for processing a batch of static, possibly unrelated, images. Default to `false`. [...] #### min\\_tracking\\_confidence:\u00b6\n\nMinimum confidence value (`[0.0,1.0]`) from the landmark-tracking model for the hand landmarks to be considered tracked successfully, or otherwise hand detection will be invoked automatically on the next input image. Setting it to a higher value can increase robustness of the solution, at the expense of a higher latency. Ignored if static\\_image\\_mode is `true`, where hand detection simply runs on every image. Default to `0.5`.\n\n### Output\u00b6\n\nNaming style may differ slightly across platforms/languages.\n\n#### multi\\_hand\\_landmarks\u00b6 [...] #### max\\_num\\_hands\u00b6\n\nMaximum number of hands to detect. Default to `2`.\n\n#### model\\_complexity\u00b6\n\nComplexity of the hand landmark model: `0` or `1`. Landmark accuracy as well as inference latency generally go up with the model complexity. Default to `1`.\n\n#### min\\_detection\\_confidence\u00b6\n\nMinimum confidence value (`[0.0,1.0]`) from the hand detection model for the detection to be considered successful. Default to `0.5`.\n\n#### min\\_tracking\\_confidence:\u00b6",
        "score": 0.52962893,
        "raw_content": null
      },
      {
        "url": "https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/",
        "title": "On-Device, Real-Time Hand Tracking with MediaPipe",
        "content": "Future Directions   \n We plan to extend this technology with more robust and stable tracking, enlarge the amount of gestures we can reliably detect, and support dynamic gestures unfolding in time. We believe that publishing this technology can give an impulse to new creative ideas and applications by the members of the research and developer community at large. We are excited to see what you can build with it!   \nAcknowledgements [...] Our MediaPipe graph for hand tracking is shown below. The graph consists of two subgraphs\u2014one for hand detection and one for hand keypoints (i.e., landmark) computation. One key optimization MediaPipe provides is that the palm detector is only run as necessary (fairly infrequently), saving significant computation time. We achieve this by inferring the hand location in the subsequent video frames from the computed hand key points in the current frame, eliminating the need to run the palm detector over each frame. For robustness, the hand tracker model outputs an additional scalar capturing the confidence that a hand is present and reasonably aligned in the input crop. Only when the confidence falls below a certain threshold is the hand detection model reapplied to the whole frame. [...] |  |\n\n| The hand landmark model\u2019s output (REJECT\\_HAND\\_FLAG) controls when the hand detection model is triggered. This behavior is achieved by MediaPipe\u2019s powerful synchronization building blocks, resulting in high performance and optimal throughput of the ML pipeline. |",
        "score": 0.40740472,
        "raw_content": null
      },
      {
        "url": "https://arxiv.org/abs/2006.10214",
        "title": "MediaPipe Hands: On-device Real-time Hand Tracking - arXiv",
        "content": "|  |  |\n --- |\n| Comments: | 5 pages, 7 figures; CVPR Workshop on Computer Vision for Augmented and Virtual Reality, Seattle, WA, USA, 2020 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | arXiv:2006.10214 [cs.CV] |\n|  | (or  arXiv:2006.10214v1 [cs.CV] for this version) |\n|  |  arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Valentin Bazarevsky [view email]   \n [v1] Thu, 18 Jun 2020 00:19:13 UTC (2,262 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MediaPipe Hands: On-device Real-time Hand Tracking, by Fan Zhang and 6 other authors\n\n View PDF\n TeX Source\n\nview license\n\nCurrent browse context:\n\ncs.CV\n\n< prev\")    |    next >\")\n\nnew  |  recent  | 2020-06\n\nChange to browse by:\n\ncs\n\n### References & Citations [...] We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\n\n> cs > arXiv:2006.10214\n\n# Computer Science > Computer Vision and Pattern Recognition\n\narXiv:2006.10214 (cs)\n\n[Submitted on 18 Jun 2020]\n\n# Title:MediaPipe Hands: On-device Real-time Hand Tracking\n\nAuthors:Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George Sung, Chuo-Ling Chang, Matthias Grundmann\n\nView a PDF of the paper titled MediaPipe Hands: On-device Real-time Hand Tracking, by Fan Zhang and 6 other authors [...] View PDF\n> Abstract:We present a real-time on-device hand tracking pipeline that predicts hand skeleton from single RGB camera for AR/VR applications. The pipeline consists of two models: 1) a palm detector, 2) a hand landmark model. It's implemented via MediaPipe, a framework for building cross-platform ML solutions. The proposed model and pipeline architecture demonstrates real-time inference speed on mobile GPUs and high prediction quality. MediaPipe Hands is open sourced at this https URL.",
        "score": 0.34052956,
        "raw_content": null
      }
    ],
    "response_time": 1.73,
    "request_id": "34c40c84-675a-4e95-8c4c-336d1aaa5032"
  },
  "brave": {
    "query": {
      "original": "MediaPipe hand tracking identity stability patterns minimizing latency in hand identity assignment alternatives to bipartite matching for multi-hand tracking simple temporal hand tracking algorithm",
      "show_strict_warning": false,
      "is_navigational": false,
      "is_news_breaking": false,
      "spellcheck_off": true,
      "country": "us",
      "bad_results": false,
      "should_fallback": false,
      "postal_code": "",
      "city": "",
      "header_country": "",
      "more_results_available": true,
      "state": ""
    },
    "mixed": {
      "type": "mixed",
      "main": [
        {
          "type": "web",
          "index": 0,
          "all": false
        },
        {
          "type": "web",
          "index": 1,
          "all": false
        },
        {
          "type": "web",
          "index": 2,
          "all": false
        },
        {
          "type": "web",
          "index": 3,
          "all": false
        },
        {
          "type": "web",
          "index": 4,
          "all": false
        },
        {
          "type": "web",
          "index": 5,
          "all": false
        },
        {
          "type": "web",
          "index": 6,
          "all": false
        },
        {
          "type": "web",
          "index": 7,
          "all": false
        },
        {
          "type": "web",
          "index": 8,
          "all": false
        },
        {
          "type": "web",
          "index": 9,
          "all": false
        },
        {
          "type": "web",
          "index": 10,
          "all": false
        },
        {
          "type": "web",
          "index": 11,
          "all": false
        },
        {
          "type": "web",
          "index": 12,
          "all": false
        },
        {
          "type": "web",
          "index": 13,
          "all": false
        },
        {
          "type": "web",
          "index": 14,
          "all": false
        },
        {
          "type": "web",
          "index": 15,
          "all": false
        },
        {
          "type": "web",
          "index": 16,
          "all": false
        },
        {
          "type": "web",
          "index": 17,
          "all": false
        }
      ],
      "top": [],
      "side": []
    },
    "type": "search",
    "web": {
      "type": "search",
      "results": [
        {
          "title": "layout: forward target: https://developers.google.com/mediapipe/solutions/vision/hand_landmarker title: Hands parent: MediaPipe Legacy Solutions nav_order: 4 \u2014 MediaPipe v0.7.5 documentation",
          "url": "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html",
          "is_source_local": false,
          "is_source_both": false,
          "description": "Complexity of the hand landmark model: 0 or 1. Landmark accuracy as well as inference latency generally go up with the model complexity. Default to 1. Minimum confidence value ([0.0, 1.0]) from the hand detection model for the detection to be considered successful.",
          "profile": {
            "name": "MediaPipe",
            "url": "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html",
            "long_name": "mediapipe.readthedocs.io",
            "img": "https://imgs.search.brave.com/siwVUd3xwHy6eFN_ZEcFH24I0GMOr2jEBCS52tTwxfc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYzkwNDIzZDAx/Mzg0MTVmODVmMTEz/MDEyMTljOGE2N2Q5/ZDliODY2YzA2MDRk/MDhkYjZkMDI2OTlh/YjBiNjMwYi9tZWRp/YXBpcGUucmVhZHRo/ZWRvY3MuaW8v"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "mediapipe.readthedocs.io",
            "hostname": "mediapipe.readthedocs.io",
            "favicon": "https://imgs.search.brave.com/siwVUd3xwHy6eFN_ZEcFH24I0GMOr2jEBCS52tTwxfc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYzkwNDIzZDAx/Mzg0MTVmODVmMTEz/MDEyMTljOGE2N2Q5/ZDliODY2YzA2MDRk/MDhkYjZkMDI2OTlh/YjBiNjMwYi9tZWRp/YXBpcGUucmVhZHRo/ZWRvY3MuaW8v",
            "path": "\u203a en  \u203a latest  \u203a solutions  \u203a hands.html"
          }
        },
        {
          "title": "[2006.10214] MediaPipe Hands: On-device Real-time Hand Tracking",
          "url": "https://ar5iv.labs.arxiv.org/html/2006.10214",
          "is_source_local": false,
          "is_source_both": false,
          "description": "This behavior is achieved by MediaPipe\u2019s powerful synchronization building blocks, resulting in high performance and optimal throughput of the ML pipeline. Our hand tracking solution can readily be used in many applications such as gesture recognition and AR effects. On top of the predicted hand skeleton, we employ a simple algorithm to compute gestures, see Figure 6.",
          "page_age": "2024-03-02T11:16:50",
          "profile": {
            "name": "arXiv",
            "url": "https://ar5iv.labs.arxiv.org/html/2006.10214",
            "long_name": "ar5iv.labs.arxiv.org",
            "img": "https://imgs.search.brave.com/Ra63-x8TeB8HQ-I2dv-lRN_mIzQ3JQGTcfoprRnqdaI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGEzNGUyYWVl/NDk2ODQ0MzYwZjhk/ZGRkOGIyNWVkN2Qz/ZWZiMjQ4ZGI2YWQw/NjQ0M2Q5YWFhMTM4/MDljYjI0OS9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy8"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "ar5iv.labs.arxiv.org",
            "hostname": "ar5iv.labs.arxiv.org",
            "favicon": "https://imgs.search.brave.com/Ra63-x8TeB8HQ-I2dv-lRN_mIzQ3JQGTcfoprRnqdaI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGEzNGUyYWVl/NDk2ODQ0MzYwZjhk/ZGRkOGIyNWVkN2Qz/ZWZiMjQ4ZGI2YWQw/NjQ0M2Q5YWFhMTM4/MDljYjI0OS9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy8",
            "path": "\u203a html  \u203a 2006.10214"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/5Tsbn2z8zn03LO10F1g1N0_ns-eYv5CwATFUyJ5Qpdc/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy9hc3NldHMv/YXI1aXZfY2FyZC5w/bmc",
            "original": "https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png",
            "logo": false
          },
          "age": "March 2, 2024"
        },
        {
          "title": "On-Device, Real-Time Hand Tracking with MediaPipe",
          "url": "https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/",
          "is_source_local": false,
          "is_source_both": false,
          "description": "In addition, as palms are smaller objects, the non-maximum suppression algorithm works well even for two-hand self-occlusion cases, like handshakes. Moreover, palms can be modelled using square bounding boxes (anchors in ML terminology) ignoring other aspect ratios, and therefore reducing the number of anchors by a factor of 3-5. Second, an encoder-decoder feature extractor is used for bigger scene context awareness even for small objects (similar to the RetinaNet approach). Lastly, we minimize the focal loss during training to support a large amount of anchors resulting from the high scale variance.",
          "profile": {
            "name": "Google Research",
            "url": "https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/",
            "long_name": "research.google",
            "img": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "research.google",
            "hostname": "research.google",
            "favicon": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv",
            "path": "\u203a blog  \u203a on-device-real-time-hand-tracking-with-mediapipe"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/Ww80QDOXmgj8yDED_u8edBEb9uOb2U_NaHeT6DId_Io/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdG9y/YWdlLmdvb2dsZWFw/aXMuY29tL2d3ZWIt/cmVzZWFyY2gyMDIz/LW1lZGlhL2ltYWdl/cy9lM2VmMDk1Njhm/ZTZiMDlkMGI5Mzc3/MmQzZjAyZGJmMy1p/LndpZHRoLTgwMC5m/b3JtYXQtanBlZy5q/cGc",
            "original": "https://storage.googleapis.com/gweb-research2023-media/images/e3ef09568fe6b09d0b93772d3f02dbf3-i.width-800.format-jpeg.jpg",
            "logo": false
          }
        },
        {
          "title": "[2006.10214] MediaPipe Hands: On-device Real-time Hand Tracking",
          "url": "https://arxiv.org/abs/2006.10214",
          "is_source_local": false,
          "is_source_both": false,
          "description": "We present <strong>a real-time on-device hand tracking pipeline that predicts hand skeleton from single RGB camera for AR/VR applications</strong>. The pipeline consists of two models: 1) a palm detector, 2) a hand landmark model.",
          "page_age": "2020-06-18T00:00:00",
          "profile": {
            "name": "arXiv",
            "url": "https://arxiv.org/abs/2006.10214",
            "long_name": "arxiv.org",
            "img": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "arxiv.org",
            "hostname": "arxiv.org",
            "favicon": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv",
            "path": "\u203a abs  \u203a 2006.10214"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/iKdq2fkHaKJYPDvdK7fP7YT23ZvBXz_aBGM5hD9_jho/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcnhp/di5vcmcvc3RhdGlj/L2Jyb3dzZS8wLjMu/NC9pbWFnZXMvYXJ4/aXYtbG9nby1mYi5w/bmc",
            "original": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png",
            "logo": true
          },
          "age": "June 18, 2020"
        },
        {
          "title": "MediaPipe Hands: Real-Time Tracking",
          "url": "https://www.emergentmind.com/topics/mediapipe-hands",
          "is_source_local": false,
          "is_source_both": false,
          "description": "Temporal Tracking and Filtering: For video-based processing, <strong>intersection-over-union (IoU) matching, Kalman filters, or exponential smoothing</strong> can be used to maintain ID continuity and suppress outliers during rapid articulation and occlusion.",
          "profile": {
            "name": "Emergent Mind",
            "url": "https://www.emergentmind.com/topics/mediapipe-hands",
            "long_name": "emergentmind.com",
            "img": "https://imgs.search.brave.com/Zc521AqQwkFLu4dxf7NKn1ehs161lCdjAkhkjS4ySGI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGM4NjVhOTE1/YjRiZTJmNWRlOTZj/ZjRhZmYxZWMwMWFj/ZDAzNTk0Y2MxZDc1/MTBiYWJmODlmZTE1/N2EwMWUxZS93d3cu/ZW1lcmdlbnRtaW5k/LmNvbS8"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "emergentmind.com",
            "hostname": "www.emergentmind.com",
            "favicon": "https://imgs.search.brave.com/Zc521AqQwkFLu4dxf7NKn1ehs161lCdjAkhkjS4ySGI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGM4NjVhOTE1/YjRiZTJmNWRlOTZj/ZjRhZmYxZWMwMWFj/ZDAzNTk0Y2MxZDc1/MTBiYWJmODlmZTE1/N2EwMWUxZS93d3cu/ZW1lcmdlbnRtaW5k/LmNvbS8",
            "path": "\u203a topics  \u203a mediapipe-hands"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/K8jnmZPNdMeWoSa5EXkTOTXDUqE7xJulK0_vb-xBIyk/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9kMnpr/OHFkeDJ5MWJlci5j/bG91ZGZyb250Lm5l/dC9hc3NldHMvNjAw/cHgtYTYxMzIwNDQ5/YjhjODQ4YmNjNTZl/MDI4MjZiMDMzYjVl/ZmEwOWZhNTJjYzQ4/YjQ4N2U4ZjlhMmVi/OGVmZDcwNS5wbmc",
            "original": "https://d2zk8qdx2y1ber.cloudfront.net/assets/600px-a61320449b8c848bcc56e02826b033b5efa09fa52cc48b487e8f9a2eb8efd705.png",
            "logo": false
          }
        },
        {
          "title": "MediaPipe Hands: On-device Real-time Hand Tracking",
          "url": "https://www.researchgate.net/publication/342302340_MediaPipe_Hands_On-device_Real-time_Hand_Tracking",
          "is_source_local": false,
          "is_source_both": false,
          "description": "Regardless, outcomes provide evidence on the robustness and stability of PHCA against perturbations to data and noise. It can be concluded that PHCA can serve as an alternative for FSL recognition, offering opportunities for further research. ... [93]. Within the designated hand areas, the MediaPipe hand landmark model identifies 21 palm-knuckle keypoints, which are highlighted in Fig.",
          "page_age": "2020-06-17T00:00:00",
          "profile": {
            "name": "ResearchGate",
            "url": "https://www.researchgate.net/publication/342302340_MediaPipe_Hands_On-device_Real-time_Hand_Tracking",
            "long_name": "researchgate.net",
            "img": "https://imgs.search.brave.com/WJ25-tL-91vp8kSoF0YS7d8CZVivP1cG-EjlGvrmxOc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjE0MDk0Yzli/MWQ3NGQ2ZTNjODVh/YWVmMDNkM2NkZWI5/NGEzNjdhMmEyY2E2/N2EzN2Y0OThlZjdi/YWU3MjkwMi93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC8"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "researchgate.net",
            "hostname": "www.researchgate.net",
            "favicon": "https://imgs.search.brave.com/WJ25-tL-91vp8kSoF0YS7d8CZVivP1cG-EjlGvrmxOc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjE0MDk0Yzli/MWQ3NGQ2ZTNjODVh/YWVmMDNkM2NkZWI5/NGEzNjdhMmEyY2E2/N2EzN2Y0OThlZjdi/YWU3MjkwMi93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC8",
            "path": "\u203a publication  \u203a 342302340_MediaPipe_Hands_On-device_Real-time_Hand_Tracking"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/NA3UWQdok5rp6uI2kyF0cHLVZy0QBikNBsFv8yDMrrc/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC9pbWFnZXMv/dGVtcGxhdGUvZGVm/YXVsdF9wdWJsaWNh/dGlvbl9wcmV2aWV3/X2xhcmdlLnBuZw",
            "original": "https://www.researchgate.net/images/template/default_publication_preview_large.png",
            "logo": false
          },
          "age": "June 17, 2020"
        },
        {
          "title": "GitHub - geaxgx/depthai_hand_tracker: Running Google Mediapipe Hand Tracking models on Luxonis DepthAI hardware (OAK-D-lite, OAK-D, OAK-1,...)",
          "url": "https://github.com/geaxgx/depthai_hand_tracker",
          "is_source_local": false,
          "is_source_both": false,
          "description": "<strong>Hand Tracking from Mediapipe is a 2-stages pipeline</strong>. First, the Hand detection stage detects where are the hands in the whole image. For each detected hand, a Region of Interest (ROI) around the hand is calculated and fed to the second stage, ...",
          "profile": {
            "name": "GitHub",
            "url": "https://github.com/geaxgx/depthai_hand_tracker",
            "long_name": "github.com",
            "img": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "software",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "github.com",
            "hostname": "github.com",
            "favicon": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw",
            "path": "\u203a geaxgx  \u203a depthai_hand_tracker"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/DVvGW1EDkApeq8PakD8kp0LpWRiuMyrxa_it-fvVL5E/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9vcGVu/Z3JhcGguZ2l0aHVi/YXNzZXRzLmNvbS8w/ZTUwOGIyMDljZTdm/Zjg2MDk3ZDg5ODA2/YjgzMDU5MDRhMzhi/MzlhZDM3ODlhMDhi/MGQ2NWVkMjk3NTFh/NjM0L2dlYXhneC9k/ZXB0aGFpX2hhbmRf/dHJhY2tlcg",
            "original": "https://opengraph.githubassets.com/0e508b209ce7ff86097d89806b8305904a38b39ad3789a08b0d65ed29751a634/geaxgx/depthai_hand_tracker",
            "logo": false
          }
        },
        {
          "title": "[PDF] MediaPipe Hands: On-device Real-time Hand Tracking | Semantic Scholar",
          "url": "https://www.semanticscholar.org/paper/MediaPipe-Hands:-On-device-Real-time-Hand-Tracking-Zhang-Bazarevsky/84b19524609ad75f309be7f87bcea783e6ecd337",
          "is_source_local": false,
          "is_source_both": false,
          "description": "This paper presents a simple but powerful algorithm based on the MediaPipe Hands solution, a highly optimized neural network that processes the landmarks provided by MediaP Pipe using morphological and logical operators to obtain the masks that allow dynamic updating of the skin color model.Expand",
          "profile": {
            "name": "Semantic Scholar",
            "url": "https://www.semanticscholar.org/paper/MediaPipe-Hands:-On-device-Real-time-Hand-Tracking-Zhang-Bazarevsky/84b19524609ad75f309be7f87bcea783e6ecd337",
            "long_name": "semanticscholar.org",
            "img": "https://imgs.search.brave.com/K-sZ3Vgcj9LFjkcTUpjjK40LWCyX6WutMxHJiE7XYPI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGI3NTZiOWVj/YTE5N2MzYTNhYzA2/NWEyMzQwMzA5N2Yy/NGY3MjhhYTcxYmI2/YWQ3NjdjNDk5NGE2/NmUyYTdlYi93d3cu/c2VtYW50aWNzY2hv/bGFyLm9yZy8"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "semanticscholar.org",
            "hostname": "www.semanticscholar.org",
            "favicon": "https://imgs.search.brave.com/K-sZ3Vgcj9LFjkcTUpjjK40LWCyX6WutMxHJiE7XYPI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGI3NTZiOWVj/YTE5N2MzYTNhYzA2/NWEyMzQwMzA5N2Yy/NGY3MjhhYTcxYmI2/YWQ3NjdjNDk5NGE2/NmUyYTdlYi93d3cu/c2VtYW50aWNzY2hv/bGFyLm9yZy8",
            "path": "  \u203a papers  \u203a mediapipe hands: on-device real-time hand tracking"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/A3fktYElQA3Sms9BB_yLvcb2tzI-8lZyB7LvUT1A-us/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/c2VtYW50aWNzY2hv/bGFyLm9yZy9pbWcv/c2VtYW50aWNfc2No/b2xhcl9vZy5wbmc",
            "original": "https://www.semanticscholar.org/img/semantic_scholar_og.png",
            "logo": false
          }
        },
        {
          "title": "Hands - mediapipe",
          "url": "https://chuoling.github.io/mediapipe/solutions/hands.html",
          "is_source_local": false,
          "is_source_both": false,
          "description": "Complexity of the hand landmark model: 0 or 1. Landmark accuracy as well as inference latency generally go up with the model complexity. Default to 1. Minimum confidence value ([0.0, 1.0]) from the hand detection model for the detection to be considered successful.",
          "profile": {
            "name": "GitHub",
            "url": "https://chuoling.github.io/mediapipe/solutions/hands.html",
            "long_name": "chuoling.github.io",
            "img": "https://imgs.search.brave.com/UQsG855bt9JVRC7rU_bzffBainVrPq-eopHOMS-wk-A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGE4MmNlN2Ez/ODk4YTAyNmM2ZGNi/NGY4OGIwZjQ0Nzkz/NDhkNDdkOWMwYjNl/ZGU0MTQzZGIzYjll/NDY4OWVlZi9jaHVv/bGluZy5naXRodWIu/aW8v"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "chuoling.github.io",
            "hostname": "chuoling.github.io",
            "favicon": "https://imgs.search.brave.com/UQsG855bt9JVRC7rU_bzffBainVrPq-eopHOMS-wk-A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGE4MmNlN2Ez/ODk4YTAyNmM2ZGNi/NGY4OGIwZjQ0Nzkz/NDhkNDdkOWMwYjNl/ZGU0MTQzZGIzYjll/NDY4OWVlZi9jaHVv/bGluZy5naXRodWIu/aW8v",
            "path": "\u203a mediapipe  \u203a solutions  \u203a hands.html"
          }
        },
        {
          "title": "MediaPipe Hands: On-device Real-time Hand Tracking",
          "url": "https://research.google/pubs/mediapipe-hands-on-device-real-time-hand-tracking/",
          "is_source_local": false,
          "is_source_both": false,
          "description": "The proposed architecture demonstrates realtime inference speed on mobile GPUs and a high prediction quality. MediaPipe Hands is open-sourced at https://github.com/google/mediapipe.",
          "profile": {
            "name": "Google Research",
            "url": "https://research.google/pubs/mediapipe-hands-on-device-real-time-hand-tracking/",
            "long_name": "research.google",
            "img": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "research.google",
            "hostname": "research.google",
            "favicon": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv",
            "path": "\u203a pubs  \u203a mediapipe-hands-on-device-real-time-hand-tracking"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/xMSx6K-DNGKBD00GCYX2R_H6N036h6fg9HWvNmYR3vs/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdG9y/YWdlLmdvb2dsZWFw/aXMuY29tL2d3ZWIt/cmVzZWFyY2gyMDIz/LW1lZGlhL2ltYWdl/cy9PcGVuX0dyYXBo/LndpZHRoLTgwMC5m/b3JtYXQtanBlZy5q/cGc",
            "original": "https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg",
            "logo": false
          }
        },
        {
          "title": "mediapipe/docs/solutions/hands.md at master \u00b7 google-ai-edge/...",
          "url": "https://github.com/google/mediapipe/blob/master/docs/solutions/hands.md",
          "is_source_local": false,
          "is_source_both": false,
          "description": "In addition, in our pipeline the crops can also be generated based on the hand landmarks identified in the previous frame, and only when the landmark model could no longer identify hand presence is palm detection invoked to relocalize the hand. The pipeline is implemented as a MediaPipe graph that uses a hand landmark tracking subgraph from the hand landmark module, and renders using a dedicated hand renderer subgraph.",
          "profile": {
            "name": "GitHub",
            "url": "https://github.com/google/mediapipe/blob/master/docs/solutions/hands.md",
            "long_name": "github.com",
            "img": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "software",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "github.com",
            "hostname": "github.com",
            "favicon": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw",
            "path": "\u203a google  \u203a mediapipe  \u203a blob  \u203a master  \u203a docs  \u203a solutions  \u203a hands.md"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/688wcydayjjovsWi9Y1muRakISJExilZNCGGw0NS29o/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9vcGVu/Z3JhcGguZ2l0aHVi/YXNzZXRzLmNvbS8w/ZjdhOTg2MjU3M2Ni/NzFlOWQyZGIzY2Fm/NDU5MDc4NTUwODQ3/ZTEyZmRjYzI0OTgy/YTgyYmJlNmJmMGMz/MGVlL2dvb2dsZS1h/aS1lZGdlL21lZGlh/cGlwZQ",
            "original": "https://opengraph.githubassets.com/0f7a9862573cb71e9d2db3caf459078550847e12fdcc24982a82bbe6bf0c30ee/google-ai-edge/mediapipe",
            "logo": false
          }
        },
        {
          "title": "Hand Gesture Mapping Using MediaPipe Algorithm | SpringerLink",
          "url": "https://link.springer.com/chapter/10.1007/978-981-16-8862-1_39",
          "is_source_local": false,
          "is_source_both": false,
          "description": "Once the user has completed the setup of his or her gestures, he or she can request the Python code that was created for that configuration. This paper provides the ability to pick any one of the workstation GUI tasks and assign motions to it, as well as the reverse of this functionality.",
          "profile": {
            "name": "Springer",
            "url": "https://link.springer.com/chapter/10.1007/978-981-16-8862-1_39",
            "long_name": "link.springer.com",
            "img": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "link.springer.com",
            "hostname": "link.springer.com",
            "favicon": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8",
            "path": "  \u203a home  \u203a proceedings of third international conference on communication, computing and electronics systems  \u203a conference paper"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/rdKOmGJHkXAowWNI03n9wxbAwO3VkWZPCeHc1c-kLm8/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9tZWRp/YS5zcHJpbmdlcm5h/dHVyZS5jb20vdzE1/My9zcHJpbmdlci1z/dGF0aWMvY292ZXIv/Ym9vay85NzgtOTgx/LTE2LTg4NjItMS5q/cGc",
            "original": "https://media.springernature.com/w153/springer-static/cover/book/978-981-16-8862-1.jpg",
            "logo": false
          }
        },
        {
          "title": "MediaPipe-Hand-Detection - Qualcomm AI Hub",
          "url": "https://aihub.qualcomm.com/models/mediapipe_hand",
          "is_source_local": false,
          "is_source_both": false,
          "description": "Real-time hand detection optimized for mobile and edge.",
          "profile": {
            "name": "Qualcomm\u00ae AI Hub",
            "url": "https://aihub.qualcomm.com/models/mediapipe_hand",
            "long_name": "aihub.qualcomm.com",
            "img": "https://imgs.search.brave.com/UTpvhPmpFihRjdmffoVQ1hNbVD9RVkCUpBjgRUHvZ8M/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvM2NkNTI4NGE5/MjhlN2JiZTY0NDUy/ZGY1M2RiNzEwN2Q3/MDJmM2EzMGVlMjJl/NzM3NWU4ZmRjNWNk/OTViZTQwOS9haWh1/Yi5xdWFsY29tbS5j/b20v"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "aihub.qualcomm.com",
            "hostname": "aihub.qualcomm.com",
            "favicon": "https://imgs.search.brave.com/UTpvhPmpFihRjdmffoVQ1hNbVD9RVkCUpBjgRUHvZ8M/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvM2NkNTI4NGE5/MjhlN2JiZTY0NDUy/ZGY1M2RiNzEwN2Q3/MDJmM2EzMGVlMjJl/NzM3NWU4ZmRjNWNk/OTViZTQwOS9haWh1/Yi5xdWFsY29tbS5j/b20v",
            "path": "\u203a models  \u203a mediapipe_hand"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/FuYaNDzTcc2_nHxBaVW_S8ObhDJAfJhxdrvNbVKWpms/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9xYWlo/dWItcHVibGljLWFz/c2V0cy5zMy51cy13/ZXN0LTIuYW1hem9u/YXdzLmNvbS9xYWkt/aHViLW1vZGVscy9t/b2RlbHMvbWVkaWFw/aXBlX2hhbmQvd2Vi/LWFzc2V0cy9tb2Rl/bF9kZW1vLnBuZw",
            "original": "https://qaihub-public-assets.s3.us-west-2.amazonaws.com/qai-hub-models/models/mediapipe_hand/web-assets/model_demo.png",
            "logo": false
          }
        },
        {
          "title": "MediaPipe - Wikipedia",
          "url": "https://en.wikipedia.org/wiki/MediaPipe",
          "is_source_local": false,
          "is_source_both": false,
          "description": "Starting with the identification of the palm, MediaPipe is able to use the positioning of the palm as an input to a second model that predicts the positions of key landmarks that will represent the hand&#x27;s structure. MediaPipe continuously monitors the confidence of its predictions and re-runs detection when needed to maintain its accuracy, while temporal smoothing helps reduce the jitter between frames.",
          "page_age": "2025-12-19T19:06:04",
          "profile": {
            "name": "Wikipedia",
            "url": "https://en.wikipedia.org/wiki/MediaPipe",
            "long_name": "en.wikipedia.org",
            "img": "https://imgs.search.brave.com/m6XxME4ek8DGIUcEPCqjRoDjf2e54EwL9pQzyzogLYk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjQwNGZhZWY0/ZTQ1YWUzYzQ3MDUw/MmMzMGY3NTQ0ZjNj/NDUwMDk5ZTI3MWRk/NWYyNTM4N2UwOTE0/NTI3ZDQzNy9lbi53/aWtpcGVkaWEub3Jn/Lw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "deep_results": {
            "buttons": [
              {
                "type": "button_result",
                "title": "History",
                "url": "https://en.wikipedia.org/wiki/MediaPipe#History"
              },
              {
                "type": "button_result",
                "title": "Solutions",
                "url": "https://en.wikipedia.org/wiki/MediaPipe#Solutions"
              },
              {
                "type": "button_result",
                "title": "Programming Language",
                "url": "https://en.wikipedia.org/wiki/MediaPipe#Programming_Language"
              },
              {
                "type": "button_result",
                "title": "How MediaPipe Works",
                "url": "https://en.wikipedia.org/wiki/MediaPipe#How_MediaPipe_Works"
              }
            ]
          },
          "meta_url": {
            "scheme": "https",
            "netloc": "en.wikipedia.org",
            "hostname": "en.wikipedia.org",
            "favicon": "https://imgs.search.brave.com/m6XxME4ek8DGIUcEPCqjRoDjf2e54EwL9pQzyzogLYk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjQwNGZhZWY0/ZTQ1YWUzYzQ3MDUw/MmMzMGY3NTQ0ZjNj/NDUwMDk5ZTI3MWRk/NWYyNTM4N2UwOTE0/NTI3ZDQzNy9lbi53/aWtpcGVkaWEub3Jn/Lw",
            "path": "\u203a wiki  \u203a MediaPipe"
          },
          "age": "3 weeks ago"
        },
        {
          "title": "Hand landmarks detection guide | MediaPipe | Google for Developers",
          "url": "https://developers.google.com/mediapipe/solutions/vision/hand_landmarker",
          "is_source_local": false,
          "is_source_both": false,
          "description": "<strong>Hand Landmarker</strong> only re-triggers the palm detection model if the hand landmarks model no longer identifies the presence of hands or fails to track the hands within the frame. This reduces the number of times <strong>Hand Landmarker</strong> tiggers the palm ...",
          "profile": {
            "name": "Google",
            "url": "https://developers.google.com/mediapipe/solutions/vision/hand_landmarker",
            "long_name": "developers.google.com",
            "img": "https://imgs.search.brave.com/xd2if5kWPozWi5tGSdOQNnrmMOZFDny4yeh0h6PJu8U/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODcyMjg1ZjQ0/ZWI1OTgzNjQxNTM4/NDQ4MDljYjE2YTZi/MTc4OWFjOGM3NDEx/ZDBmZWJjYjg2YTBj/MGI3Zjk3OC9kZXZl/bG9wZXJzLmdvb2ds/ZS5jb20v"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "developers.google.com",
            "hostname": "developers.google.com",
            "favicon": "https://imgs.search.brave.com/xd2if5kWPozWi5tGSdOQNnrmMOZFDny4yeh0h6PJu8U/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODcyMjg1ZjQ0/ZWI1OTgzNjQxNTM4/NDQ4MDljYjE2YTZi/MTc4OWFjOGM3NDEx/ZDBmZWJjYjg2YTBj/MGI3Zjk3OC9kZXZl/bG9wZXJzLmdvb2ds/ZS5jb20v",
            "path": "  \u203a mediapipe  \u203a hand landmarks detection guide"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/GAPHAOESmySObnKYOeWfpQqXqlFxcq796DV_PD-ziIY/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/Z3N0YXRpYy5jb20v/ZGV2cmVsLWRldnNp/dGUvcHJvZC92N2Vj/MWNkYmY5MDk4OWFi/MDgyZjMwYmY5Yjlj/YmU2Mjc4MDQ4NDhj/MThiNzBkNzIyMDYy/YWViNmM2ZDg5NThi/NS9kZXZlbG9wZXJz/L2ltYWdlcy9vcGVu/Z3JhcGgvcGFsZS1i/bHVlLnBuZw",
            "original": "https://www.gstatic.com/devrel-devsite/prod/v7ec1cdbf90989ab082f30bf9b9cbe627804848c18b70d722062aeb6c6d8958b5/developers/images/opengraph/pale-blue.png",
            "logo": false
          }
        },
        {
          "title": "Hand tracking using Mediapipe. Access full tutorial of body pose\u2026",
          "url": "https://medium.com/@sunnykumar1516/hand-tracking-using-mediapipe-263c40ad6914",
          "is_source_local": false,
          "is_source_both": false,
          "description": "MediaPipe simplifies the development of ML-based applications by providing a modular and customizable framework. It offers a variety of pre-trained models and reusable building blocks for tasks like object detection, face detection and tracking, pose estimation, hand tracking, and more.",
          "page_age": "2024-01-04T14:39:28",
          "profile": {
            "name": "Medium",
            "url": "https://medium.com/@sunnykumar1516/hand-tracking-using-mediapipe-263c40ad6914",
            "long_name": "medium.com",
            "img": "https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "medium.com",
            "hostname": "medium.com",
            "favicon": "https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw",
            "path": "\u203a @sunnykumar1516  \u203a hand-tracking-using-mediapipe-263c40ad6914"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/SQJB29X3dmFdn9oc8q7n7zRdq7JUrSrSZnCIJQ7psiU/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9taXJv/Lm1lZGl1bS5jb20v/djIvcmVzaXplOmZp/dDoxMjAwLzEqQnFs/YkN2cmFNNDByQkdZ/Rl9YOWd5Zy5wbmc",
            "original": "https://miro.medium.com/v2/resize:fit:1200/1*BqlbCvraM40rBGYF_X9gyg.png",
            "logo": false
          },
          "age": "January 4, 2024"
        },
        {
          "title": "qualcomm/MediaPipe-Hand-Detection \u00b7 Hugging Face",
          "url": "https://huggingface.co/qualcomm/MediaPipe-Hand-Detection",
          "is_source_local": false,
          "is_source_both": false,
          "description": "<strong>The MediaPipe Hand Landmark Detector</strong> is a machine learning pipeline that predicts bounding boxes and pose skeletons of hands in an image.",
          "profile": {
            "name": "Hugging Face",
            "url": "https://huggingface.co/qualcomm/MediaPipe-Hand-Detection",
            "long_name": "huggingface.co",
            "img": "https://imgs.search.brave.com/L-O7pGsoxRbYSIBhk64OxIe19zjGm3DcqlEnCuoI678/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNmYwNDg3ZTZh/YzcxMDgwYmRkOGFm/YTk1MTJiYzQ2NmM5/NWViOGEyNGYyZWEx/NDg4NzQ5NzlhYTA5/NTI0MDA4Yy9odWdn/aW5nZmFjZS5jby8"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "huggingface.co",
            "hostname": "huggingface.co",
            "favicon": "https://imgs.search.brave.com/L-O7pGsoxRbYSIBhk64OxIe19zjGm3DcqlEnCuoI678/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNmYwNDg3ZTZh/YzcxMDgwYmRkOGFm/YTk1MTJiYzQ2NmM5/NWViOGEyNGYyZWEx/NDg4NzQ5NzlhYTA5/NTI0MDA4Yy9odWdn/aW5nZmFjZS5jby8",
            "path": "\u203a qualcomm  \u203a MediaPipe-Hand-Detection"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/glL8I8zIEdl1fZsXyjijrXYGATuQfjNnK-OeRhsRIgg/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jZG4t/dGh1bWJuYWlscy5o/dWdnaW5nZmFjZS5j/by9zb2NpYWwtdGh1/bWJuYWlscy9tb2Rl/bHMvcXVhbGNvbW0v/TWVkaWFQaXBlLUhh/bmQtRGV0ZWN0aW9u/LnBuZw",
            "original": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/qualcomm/MediaPipe-Hand-Detection.png",
            "logo": false
          }
        },
        {
          "title": "Google open sources an on-device, real-time hand gesture recognition algorithm built with MediaPipe",
          "url": "https://www.packtpub.com/en-us/learning/how-to-tutorials/google-open-sources-an-on-device-real-time-hand-gesture-recognition-algorithm-built-with-mediapipe/",
          "is_source_local": false,
          "is_source_both": false,
          "description": "Their algorithm <strong>uses machine learning to compute 3D keypoints of a hand from a video frame</strong>. This research is implemented in MediaPipe which is an open-source cross-platform framework for building multimodal (eg.",
          "profile": {
            "name": "Packt",
            "url": "https://www.packtpub.com/en-us/learning/how-to-tutorials/google-open-sources-an-on-device-real-time-hand-gesture-recognition-algorithm-built-with-mediapipe/",
            "long_name": "packtpub.com",
            "img": "https://imgs.search.brave.com/jmMH8m2C6oTR0y9v_5fVd60yhumrU_CUcH-gwVKvrPY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODAxNjFmYzkw/YWNhMzMzMjk2NDk5/ODkwMGY0YTg5NWYw/YWRjY2MzOTljNjUz/OGE2YjVmOWYyMDAx/NmY5ZWVjNS93d3cu/cGFja3RwdWIuY29t/Lw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "packtpub.com",
            "hostname": "www.packtpub.com",
            "favicon": "https://imgs.search.brave.com/jmMH8m2C6oTR0y9v_5fVd60yhumrU_CUcH-gwVKvrPY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODAxNjFmYzkw/YWNhMzMzMjk2NDk5/ODkwMGY0YTg5NWYw/YWRjY2MzOTljNjUz/OGE2YjVmOWYyMDAx/NmY5ZWVjNS93d3cu/cGFja3RwdWIuY29t/Lw",
            "path": "\u203a en-us  \u203a learning  \u203a how-to-tutorials  \u203a google-open-sources-an-on-device-real-time-hand-gesture-recognition-algorithm-built-with-mediapipe"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/TAfXJOPJD2tCOxYi6BDenOqjhxoonZIbBp-b1sHf5YU/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdWJz/Y3JpcHRpb24ucGFj/a3RwdWIuY29tL2lt/YWdlcy9sb2dvLnBu/Zw",
            "original": "https://subscription.packtpub.com/images/logo.png",
            "logo": true
          }
        }
      ],
      "family_friendly": true
    }
  },
  "repo": {
    "error": "name 'subprocess' is not defined"
  },
  "timestamp": "2026-01-10T10:47:08.779612"
}