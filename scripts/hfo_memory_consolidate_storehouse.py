#!/usr/bin/env python3
"""Medallion: Silver | Mutation: N/A | HIVE: V

Consolidate *all* discovered memory fragments into a single Hot Bronze storehouse.

What it does
  - Reads the latest fragments manifest YAML (generated by hfo_memory_fragments_manifest.py)
  - Moves each fragment (file/dir) into a canonical storehouse directory
  - Creates a symlink at the original path pointing to the new canonical location (so legacy code doesn't break)
  - Updates hfo_pointers.json so pointers reference the canonical storehouse paths
  - Writes a timestamped consolidation receipt YAML

Safety
  - Uses os.replace / rename semantics; if a move crosses filesystems (EXDEV), it will *not* copy.
    Instead it records a failure and continues.
  - Idempotent-ish: if a fragment path is already a symlink pointing into the storehouse, it is skipped.

This implements: "sqlite-only SSOT, everything else derived" by making the physical layout obvious,
while keeping compatibility via symlinks.
"""

from __future__ import annotations

import argparse
import datetime as dt
import errno
import os
from pathlib import Path
from typing import Any

import yaml

REPO_ROOT = Path(__file__).resolve().parents[1]


def _find_latest_manifest(manifest_dir: Path) -> Path | None:
    candidates = sorted(manifest_dir.glob("hfo_memory_fragments_gen88_v4_*.yaml"))
    if not candidates:
        return None
    return candidates[-1]


def _safe_load_yaml(path: Path) -> dict[str, Any]:
    return yaml.safe_load(path.read_text(encoding="utf-8"))


def _safe_dump_yaml(obj: Any) -> str:
    return yaml.safe_dump(obj, sort_keys=False, width=110)


def _safe_rel(path: Path) -> str:
    try:
        return str(path.resolve().relative_to(REPO_ROOT))
    except Exception:
        return str(path)


def _slug(rel_path: str) -> str:
    # Preserve extension, but make a stable filename per original path.
    return rel_path.replace("/", "__")


def _ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def _is_symlink_into(path: Path, storehouse_root: Path) -> bool:
    if not path.is_symlink():
        return False
    try:
        target = path.resolve()
    except Exception:
        return False
    try:
        target.relative_to(storehouse_root)
        return True
    except Exception:
        return False


def _move_no_cross_device(src: Path, dst: Path) -> tuple[bool, str | None]:
    """Move src -> dst, but never copy across devices."""
    try:
        os.replace(src, dst)
        return True, None
    except OSError as e:
        if e.errno == errno.EXDEV:
            return False, "cross_device_move_blocked"
        return False, f"move_failed:{type(e).__name__}:{e}"


def _symlink_at(src_original: Path, dst_target: Path) -> tuple[bool, str | None]:
    try:
        # Ensure parent exists.
        src_original.parent.mkdir(parents=True, exist_ok=True)
        src_original.symlink_to(dst_target)
        return True, None
    except Exception as e:
        return False, f"symlink_failed:{type(e).__name__}:{e}"


def _update_pointers(
    pointers: dict[str, Any], moved_map: dict[str, str]
) -> tuple[dict[str, Any], int]:
    """Replace pointer paths if they match a moved source path."""
    changed = 0
    paths = pointers.get("paths")
    if not isinstance(paths, dict):
        return pointers, 0
    for k, v in list(paths.items()):
        if isinstance(v, str) and v in moved_map:
            paths[k] = moved_map[v]
            changed += 1
    pointers["paths"] = paths
    return pointers, changed


def main() -> int:
    ap = argparse.ArgumentParser(
        description="Consolidate all Gen88 memory fragments into a Hot Bronze storehouse."
    )
    ap.add_argument(
        "--manifest",
        default="",
        help="Fragments manifest YAML. If omitted, uses latest in artifacts/memory_manifest/.",
    )
    ap.add_argument(
        "--storehouse-root",
        default="hfo_hot_obsidian/bronze/3_resources/memory_fragments_storehouse",
        help="Canonical storehouse root (repo-relative).",
    )
    ap.add_argument(
        "--dry-run",
        action="store_true",
        help="Do not move files; compute plan + write receipt only.",
    )
    args = ap.parse_args()

    manifest_dir = REPO_ROOT / "artifacts/memory_manifest"
    manifest_path = (
        Path(args.manifest) if args.manifest else _find_latest_manifest(manifest_dir)
    )
    if manifest_path is None:
        raise SystemExit(
            "No fragments manifest found. Run scripts/hfo_memory_fragments_manifest.py first."
        )
    if not manifest_path.is_absolute():
        manifest_path = (REPO_ROOT / manifest_path).resolve()

    storehouse_root = (REPO_ROOT / args.storehouse_root).resolve()
    if not args.dry_run:
        _ensure_dir(storehouse_root)

    doc = _safe_load_yaml(manifest_path)
    fragments = doc.get("fragments", [])
    if not isinstance(fragments, list):
        raise SystemExit("Invalid manifest: fragments is not a list")

    pointers_path = REPO_ROOT / "hfo_pointers.json"
    pointers: dict[str, Any] = {}
    if pointers_path.exists():
        import json

        pointers = json.loads(pointers_path.read_text(encoding="utf-8"))

    actions: list[dict[str, Any]] = []
    moved_map: dict[str, str] = {}

    for frag in fragments:
        if not isinstance(frag, dict):
            continue
        rel = frag.get("path")
        if not isinstance(rel, str) or not rel:
            continue

        src_original = REPO_ROOT / rel
        classification = frag.get("classification")
        if not isinstance(classification, str) or not classification:
            classification = "unclassified"

        # Skip if original path is already a symlink into storehouse.
        if src_original.exists() and _is_symlink_into(src_original, storehouse_root):
            actions.append(
                {
                    "path": rel,
                    "classification": classification,
                    "status": "skipped_already_symlinked",
                    "storehouse": _safe_rel(src_original.resolve()),
                }
            )
            moved_map[rel] = _safe_rel(src_original.resolve())
            continue

        # If missing, record and continue.
        if not src_original.exists() and not src_original.is_symlink():
            actions.append(
                {
                    "path": rel,
                    "classification": classification,
                    "status": "missing",
                }
            )
            continue

        # If it's a symlink but not into storehouse, resolve it and treat the resolved target as the src.
        # We'll move the *resolved target* if it is within the repo.
        src_resolved = src_original.resolve() if src_original.exists() else src_original
        try:
            src_resolved.relative_to(REPO_ROOT)
        except Exception:
            actions.append(
                {
                    "path": rel,
                    "classification": classification,
                    "status": "skipped_external_target",
                    "resolved": str(src_resolved),
                }
            )
            continue

        safe_name = _slug(rel)
        dst = storehouse_root / classification / safe_name
        if not args.dry_run:
            _ensure_dir(dst.parent)

        # If destination already exists, do not overwrite; record and continue.
        if dst.exists() or dst.is_symlink():
            actions.append(
                {
                    "path": rel,
                    "classification": classification,
                    "status": "dst_exists_skipped",
                    "dst": _safe_rel(dst),
                }
            )
            continue

        action: dict[str, Any] = {
            "path": rel,
            "classification": classification,
            "src": _safe_rel(src_resolved),
            "dst": _safe_rel(dst),
            "status": None,
        }

        if args.dry_run:
            action["status"] = "dry_run_planned"
            actions.append(action)
            moved_map[rel] = _safe_rel(dst)
            continue

        ok_move, move_err = _move_no_cross_device(src_resolved, dst)
        if not ok_move:
            action["status"] = "move_failed"
            action["error"] = move_err
            actions.append(action)
            continue

        # Remove original path if it still exists (it might be a symlink file).
        try:
            if src_original.exists() or src_original.is_symlink():
                src_original.unlink()
        except Exception:
            # We'll try to create symlink anyway if possible.
            pass

        ok_link, link_err = _symlink_at(src_original, dst)
        if not ok_link:
            action["status"] = "moved_but_symlink_failed"
            action["error"] = link_err
            actions.append(action)
            continue

        action["status"] = "moved_and_symlinked"
        actions.append(action)
        moved_map[rel] = _safe_rel(dst)

    pointers_updated = pointers
    pointers_changes = 0
    if pointers:
        pointers_updated, pointers_changes = _update_pointers(pointers, moved_map)
        if not args.dry_run and pointers_changes > 0:
            import json

            pointers_path.write_text(
                json.dumps(pointers_updated, ensure_ascii=False, indent=2) + "\n",
                encoding="utf-8",
            )

    ts = dt.datetime.now(dt.timezone.utc).strftime("%Y_%m_%dT%H%M%SZ")
    receipt_path = manifest_dir / f"hfo_memory_consolidation_receipt_{ts}.yaml"

    receipt = {
        "receipt_version": "hfo.memory_consolidation_receipt.v1",
        "generated_at_utc": dt.datetime.now(dt.timezone.utc)
        .isoformat()
        .replace("+00:00", "Z"),
        "dry_run": bool(args.dry_run),
        "manifest_source": _safe_rel(manifest_path),
        "storehouse_root": _safe_rel(storehouse_root),
        "pointers_updated": bool(pointers_changes > 0),
        "pointers_changes": pointers_changes,
        "actions": actions,
        "note": "Original paths are preserved via symlinks. Pointers are updated to canonical storehouse paths.",
    }
    receipt_path.write_text(_safe_dump_yaml(receipt), encoding="utf-8")
    print(str(receipt_path))

    failures = [
        a
        for a in actions
        if isinstance(a, dict) and str(a.get("status", "")).endswith("failed")
    ]
    return 2 if failures else 0


if __name__ == "__main__":
    raise SystemExit(main())
