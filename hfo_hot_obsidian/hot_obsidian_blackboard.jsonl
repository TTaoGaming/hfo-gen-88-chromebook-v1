{"timestamp": "2026-01-10T12:00:00Z", "port": "P0", "mission": "Thread Omega", "phase": "H", "p0": {"receipt": "P0_SENSE_SEARCH_V30_MENU_FIX"}, "thought_hash": "v30_h"}
{"timestamp": "2026-01-10T12:01:01Z", "port": "P7", "mission": "Thread Omega", "phase": "I", "details": {"step": 1}, "thought_hash": "v30_i1"}
{"timestamp": "2026-01-10T12:01:02Z", "port": "P7", "mission": "Thread Omega", "phase": "I", "details": {"step": 2}, "thought_hash": "v30_i2"}
{"timestamp": "2026-01-10T12:01:03Z", "port": "P7", "mission": "Thread Omega", "phase": "I", "details": {"step": 3}, "thought_hash": "v30_i3"}
{"timestamp": "2026-01-10T12:01:04Z", "port": "P7", "mission": "Thread Omega", "phase": "I", "details": {"step": 4}, "thought_hash": "v30_i4"}
{"timestamp": "2026-01-10T12:01:05Z", "port": "P7", "mission": "Thread Omega", "phase": "I", "details": {"step": 5}, "thought_hash": "v30_i5"}
{"timestamp": "2026-01-10T12:01:06Z", "port": "P7", "mission": "Thread Omega", "phase": "I", "details": {"step": 6}, "thought_hash": "v30_i6"}
{"timestamp": "2026-01-10T12:01:07Z", "port": "P7", "mission": "Thread Omega", "phase": "I", "details": {"step": 7}, "thought_hash": "v30_i7"}
{"timestamp": "2026-01-10T12:10:00Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "event": "V30_HARDENING", "payload": {"summary": "V30 Hardened and Verified."}, "thought_hash": "v30_e"}
{"timestamp": "2026-01-10T13:00:00Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "event": "V31_TRACKING_HARDENED", "payload": {"summary": "Implemented Semantic Snapback + Physics Hybrid in V31. Resolved center ghosting and ID swap collisions."}, "thought_hash": "v31_tracking_fix"}
{"timestamp": "2026-01-10T13:10:00Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "thought_hash": "f6e2...", "payload": {"summary": "REVERT: Restored click functionality by removing Hand 0 ID restriction. Analyzing multi-touch vs rubber-banding conflict.", "octet": ["T0: Identified ID-lock failure", "T1: Reverted p3InjectPointer", "T2: Reinstated interactionOwner for stable baseline", "T3: Restored MouseEvent for all hands", "T4: Plan: Use Dynamic State-based Ownership", "T5: Medallion Bronze status: Stable but Rubber-Banding", "T6: Blackboard entry created", "T7: Awaiting user instruction on Multi-Touch vs Lockout"]}}
{"timestamp": "2026-01-10T13:20:00Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "thought_hash": "a4d1...", "payload": {"summary": "Option 2 + Snaplock Fix deployed. Restored instant-snap discovery and implemented Shared Mouse Authority.", "octet": ["T0: Identified 0.15 distance clip in discovery", "T1: Removed otherH.active check that blocked discovery", "T2: Implemented Option 2: Pointer(All) + Mouse(Owner)", "T3: interactionOwner now acts as a dynamic Mouse Token", "T4: Verified bipartite weights (0.6 matching favoring)", "T5: Medallion Bronze: Multi-Touch Validated", "T6: Blackboard updated", "T7: Ready for Excalidraw stress test"]}}
{"timestamp": "2026-01-10T13:30:00Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "thought_hash": "c8b4...", "payload": {"summary": "Fixed Mouse Authority Lock Clearance and restored isPrimary:true for all hands.", "octet": ["T0: Identified permanent interactionOwner lock-in", "T1: Added hfoState.interactionOwner = null on mouseup", "T2: Restored isPrimary:true to fix single-finger browser heuristics", "T3: Option 2 logic now cycles authority between hands", "T4: Verified and fixed regression vs v31 click-parity", "T5: Medallion Bronze: V32.2 Hotfix", "T6: Blackboard updated", "T7: Ready for Excalidraw menu verification"]}}
{"timestamp": "2026-01-10T14:00:00Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "event": "V33_IDENTITY_STABILIZATION", "payload": {"summary": "V33 Stabilized: Geometric Identity + Coastal Persistence (1s) + Identity Repulsion (0.1).", "octet": ["T0: Observed ID doubling on single landmark", "T1: Planned territorial repulsion logic", "T2: Modified Bipartite assignment with assignedPos checks", "T3: Verified repulsionDistance=0.1 prevents overlaps", "T4: Verified Coastal Persistence survives 1s drops", "T5: P5 Forensic Harness: ALL NOMINAL", "T6: Blackboard updated with v33 receipts", "T7: Promoted V33 to Cold Bronze Candidate"]}, "thought_hash": "v33_stable_e"}
{"timestamp": "2026-01-10T14:15:00Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "event": "V33_HIGH_INERTIA_STABILIZATION", "payload": {"summary": "V33 Hardened: High-Inertia Tracker. Active hands now have priority; inactive hands face +0.5 cost barrier. Added 0.3 teleportLimit and increased coast to 60f.", "octet": ["T0: Diagnosed ID-theft by inactive hands", "T1: Inverted cost bias (Barrier-based Discovery)", "T2: Implemented cost = d + 0.5 for inactive IDs", "T3: Added d > 0.3 teleport rejection", "T4: Increased repulsion to 0.12 for multi-hand stability", "T5: P5 Forensic Harness: ALL NOMINAL", "T6: Blackboard entry created", "T7: Ready for real-world movement stress test"]}, "thought_hash": "v33_inertia_e"}
{"timestamp": "2026-01-10T14:30:00Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "event": "V33_COLD_FREEZE_V34_SCAFFOLD", "payload": {"summary": "V33 Frozen to Cold Bronze as Golden Example. Scaffolded V34 in Hot Bronze for tuning.", "octet": ["T0: Verified V33 stability with Harness", "T1: Copied V33 to hfo_cold_obsidian", "T2: Cloned V33 to V34.html", "T3: Updated V34 headers to H-Phase", "T4: Verified path integrity", "T5: Updated AGENTS.md status", "T6: Stigmergy receipt generated", "T7: Ready for V34 tuning phase"]}, "thought_hash": "v33_freeze_v34_start"}
{"timestamp": "2026-01-10T14:40:00Z", "port": "P7", "mission": "Thread Omega", "phase": "I", "event": "V34_UI_REFINEMENT", "payload": {"summary": "V34 Refactored: Implemented Hero Pattern (Excalidraw 70% Left) + Diagnostic Stack (Top Right) + MediaPipe (Bottom Right).", "octet": ["T0: Identified cramped UI in V33", "T1: Planned Hero pattern relocation", "T2: Refactored Golden Layout config", "T3: Moved Excalidraw to 70% Hero area", "T4: Stacked P3, P2, P7, P3 panels in Diagnostic tabset", "T5: P5 Syntax Gate: NOMINAL", "T6: Blackboard entry created", "T7: UI focused and ready for high-fidelity tuning"]}, "thought_hash": "v34_ui_hero"}
{"timestamp": "2026-01-10T14:50:00Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "event": "V34_COLD_FREEZE_V35_SCAFFOLD", "payload": {"summary": "V34 Frozen to Cold Bronze. Hero Layout verified. Scaffolded V35 for further tuning.", "octet": ["T0: Verified V34 Hero Layout", "T1: Copied V34 to Cold Obsidian", "T2: Cloned V34 to V35.html", "T3: Updated V35 title and HIVE phase", "T4: Path integrity check passed", "T5: Updated AGENTS.md mission status", "T6: Terminal receipt generated", "T7: Ready for V35 tuning session"]}, "thought_hash": "v34_freeze_v35_start"}
{"timestamp": "2026-01-10T17:24:39Z", "port": "P7", "mission": "Thread Omega", "phase": "I", "payload": {"summary": "Explaining Mermaid breakage factors (Renderer, Security, Syntax).", "octet": ["T0: Detected unquoted separators and math operators in labels.", "T1: Linked to Mermaid parser sensitivity versus standard Markdown.", "T2: Environment audit: Chromebook V-1 Cold Start extension gap.", "T3: Preview security simulation: Script blocking factors.", "T4: Recalled P7 Blood Grudge regarding diagram hallucinations.", "T5: Integrity check: Markdown linting passes but drawing fails.", "T6: Receipt: User-lead syntax simplification efforts noted in git.", "T7: Final conceptual bridge: Environmental vs. Syntactic breakage."]}, "thought_hash": "mermaid_breakage_i"}
{"timestamp": "2026-01-10T17:29:35Z", "port": "P7", "mission": "Thread Omega", "phase": "V", "payload": {"summary": "Deployed Mermaid Test Suite and diagnosed unquoted label crashes.", "octet": ["T0: Created MERMAID_TEST.md with 5 isolated test cases.", "T1: Identified MPE vs standard Mermaid extension collision potential.", "T2: Verified unquoted '<' as a syntax-killing factor in V34 docs.", "T3: Predicted 'Strict' security policy blocking script execution.", "T4: Recalled P7 Blood Grudge: Hallucinated diagrams are the root of mistrust.", "T5: Integrity check: V34 Analysis syntax requires manual escaping.", "T6: Receipt: MERMAID_TEST.md deployed to hfo_hot_obsidian/.", "T7: Recommendation: User must allow scripts in Preview and quote all labels."]}, "thought_hash": "mermaid_test_deployment"}
{"timestamp": "2026-01-10T17:31:42Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "payload": {"summary": "Recommending bierner.markdown-mermaid and providing CP-based security override instructions.", "octet": ["T0: Identified extension triad: Bierner, Vstirbu, and MPE.", "T1: Recommendation: bierner.markdown-mermaid for core stability.", "T2: MPE detected as possible source of 'auto.css' conflict.", "T3: Security unblock via Command Palette is the robust path vs. UI icons.", "T4: Recalled P7 Blood Grudge: Environmental instability is a 'Ghost' bug.", "T5: Integrity check: Removing MPE dependencies from settings.json if needed.", "T6: Stigmergy: Logging recommendation to the blackboard.", "T7: Final instruction set prepared for user."]}, "thought_hash": "extension_consolidation_e"}
{"timestamp": "2026-01-10T17:34:28Z", "port": "P7", "mission": "Thread Omega", "phase": "V", "payload": {"summary": "Physics Track Diagnostic: MPE Hijacking confirmed via 'No diagram type detected' error signature.", "octet": ["T0: Observation: 'Flash' of rendering followed by override error.", "T1: Identification: Error message text is a unique fingerprint of Markdown Preview Enhanced (MPE).", "T2: Mechanism: MPE attempts to intercept mermaid blocks but fails its internal config load.", "T3: Simulation: The standard Bierner extension renders frame 1, MPE overrides in frame 2.", "T4: Recalled P7 Blood Grudge: Conflicting authorities (multiple renderers) lead to documentation death.", "T5: Integrity check: User must explicitly disable MPE to restore native rendering.", "T6: Receipt: Logging MPE as the primary blocking agent.", "T7: Strategy: Command Palette unblock + MPE removal is the only viable path."]}, "thought_hash": "mermaid_physics_track_v"}
{"timestamp": "2026-01-10T17:44:43Z", "phase": "H", "port": "P7", "thought_hash": "p7_tools_audit_h", "payload": {"summary": "Tool Audit: Identified state of Port Tools 0-7. Port 0 is 8-pillar aligned. Port 4 is missing. Port 5 is fragmented. Others minimal.", "octet": ["T0: Audited all port directories and tool scripts.", "T1: Confirmed Port 0 8-pillar manifold as the gold standard.", "T2: Identified 8x8 requirement gaps for Ports 1-6.", "T3: Prepared plan for Fractal Oct-Tree sharding (64 tool targets).", "T4: Identified lack of automatic enforcement (Copilot manual only).", "T5: Integrity check: Port 4 (Red Regnant) is currently a ghost port.", "T6: Synthesis: Blackboard entries confirm manual octet logging but no automated pillars for p1-p6.", "T7: Recommendation: Scaffold unified port_N_tools.py for all ports."]}}
{"timestamp": "2026-01-10T17:46:39.114043+00:00", "phase": "H", "summary": "Port 0 Sense: AI tool enforcement gap and po", "p0": {"status": "complete", "query": "AI tool enforcement gap and port tool sharding requirements", "receipt": "P0_SENSE_20260110_104639", "payload_meta": {"chars": 13807, "tokens": 3451}, "data": {"p1_tavily": [{"url": "https://docs.port.io/guides/all/enforce-ai-coding-security-standards/", "title": "Enforce AI coding security standards", "content": "GitHub Copilot Documentation Standards: Verifies presence of documentation instructions for AI tools; GitHub Copilot Coding Standards", "score": 0.9967269, "raw_content": null}, {"url": "https://dev.to/ibmdeveloper/why-sharding-is-essential-to-fine-tuning-llms-4he4", "title": "Why sharding is essential to fine-tuning LLMs", "content": "... sharding strategies, and provide practical guidance based on industry-standard tools. Why training and fine-tuning LLMs require sharding.", "score": 0.9872773, "raw_content": null}, {"url": "https://milvus.io/ai-quick-reference/how-does-sharding-and-partitioning-work-in-ai-databases", "title": "How does sharding and partitioning work in AI databases?", "content": "AI Quick Reference. Tutorials. Bootcamps \u00b7 Demo \u00b7 Video. Tools. Attu \u00b7 Milvus CLI \u00b7 Milvus Sizing Tool \u00b7 Milvus Backup Tool \u00b7 Vector Transport Service (VTS)", "score": 0.98667485, "raw_content": null}], "p2_brave": [{"title": "Enforce AI coding security standards | Port", "url": "https://docs.port.io/guides/all/enforce-ai-coding-security-standards/", "is_source_local": false, "is_source_both": false, "description": "This guide demonstrates how to use Port scorecards to automatically check if repositories have AI coding rules defined, helping enforce security best practices across your organization. Enforce security standards by ensuring repositories have detailed Cursor security rules and GitHub Copilot instructions \u00b7 Track security compliance across development teams with specific, actionable security requirements \u00b7 Identify security gaps in AI coding practices and prioritize remediation efforts", "profile": {"name": "Port", "url": "https://docs.port.io/guides/all/enforce-ai-coding-security-standards/", "long_name": "docs.port.io", "img": "https://imgs.search.brave.com/LaG-iIRxvZ-Dx6xFJKkrqBn2z7WnVWiDW8QUBd53nyw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZDQxZWY3ODIx/NDk4OGY1YTA4NTgy/ZmZlZGM0NmI4ZGEy/OWE3YzRiYmQzNDA2/ODg5MjdjOGYyNTBj/ZWUzNzBmYS9kb2Nz/LnBvcnQuaW8v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "docs.port.io", "hostname": "docs.port.io", "favicon": "https://imgs.search.brave.com/LaG-iIRxvZ-Dx6xFJKkrqBn2z7WnVWiDW8QUBd53nyw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZDQxZWY3ODIx/NDk4OGY1YTA4NTgy/ZmZlZGM0NmI4ZGEy/OWE3YzRiYmQzNDA2/ODg5MjdjOGYyNTBj/ZWUzNzBmYS9kb2Nz/LnBvcnQuaW8v", "path": "\u203a guides  \u203a all  \u203a enforce-ai-coding-security-standards"}}, {"title": "AI Guardrails: Enforcing Safety Without Slowing Innovation", "url": "https://www.obsidiansecurity.com/blog/ai-guardrails", "is_source_local": false, "is_source_both": false, "description": "Automating SaaS compliance reduces manual burden while ensuring consistent policy enforcement across AI deployments. AI guardrails must work seamlessly with current security stack and infrastructure. Modern AI deployments span multiple SaaS platforms. Integration points include: Identity providers: Azure AD, Okta, Ping Identity for centralized authentication \u00b7 Data platforms: Snowflake, Databricks, BigQuery for training data governance \u00b7 Collaboration tools: Slack, Teams, Google Workspace where AI assistants operate", "page_age": "2025-11-05T23:42:19", "profile": {"name": "Obsidian Security", "url": "https://www.obsidiansecurity.com/blog/ai-guardrails", "long_name": "obsidiansecurity.com", "img": "https://imgs.search.brave.com/9ahrTp4g2VgjxA8qvnDc7ZKXcvIAgbiFwHdreCLcrwc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZDQ5NjcxY2Fi/YmZmOTdkOTc1ODQw/ZjU4YWZlMjk2YTdm/ZTExYTExMmFhNjFk/NWRjMWE5OWUzMDUw/Y2FhOTM2My93d3cu/b2JzaWRpYW5zZWN1/cml0eS5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "obsidiansecurity.com", "hostname": "www.obsidiansecurity.com", "favicon": "https://imgs.search.brave.com/9ahrTp4g2VgjxA8qvnDc7ZKXcvIAgbiFwHdreCLcrwc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZDQ5NjcxY2Fi/YmZmOTdkOTc1ODQw/ZjU4YWZlMjk2YTdm/ZTExYTExMmFhNjFk/NWRjMWE5OWUzMDUw/Y2FhOTM2My93d3cu/b2JzaWRpYW5zZWN1/cml0eS5jb20v", "path": "\u203a blog  \u203a ai-guardrails"}, "thumbnail": {"src": "https://imgs.search.brave.com/X6sq34uFdOB_j1IEHPvVO1XjTrWREYe1Zg0qftmec04/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jZG4u/cHJvZC53ZWJzaXRl/LWZpbGVzLmNvbS82/N2IzYmYyMDAzZjlj/M2Q3OTVlNzVlN2Iv/NjhmYzI4N2U2NTgx/MjliYTIxZTI2Y2Mx/X1VudGl0bGVkLTEw/LnBuZw", "original": "https://cdn.prod.website-files.com/67b3bf2003f9c3d795e75e7b/68fc287e658129ba21e26cc1_Untitled-10.png", "logo": false}, "age": "November 5, 2025"}, {"title": "Securing the Model Context Protocol (MCP): Risks, Controls, and Governance", "url": "https://arxiv.org/html/2511.20920v1", "is_source_local": false, "is_source_both": false, "description": "We present a comprehensive defense-in-depth control framework with five categories of controls (authentication/authorization, provenance tracking, sandboxing, policy enforcement, centralized governance) and a gateway architecture pattern that operationalizes these controls through a single enforcement point (Section 4). ... We map our controls to established governance frameworks (NIST AI RMF, ISO/IEC 42001, ISO/IEC 27001), providing concrete control-to-framework alignments and a phased implementation strategy that enables organizations to integrate MCP security into existing compliance programs (Section 5). ... We identify critical open research problems including verifiable tool registries, formal verification for adaptive systems, and privacy-preserving agent operations, establishing a research agenda for securing dynamic agent systems (Section 6).", "page_age": "2025-11-25T23:20:07", "profile": {"name": "arXiv", "url": "https://arxiv.org/html/2511.20920v1", "long_name": "arxiv.org", "img": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "arxiv.org", "hostname": "arxiv.org", "favicon": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv", "path": "\u203a html  \u203a 2511.20920v1"}, "age": "November 25, 2025"}], "p3_stigmergy": {"hot": [{"timestamp": "2026-01-10T17:24:39Z", "port": "P7", "mission": "Thread Omega", "phase": "I", "payload": {"summary": "Explaining Mermaid breakage factors (Renderer, Security, Syntax).", "octet": ["T0: Detected unquoted separators and math operators in labels.", "T1: Linked to Mermaid parser sensitivity versus standard Markdown.", "T2: Environment audit: Chromebook V-1 Cold Start extension gap.", "T3: Preview security simulation: Script blocking factors.", "T4: Recalled P7 Blood Grudge regarding diagram hallucinations.", "T5: Integrity check: Markdown linting passes but drawing fails.", "T6: Receipt: User-lead syntax simplification efforts noted in git.", "T7: Final conceptual bridge: Environmental vs. Syntactic breakage."]}, "thought_hash": "mermaid_breakage_i"}, {"timestamp": "2026-01-10T17:29:35Z", "port": "P7", "mission": "Thread Omega", "phase": "V", "payload": {"summary": "Deployed Mermaid Test Suite and diagnosed unquoted label crashes.", "octet": ["T0: Created MERMAID_TEST.md with 5 isolated test cases.", "T1: Identified MPE vs standard Mermaid extension collision potential.", "T2: Verified unquoted '<' as a syntax-killing factor in V34 docs.", "T3: Predicted 'Strict' security policy blocking script execution.", "T4: Recalled P7 Blood Grudge: Hallucinated diagrams are the root of mistrust.", "T5: Integrity check: V34 Analysis syntax requires manual escaping.", "T6: Receipt: MERMAID_TEST.md deployed to hfo_hot_obsidian/.", "T7: Recommendation: User must allow scripts in Preview and quote all labels."]}, "thought_hash": "mermaid_test_deployment"}, {"timestamp": "2026-01-10T17:31:42Z", "port": "P7", "mission": "Thread Omega", "phase": "E", "payload": {"summary": "Recommending bierner.markdown-mermaid and providing CP-based security override instructions.", "octet": ["T0: Identified extension triad: Bierner, Vstirbu, and MPE.", "T1: Recommendation: bierner.markdown-mermaid for core stability.", "T2: MPE detected as possible source of 'auto.css' conflict.", "T3: Security unblock via Command Palette is the robust path vs. UI icons.", "T4: Recalled P7 Blood Grudge: Environmental instability is a 'Ghost' bug.", "T5: Integrity check: Removing MPE dependencies from settings.json if needed.", "T6: Stigmergy: Logging recommendation to the blackboard.", "T7: Final instruction set prepared for user."]}, "thought_hash": "extension_consolidation_e"}, {"timestamp": "2026-01-10T17:34:28Z", "port": "P7", "mission": "Thread Omega", "phase": "V", "payload": {"summary": "Physics Track Diagnostic: MPE Hijacking confirmed via 'No diagram type detected' error signature.", "octet": ["T0: Observation: 'Flash' of rendering followed by override error.", "T1: Identification: Error message text is a unique fingerprint of Markdown Preview Enhanced (MPE).", "T2: Mechanism: MPE attempts to intercept mermaid blocks but fails its internal config load.", "T3: Simulation: The standard Bierner extension renders frame 1, MPE overrides in frame 2.", "T4: Recalled P7 Blood Grudge: Conflicting authorities (multiple renderers) lead to documentation death.", "T5: Integrity check: User must explicitly disable MPE to restore native rendering.", "T6: Receipt: Logging MPE as the primary blocking agent.", "T7: Strategy: Command Palette unblock + MPE removal is the only viable path."]}, "thought_hash": "mermaid_physics_track_v"}, {"timestamp": "2026-01-10T17:44:43Z", "phase": "H", "port": "P7", "thought_hash": "p7_tools_audit_h", "payload": {"summary": "Tool Audit: Identified state of Port Tools 0-7. Port 0 is 8-pillar aligned. Port 4 is missing. Port 5 is fragmented. Others minimal.", "octet": ["T0: Audited all port directories and tool scripts.", "T1: Confirmed Port 0 8-pillar manifold as the gold standard.", "T2: Identified 8x8 requirement gaps for Ports 1-6.", "T3: Prepared plan for Fractal Oct-Tree sharding (64 tool targets).", "T4: Identified lack of automatic enforcement (Copilot manual only).", "T5: Integrity check: Port 4 (Red Regnant) is currently a ghost port.", "T6: Synthesis: Blackboard entries confirm manual octet logging but no automated pillars for p1-p6.", "T7: Recommendation: Scaffold unified port_N_tools.py for all ports."]}}], "cold": [{"timestamp": "2026-01-10T08:52:37Z", "layer": "cold-bronze", "port": "P7", "hive": "E", "thought_hash": "b2c3d4e5f6g7h8i9", "payload": {"summary": "Cold Bronze Promotion: V23 Physics Cursor. Integrated 5-state predictive FSM with lookahead recovery.", "mutation_score": "0%"}}, {"timestamp": "2026-01-10T09:04:38Z", "layer": "cold-bronze", "port": "P7", "hive": "E", "thought_hash": "f6g7h8i9j0k1l2m3", "payload": {"summary": "Cold Bronze Promotion: V24 Prefire Logic. Zero-latency lookahead enabled.", "mutation_score": "0%"}}, {"timestamp": "2026-01-10T06:35:00Z", "phase": "E-Phase", "thought_hash": "6b7e...", "payload": {"summary": "V29 Promotion: Trans-Boundary Drill frozen into Cold Bronze. Verified 1:1 same-origin mapping and mouse parity. Manifest and Agent Brief updated."}}]}, "p4_repo": {"status": "no_direct_symbol_match"}, "p5_docs": "N/A", "p6_arxiv": [{"title": "AIRCC-Clim: a user-friendly tool for generating regional probabilistic climate change scenarios and risk measures", "summary": "Complex physical models are the most advanced tools available for producing realistic simulations of the climate system. However, such levels of realism imply high computational cost and restrictions ...", "url": "https://arxiv.org/pdf/2111.01762v1"}, {"title": "ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching", "summary": "Security critical software, e.g., OpenSSL, comes with numerous side-channel leakages left unpatched due to a lack of resources or experts. The situation will only worsen as the pace of code developmen...", "url": "https://arxiv.org/pdf/2308.13062v1"}, {"title": "FastSpec: Scalable Generation and Detection of Spectre Gadgets Using Neural Embeddings", "summary": "Several techniques have been proposed to detect vulnerable Spectre gadgets in widely deployed commercial software. Unfortunately, detection techniques proposed so far rely on hand-written rules which ...", "url": "https://arxiv.org/pdf/2006.14147v2"}], "p7_wiki": {"title": "Offshore wind power in the United States", "summary": "Offshore wind power in the United States is in the early stages of development. In 2022, the National Renewable Energy Laboratory (NREL) estimated that the country has a \"technical\" resource potential of 1,476 GW (fixed-bottom) and 2,773 GW (floating) offshore wind power.", "url": "https://en.wikipedia.org/wiki/Offshore_wind_power_in_the_United_States"}, "p8_git": ["753ec67 docs(omega): force update analysis to clear preview cache", "bdb7d1f docs(omega): fix unclosed mermaid blocks and simplify syntax", "22f214f docs(omega): fix mermaid syntax in v34 analysis"]}}}
{"timestamp": "2026-01-10T17:46:57.070998Z", "phase": "H", "summary": "P0-Sense QUAD Hunt: hand tracking identity assignment algorithms simplifying bipartite matching for hand tracking in mediapipe temporal hand tracking consistency algorithms", "p0": {"status": "complete", "query": "hand tracking identity assignment algorithms simplifying bipartite matching for hand tracking in mediapipe temporal hand tracking consistency algorithms", "receipt": "P0_QUAD_SEARCH_20260110_174657", "data": {"web_tavily": [{"url": "https://www.sciencedirect.com/science/article/pii/S1746809424005664", "title": "Hand tracking for clinical applications: Validation of the ...", "content": "In this paper, the aim is to validate the hand-tracking framework implemented by Google MediaPipe Hand (GMH) and an innovative enhanced version, GMH-D, that exploits the depth estimation of an RGB-Depth camera to achieve more accurate tracking of 3D movements. Three dynamic exercises commonly administered by clinicians to assess hand dysfunctions, namely hand opening\u2013closing, single finger tapping and multiple finger tapping are considered. Results demonstrate high temporal and spectral consistency of both frameworks with the gold standard. However, the enhanced GMH-D framework exhibits superior accuracy in spatial measurements compared to the baseline GMH, for both slow and fast movements. Overall, our study contributes to the advancement of hand tracking technology, and the [...] to the advancement of hand tracking technology, and the establishment of a validation procedure as a good-practice to prove efficacy of deep-learning-based hand-tracking. Moreover, it proves that GMH-D is a reliable framework for assessing 3D hand movements in clinical applications.", "score": 0.99620515, "raw_content": null}, {"url": "https://www.emergentmind.com/topics/mediapipe-hands", "title": "MediaPipe Hands: Real-Time Tracking - Emergent Mind", "content": "3. Temporal Tracking and Filtering: For video-based processing, intersection-over-union (IoU) matching, Kalman filters, or exponential smoothing can be used to maintain ID continuity and suppress outliers during rapid articulation and occlusion. Block feature refinement, as in 3DCPN (Li et al., 2021), enhances discriminative robustness, although MediaPipe Hands typically opts for lightweight, residual-based filtering for speed.\n4. Multihand Pipeline: The system runs multiple palm/landmark pipelines in parallel, supporting up to sixteen hands per frame in theory, although practical device limitations often cap performance at two hands in real time. [...] MediaPipe Hands is an open-source, real-time hand and finger tracking solution originating from Google\u2019s MediaPipe framework, built to deliver precise, markerless multi-hand localization and articulated pose estimation in unconstrained scenarios. It is extensively used in computer vision, augmented reality, gesture-based interaction, sign language translation, and robotics. The system is distinguished by its lightweight pipeline architecture, combining fast palm detectors, regression-based keypoint estimators, and temporal filtering, suitable for large-scale deployment across mobile platforms.\n\n## 1. Architecture and Key Algorithms\n\nMediaPipe Hands employs a cascaded pipeline optimizing for both robustness and low-latency: [...] Sign Up to Follow Topic by Email\n\n### Continue Learning\n\n1. How does MediaPipe Hands achieve real-time performance on mobile devices?\n2. What are the advantages of a palm-centric approach over traditional fingertip detection methods?\n3. How do regression-based keypoint estimators improve the accuracy of hand landmark detection?\n4. What challenges arise when scaling MediaPipe Hands for tracking multiple hands in crowded scenes?\n5. Find recent papers about real-time hand tracking.\n\n### Related Topics", "score": 0.99620515, "raw_content": null}, {"url": "https://ar5iv.labs.arxiv.org/html/2006.10214", "title": "MediaPipe Hands: On-device Real-time Hand Tracking - ar5iv", "content": "Our MediaPipe graph for hand tracking is shown in Figure 5. The graph consists of two subgraphs \u2014 one for hand detection and another for landmarks computation. One key optimization MediaPipe provides is that the palm detector only runs as needed (fairly infrequently), saving significant computation. We achieve this by deriving the hand location in the current video frames from the computed hand landmarks in the previous frame, eliminating the need to apply the palm detector on every frame. For robustness, the hand tracker model also outputs an additional scalar capturing the confidence that a hand is present and reasonably aligned in the input crop. Only when the confidence falls below a certain threshold is the hand detection model reapplied to the next frame.\n\n## 6 Application examples [...] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll\u00e1r.   Focal loss for dense object detection.   CoRR, abs/1708.02002, 2017.\n   Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg.   SSD: single shot multibox detector.   CoRR, abs/1512.02325, 2015.\n   Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, Wan-Teh Chang, Wei Hua, Manfred Georg, and Matthias Grundmann.   Mediapipe: A framework for building perception pipelines.   volume abs/1906.08172, 2019.\n   Iason Oikonomidis, Nikolaos Kyriazis, and Antonis A Argyros.   Efficient model-based 3d tracking of hand articulations using kinect. [...] # MediaPipe Hands: On-device Real-time Hand Tracking\n\nFan Zhang\u2003Valentin Bazarevsky\u2003Andrey Vakunov   \nAndrei Tkachenka\u2003George Sung\u2003Chuo-Ling Chang\u2003Matthias Grundmann   \nGoogle Research   \n1600 Amphitheatre Pkwy, Mountain View, CA 94043, USA   \n{zhafang, valik, vakunov, atkach, gsung, chuoling, grundman}@google.com\n\n###### Abstract", "score": 0.97539, "raw_content": null}, {"url": "https://arxiv.org/abs/2006.10214", "title": "MediaPipe Hands: On-device Real-time Hand Tracking - arXiv", "content": "We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\n\n> cs > arXiv:2006.10214\n\n# Computer Science > Computer Vision and Pattern Recognition\n\narXiv:2006.10214 (cs)\n\n[Submitted on 18 Jun 2020]\n\n# Title:MediaPipe Hands: On-device Real-time Hand Tracking\n\nAuthors:Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George Sung, Chuo-Ling Chang, Matthias Grundmann\n\nView a PDF of the paper titled MediaPipe Hands: On-device Real-time Hand Tracking, by Fan Zhang and 6 other authors [...] |  |  |\n --- |\n| Comments: | 5 pages, 7 figures; CVPR Workshop on Computer Vision for Augmented and Virtual Reality, Seattle, WA, USA, 2020 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | arXiv:2006.10214 [cs.CV] |\n|  | (or  arXiv:2006.10214v1 [cs.CV] for this version) |\n|  |  arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Valentin Bazarevsky [view email]   \n [v1] Thu, 18 Jun 2020 00:19:13 UTC (2,262 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MediaPipe Hands: On-device Real-time Hand Tracking, by Fan Zhang and 6 other authors\n\n View PDF\n TeX Source\n\nview license\n\nCurrent browse context:\n\ncs.CV\n\n< prev\")    |    next >\")\n\nnew  |  recent  | 2020-06\n\nChange to browse by:\n\ncs\n\n### References & Citations [...] View PDF\n> Abstract:We present a real-time on-device hand tracking pipeline that predicts hand skeleton from single RGB camera for AR/VR applications. The pipeline consists of two models: 1) a palm detector, 2) a hand landmark model. It's implemented via MediaPipe, a framework for building cross-platform ML solutions. The proposed model and pipeline architecture demonstrates real-time inference speed on mobile GPUs and high prediction quality. MediaPipe Hands is open sourced at this https URL.", "score": 0.9585377, "raw_content": null}, {"url": "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html", "title": "MediaPipe Hands - Read the Docs", "content": "MediaPipe Hands is a high-fidelity hand and finger tracking solution. It employs machine learning (ML) to infer 21 3D landmarks of a hand from just a single frame. Whereas current state-of-the-art approaches rely primarily on powerful desktop environments for inference, our method achieves real-time performance on a mobile phone, and even scales to multiple hands. We hope that providing this hand perception functionality to the wider research and development community will result in an emergence of creative use cases, stimulating new applications and new research avenues.\n\nhand\\_tracking\\_3d\\_android\\_gpu.gif | :\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014: | Fig 1. Tracked 3D hand landmarks are represented by dots in different shades, with the brighter ones denoting landmarks closer to the camera. | [...] approach). Lastly, we minimize the focal loss during training to support a large amount of anchors resulting from the high scale variance. [...] With the above techniques, we achieve an average precision of 95.7% in palm detection. Using a regular cross entropy loss and no decoder gives a baseline of just 86.22%.\n\n### Hand Landmark Model\u00b6\n\nAfter the palm detection over the whole image our subsequent hand landmark model performs precise keypoint localization of 21 3D hand-knuckle coordinates inside the detected hand regions via regression, that is direct coordinate prediction. The model learns a consistent internal hand pose representation and is robust even to partially visible hands and self-occlusions.", "score": 0.9458012, "raw_content": null}], "web_brave": [{"title": "[2006.10214] MediaPipe Hands: On-device Real-time Hand Tracking", "url": "https://arxiv.org/abs/2006.10214", "is_source_local": false, "is_source_both": false, "description": "View a PDF of the paper titled ... authors View PDF \u00b7 Abstract:<strong>We present a real-time on-device hand tracking pipeline that predicts hand skeleton from single RGB camera for AR/VR applications</strong>....", "page_age": "2020-06-18T00:00:00", "profile": {"name": "arXiv", "url": "https://arxiv.org/abs/2006.10214", "long_name": "arxiv.org", "img": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "arxiv.org", "hostname": "arxiv.org", "favicon": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv", "path": "\u203a abs  \u203a 2006.10214"}, "thumbnail": {"src": "https://imgs.search.brave.com/iKdq2fkHaKJYPDvdK7fP7YT23ZvBXz_aBGM5hD9_jho/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcnhp/di5vcmcvc3RhdGlj/L2Jyb3dzZS8wLjMu/NC9pbWFnZXMvYXJ4/aXYtbG9nby1mYi5w/bmc", "original": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png", "logo": true}, "age": "June 18, 2020"}, {"title": "On-Device, Real-Time Hand Tracking with MediaPipe", "url": "https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/", "is_source_local": false, "is_source_both": false, "description": "Mediapipe comes with an extendable set of Calculators to solve tasks like model inference, media processing algorithms, and data transformations across a wide variety of devices and platforms. Individual calculators like cropping, rendering and neural network computations can be performed exclusively on the GPU. For example, we employ TFLite GPU inference on most modern phones. Our MediaPipe graph for hand tracking is shown below. The graph consists of two subgraphs\u2014one for hand detection and one for hand keypoints (i.e., landmark) computation.", "profile": {"name": "Google Research", "url": "https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/", "long_name": "research.google", "img": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "research.google", "hostname": "research.google", "favicon": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv", "path": "\u203a blog  \u203a on-device-real-time-hand-tracking-with-mediapipe"}, "thumbnail": {"src": "https://imgs.search.brave.com/Ww80QDOXmgj8yDED_u8edBEb9uOb2U_NaHeT6DId_Io/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdG9y/YWdlLmdvb2dsZWFw/aXMuY29tL2d3ZWIt/cmVzZWFyY2gyMDIz/LW1lZGlhL2ltYWdl/cy9lM2VmMDk1Njhm/ZTZiMDlkMGI5Mzc3/MmQzZjAyZGJmMy1p/LndpZHRoLTgwMC5m/b3JtYXQtanBlZy5q/cGc", "original": "https://storage.googleapis.com/gweb-research2023-media/images/e3ef09568fe6b09d0b93772d3f02dbf3-i.width-800.format-jpeg.jpg", "logo": false}}, {"title": "[2006.10214] MediaPipe Hands: On-device Real-time Hand Tracking", "url": "https://ar5iv.labs.arxiv.org/html/2006.10214", "is_source_local": false, "is_source_both": false, "description": "<strong>We present a real-time on-device hand tracking solution that predicts a hand skeleton of a human from a single RGB camera for AR/VR applications</strong>. Our pipeline consists of two models: 1) a palm detector, that is providi\u2026", "page_age": "2024-03-02T11:16:50", "profile": {"name": "arXiv", "url": "https://ar5iv.labs.arxiv.org/html/2006.10214", "long_name": "ar5iv.labs.arxiv.org", "img": "https://imgs.search.brave.com/Ra63-x8TeB8HQ-I2dv-lRN_mIzQ3JQGTcfoprRnqdaI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGEzNGUyYWVl/NDk2ODQ0MzYwZjhk/ZGRkOGIyNWVkN2Qz/ZWZiMjQ4ZGI2YWQw/NjQ0M2Q5YWFhMTM4/MDljYjI0OS9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "ar5iv.labs.arxiv.org", "hostname": "ar5iv.labs.arxiv.org", "favicon": "https://imgs.search.brave.com/Ra63-x8TeB8HQ-I2dv-lRN_mIzQ3JQGTcfoprRnqdaI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGEzNGUyYWVl/NDk2ODQ0MzYwZjhk/ZGRkOGIyNWVkN2Qz/ZWZiMjQ4ZGI2YWQw/NjQ0M2Q5YWFhMTM4/MDljYjI0OS9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy8", "path": "\u203a html  \u203a 2006.10214"}, "thumbnail": {"src": "https://imgs.search.brave.com/5Tsbn2z8zn03LO10F1g1N0_ns-eYv5CwATFUyJ5Qpdc/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy9hc3NldHMv/YXI1aXZfY2FyZC5w/bmc", "original": "https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png", "logo": false}, "age": "March 2, 2024"}, {"title": "(PDF) Hand tracking for clinical applications: validation of the Google MediaPipe Hand (GMH) and the depth-enhanced GMH-D frameworks", "url": "https://www.researchgate.net/publication/372858609_Hand_tracking_for_clinical_applications_validation_of_the_Google_MediaPipe_Hand_GMH_and_the_depth-enhanced_GMH-D_frameworks", "is_source_local": false, "is_source_both": false, "description": "tracking framework implemented by Google MediaPipe Hand (GMH) and an innovative enhanced \u00b7 version, GMH-D, that exploits the depth estimation of an RGB-Depth camera to achieve more \u00b7 accurate tracking of 3D movements. Three dynamic exercises commonly administered by clinicians \u00b7 to assess hand dysfunctions, namely Hand Opening-Closing, Single Finger Tapping and Multiple \u00b7 Finger Tapping are considered. Results demonstrate high temporal and spectral consistency of both", "page_age": "2023-08-02T00:00:00", "profile": {"name": "ResearchGate", "url": "https://www.researchgate.net/publication/372858609_Hand_tracking_for_clinical_applications_validation_of_the_Google_MediaPipe_Hand_GMH_and_the_depth-enhanced_GMH-D_frameworks", "long_name": "researchgate.net", "img": "https://imgs.search.brave.com/WJ25-tL-91vp8kSoF0YS7d8CZVivP1cG-EjlGvrmxOc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjE0MDk0Yzli/MWQ3NGQ2ZTNjODVh/YWVmMDNkM2NkZWI5/NGEzNjdhMmEyY2E2/N2EzN2Y0OThlZjdi/YWU3MjkwMi93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "researchgate.net", "hostname": "www.researchgate.net", "favicon": "https://imgs.search.brave.com/WJ25-tL-91vp8kSoF0YS7d8CZVivP1cG-EjlGvrmxOc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjE0MDk0Yzli/MWQ3NGQ2ZTNjODVh/YWVmMDNkM2NkZWI5/NGEzNjdhMmEyY2E2/N2EzN2Y0OThlZjdi/YWU3MjkwMi93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC8", "path": "\u203a publication  \u203a 372858609_Hand_tracking_for_clinical_applications_validation_of_the_Google_MediaPipe_Hand_GMH_and_the_depth-enhanced_GMH-D_frameworks"}, "thumbnail": {"src": "https://imgs.search.brave.com/L_ETLAhdQTE59HvZryY6Jc3abJPmoVzc3-Tj7iCYmU8/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pMS5y/Z3N0YXRpYy5uZXQv/cHVibGljYXRpb24v/MzcyODU4NjA5X0hh/bmRfdHJhY2tpbmdf/Zm9yX2NsaW5pY2Fs/X2FwcGxpY2F0aW9u/c192YWxpZGF0aW9u/X29mX3RoZV9Hb29n/bGVfTWVkaWFQaXBl/X0hhbmRfR01IX2Fu/ZF90aGVfZGVwdGgt/ZW5oYW5jZWRfR01I/LURfZnJhbWV3b3Jr/cy9saW5rcy82NGNi/MTllYzkxZmIwMzZi/YTZiZmM4ZDUvbGFy/Z2VwcmV2aWV3LnBu/Zw", "original": "https://i1.rgstatic.net/publication/372858609_Hand_tracking_for_clinical_applications_validation_of_the_Google_MediaPipe_Hand_GMH_and_the_depth-enhanced_GMH-D_frameworks/links/64cb19ec91fb036ba6bfc8d5/largepreview.png", "logo": false}, "age": "August 2, 2023"}, {"title": "Hand tracking for clinical applications: Validation of the Google MediaPipe Hand (GMH) and the depth-enhanced GMH-D frameworks - ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1746809424005664", "is_source_local": false, "is_source_both": false, "description": "In this paper, the aim is to validate the hand-tracking framework implemented by Google MediaPipe Hand (GMH) and an innovative enhanced version, GMH-D, that exploits the depth estimation of an RGB-Depth camera to achieve more accurate tracking of 3D movements. Three dynamic exercises commonly administered by clinicians to assess hand dysfunctions, namely hand opening\u2013closing, single finger tapping and multiple finger tapping are considered. Results demonstrate high temporal and spectral consistency of both frameworks with the gold standard.", "page_age": "2024-06-04T00:00:00", "profile": {"name": "ScienceDirect", "url": "https://www.sciencedirect.com/science/article/pii/S1746809424005664", "long_name": "sciencedirect.com", "img": "https://imgs.search.brave.com/NJbb0P2bwbdnGnD5FijFoBbn5rA0Jwz-yK3fG5QOBng/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMjc5YWUwYmQ5/YWNmYWRiMDAxZWU5/ODc3NjJmYTVmOTNi/ZjRmYmE2OTk5Zjk2/MTUzZjMwZmE5OTQ5/MWFjY2ZjNS93d3cu/c2NpZW5jZWRpcmVj/dC5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "sciencedirect.com", "hostname": "www.sciencedirect.com", "favicon": "https://imgs.search.brave.com/NJbb0P2bwbdnGnD5FijFoBbn5rA0Jwz-yK3fG5QOBng/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMjc5YWUwYmQ5/YWNmYWRiMDAxZWU5/ODc3NjJmYTVmOTNi/ZjRmYmE2OTk5Zjk2/MTUzZjMwZmE5OTQ5/MWFjY2ZjNS93d3cu/c2NpZW5jZWRpcmVj/dC5jb20v", "path": "\u203a science  \u203a article  \u203a pii  \u203a S1746809424005664"}, "thumbnail": {"src": "https://imgs.search.brave.com/cyI-tyIfZO9yO8t6EIXtb7HtH1xYwssI2ZLEyrZcuzk/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcnMu/ZWxzLWNkbi5jb20v/Y29udGVudC9pbWFn/ZS8xLXMyLjAtUzE3/NDY4MDk0MjRYMDAw/ODctY292MTUwaC5n/aWY.jpeg", "original": "https://ars.els-cdn.com/content/image/1-s2.0-S1746809424X00087-cov150h.gif", "logo": false}, "age": "June 4, 2024"}, {"title": "Hand Gesture Mapping Using MediaPipe Algorithm | SpringerLink", "url": "https://link.springer.com/chapter/10.1007/978-981-16-8862-1_39", "is_source_local": false, "is_source_both": false, "description": "Once the user has completed the setup of his or her gestures, he or she can request the Python code that was created for that configuration. This paper provides the ability to pick any one of the workstation GUI tasks and assign motions to it, as well as the reverse of this functionality.", "profile": {"name": "Springer", "url": "https://link.springer.com/chapter/10.1007/978-981-16-8862-1_39", "long_name": "link.springer.com", "img": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "link.springer.com", "hostname": "link.springer.com", "favicon": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8", "path": "  \u203a home  \u203a proceedings of third international conference on communication, computing and electronics systems  \u203a conference paper"}, "thumbnail": {"src": "https://imgs.search.brave.com/rdKOmGJHkXAowWNI03n9wxbAwO3VkWZPCeHc1c-kLm8/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9tZWRp/YS5zcHJpbmdlcm5h/dHVyZS5jb20vdzE1/My9zcHJpbmdlci1z/dGF0aWMvY292ZXIv/Ym9vay85NzgtOTgx/LTE2LTg4NjItMS5q/cGc", "original": "https://media.springernature.com/w153/springer-static/cover/book/978-981-16-8862-1.jpg", "logo": false}}, {"title": "GitHub - geaxgx/depthai_hand_tracker: Running Google Mediapipe Hand Tracking models on Luxonis DepthAI hardware (OAK-D-lite, OAK-D, OAK-1,...)", "url": "https://github.com/geaxgx/depthai_hand_tracker", "is_source_local": false, "is_source_both": false, "description": "First, the Hand detection stage detects where are the hands in the whole image. <strong>For each detected hand, a Region of Interest (ROI) around the hand is calculated and fed to the second stage, which infers the landmarks</strong>.", "profile": {"name": "GitHub", "url": "https://github.com/geaxgx/depthai_hand_tracker", "long_name": "github.com", "img": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "software", "is_live": false, "meta_url": {"scheme": "https", "netloc": "github.com", "hostname": "github.com", "favicon": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw", "path": "\u203a geaxgx  \u203a depthai_hand_tracker"}, "thumbnail": {"src": "https://imgs.search.brave.com/DVvGW1EDkApeq8PakD8kp0LpWRiuMyrxa_it-fvVL5E/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9vcGVu/Z3JhcGguZ2l0aHVi/YXNzZXRzLmNvbS8w/ZTUwOGIyMDljZTdm/Zjg2MDk3ZDg5ODA2/YjgzMDU5MDRhMzhi/MzlhZDM3ODlhMDhi/MGQ2NWVkMjk3NTFh/NjM0L2dlYXhneC9k/ZXB0aGFpX2hhbmRf/dHJhY2tlcg", "original": "https://opengraph.githubassets.com/0e508b209ce7ff86097d89806b8305904a38b39ad3789a08b0d65ed29751a634/geaxgx/depthai_hand_tracker", "logo": false}}, {"title": "MediaPipe Hands: On-device Real-time Hand Tracking", "url": "https://www.researchgate.net/publication/342302340_MediaPipe_Hands_On-device_Real-time_Hand_Tracking", "is_source_local": false, "is_source_both": false, "description": "Penelitian ini bertujuan mengevaluasi pendekatan klasifikasi huruf SIBI berbasis urutan frame landmark tangan yang diperoleh dari MediaPipe Hands. Data sekuensial hasil ekstraksi digunakan sebagai input model Long Short-Term Memory (LSTM) untuk mengenali pola temporal dari pergerakan tangan.", "page_age": "2020-06-17T00:00:00", "profile": {"name": "ResearchGate", "url": "https://www.researchgate.net/publication/342302340_MediaPipe_Hands_On-device_Real-time_Hand_Tracking", "long_name": "researchgate.net", "img": "https://imgs.search.brave.com/WJ25-tL-91vp8kSoF0YS7d8CZVivP1cG-EjlGvrmxOc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjE0MDk0Yzli/MWQ3NGQ2ZTNjODVh/YWVmMDNkM2NkZWI5/NGEzNjdhMmEyY2E2/N2EzN2Y0OThlZjdi/YWU3MjkwMi93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "researchgate.net", "hostname": "www.researchgate.net", "favicon": "https://imgs.search.brave.com/WJ25-tL-91vp8kSoF0YS7d8CZVivP1cG-EjlGvrmxOc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjE0MDk0Yzli/MWQ3NGQ2ZTNjODVh/YWVmMDNkM2NkZWI5/NGEzNjdhMmEyY2E2/N2EzN2Y0OThlZjdi/YWU3MjkwMi93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC8", "path": "\u203a publication  \u203a 342302340_MediaPipe_Hands_On-device_Real-time_Hand_Tracking"}, "thumbnail": {"src": "https://imgs.search.brave.com/NA3UWQdok5rp6uI2kyF0cHLVZy0QBikNBsFv8yDMrrc/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC9pbWFnZXMv/dGVtcGxhdGUvZGVm/YXVsdF9wdWJsaWNh/dGlvbl9wcmV2aWV3/X2xhcmdlLnBuZw", "original": "https://www.researchgate.net/images/template/default_publication_preview_large.png", "logo": false}, "age": "June 17, 2020"}, {"title": "MediaPipe Hands: On-device Real-time Hand Tracking", "url": "https://research.google/pubs/mediapipe-hands-on-device-real-time-hand-tracking/", "is_source_local": false, "is_source_both": false, "description": "<strong>We present a real-time on-device hand tracking pipeline that predicts hand skeleton from only single camera input for AR/VR applications</strong>. The pipeline consists of two models: 1) a palm detector, 2) a hand landmark prediction.", "profile": {"name": "Google Research", "url": "https://research.google/pubs/mediapipe-hands-on-device-real-time-hand-tracking/", "long_name": "research.google", "img": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "research.google", "hostname": "research.google", "favicon": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv", "path": "\u203a pubs  \u203a mediapipe-hands-on-device-real-time-hand-tracking"}, "thumbnail": {"src": "https://imgs.search.brave.com/xMSx6K-DNGKBD00GCYX2R_H6N036h6fg9HWvNmYR3vs/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdG9y/YWdlLmdvb2dsZWFw/aXMuY29tL2d3ZWIt/cmVzZWFyY2gyMDIz/LW1lZGlhL2ltYWdl/cy9PcGVuX0dyYXBo/LndpZHRoLTgwMC5m/b3JtYXQtanBlZy5q/cGc", "original": "https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg", "logo": false}}, {"title": "GitHub - Sousannah/hand-tracking-using-mediapipe", "url": "https://github.com/Sousannah/hand-tracking-using-mediapipe", "is_source_local": false, "is_source_both": false, "description": "<strong>It utilizes MediaPipe&#x27;s Hands model to detect hand landmarks in each frame</strong>.", "profile": {"name": "GitHub", "url": "https://github.com/Sousannah/hand-tracking-using-mediapipe", "long_name": "github.com", "img": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "software", "is_live": false, "meta_url": {"scheme": "https", "netloc": "github.com", "hostname": "github.com", "favicon": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw", "path": "\u203a Sousannah  \u203a hand-tracking-using-mediapipe"}, "thumbnail": {"src": "https://imgs.search.brave.com/kHjr0rVafuQUjJ26cLfjvt6cB4nru6WRF5Uby_bruWM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9vcGVu/Z3JhcGguZ2l0aHVi/YXNzZXRzLmNvbS9k/MGQ3MDczN2E0NGIz/MzBjYTA5ZTY3YzI4/MWU0YWU5OWE3YzA2/MTQ2MzEyYTBlMzRk/YmYwOTg1ZjliMGFl/YjY1L1NvdXNhbm5h/aC9oYW5kLXRyYWNr/aW5nLXVzaW5nLW1l/ZGlhcGlwZQ", "original": "https://opengraph.githubassets.com/d0d70737a44b330ca09e67c281e4ae99a7c06146312a0e34dbf0985f9b0aeb65/Sousannah/hand-tracking-using-mediapipe", "logo": false}}, {"title": "MediaPipe Hands: Real-Time Tracking", "url": "https://www.emergentmind.com/topics/mediapipe-hands", "is_source_local": false, "is_source_both": false, "description": "Temporal Tracking and Filtering: For video-based processing, <strong>intersection-over-union (IoU) matching, Kalman filters, or exponential smoothing</strong> can be used to maintain ID continuity and suppress outliers during rapid articulation and occlusion.", "profile": {"name": "Emergent Mind", "url": "https://www.emergentmind.com/topics/mediapipe-hands", "long_name": "emergentmind.com", "img": "https://imgs.search.brave.com/Zc521AqQwkFLu4dxf7NKn1ehs161lCdjAkhkjS4ySGI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGM4NjVhOTE1/YjRiZTJmNWRlOTZj/ZjRhZmYxZWMwMWFj/ZDAzNTk0Y2MxZDc1/MTBiYWJmODlmZTE1/N2EwMWUxZS93d3cu/ZW1lcmdlbnRtaW5k/LmNvbS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "emergentmind.com", "hostname": "www.emergentmind.com", "favicon": "https://imgs.search.brave.com/Zc521AqQwkFLu4dxf7NKn1ehs161lCdjAkhkjS4ySGI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGM4NjVhOTE1/YjRiZTJmNWRlOTZj/ZjRhZmYxZWMwMWFj/ZDAzNTk0Y2MxZDc1/MTBiYWJmODlmZTE1/N2EwMWUxZS93d3cu/ZW1lcmdlbnRtaW5k/LmNvbS8", "path": "\u203a topics  \u203a mediapipe-hands"}, "thumbnail": {"src": "https://imgs.search.brave.com/K8jnmZPNdMeWoSa5EXkTOTXDUqE7xJulK0_vb-xBIyk/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9kMnpr/OHFkeDJ5MWJlci5j/bG91ZGZyb250Lm5l/dC9hc3NldHMvNjAw/cHgtYTYxMzIwNDQ5/YjhjODQ4YmNjNTZl/MDI4MjZiMDMzYjVl/ZmEwOWZhNTJjYzQ4/YjQ4N2U4ZjlhMmVi/OGVmZDcwNS5wbmc", "original": "https://d2zk8qdx2y1ber.cloudfront.net/assets/600px-a61320449b8c848bcc56e02826b033b5efa09fa52cc48b487e8f9a2eb8efd705.png", "logo": false}}, {"title": "[PDF] MediaPipe Hands: On-device Real-time Hand Tracking | Semantic Scholar", "url": "https://www.semanticscholar.org/paper/MediaPipe-Hands:-On-device-Real-time-Hand-Tracking-Zhang-Bazarevsky/84b19524609ad75f309be7f87bcea783e6ecd337", "is_source_local": false, "is_source_both": false, "description": "<strong>This paper presents a simple but powerful algorithm based on the MediaPipe Hands solution</strong>, a highly optimized neural network that processes the landmarks provided by MediaP Pipe using morphological and logical operators to obtain the masks that ...", "profile": {"name": "Semantic Scholar", "url": "https://www.semanticscholar.org/paper/MediaPipe-Hands:-On-device-Real-time-Hand-Tracking-Zhang-Bazarevsky/84b19524609ad75f309be7f87bcea783e6ecd337", "long_name": "semanticscholar.org", "img": "https://imgs.search.brave.com/K-sZ3Vgcj9LFjkcTUpjjK40LWCyX6WutMxHJiE7XYPI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGI3NTZiOWVj/YTE5N2MzYTNhYzA2/NWEyMzQwMzA5N2Yy/NGY3MjhhYTcxYmI2/YWQ3NjdjNDk5NGE2/NmUyYTdlYi93d3cu/c2VtYW50aWNzY2hv/bGFyLm9yZy8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "semanticscholar.org", "hostname": "www.semanticscholar.org", "favicon": "https://imgs.search.brave.com/K-sZ3Vgcj9LFjkcTUpjjK40LWCyX6WutMxHJiE7XYPI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGI3NTZiOWVj/YTE5N2MzYTNhYzA2/NWEyMzQwMzA5N2Yy/NGY3MjhhYTcxYmI2/YWQ3NjdjNDk5NGE2/NmUyYTdlYi93d3cu/c2VtYW50aWNzY2hv/bGFyLm9yZy8", "path": "  \u203a papers  \u203a mediapipe hands: on-device real-time hand tracking"}, "thumbnail": {"src": "https://imgs.search.brave.com/A3fktYElQA3Sms9BB_yLvcb2tzI-8lZyB7LvUT1A-us/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/c2VtYW50aWNzY2hv/bGFyLm9yZy9pbWcv/c2VtYW50aWNfc2No/b2xhcl9vZy5wbmc", "original": "https://www.semanticscholar.org/img/semantic_scholar_og.png", "logo": false}}, {"title": "layout: forward target: https://developers.google.com/mediapipe/solutions/vision/hand_landmarker title: Hands parent: MediaPipe Legacy Solutions nav_order: 4 \u2014 MediaPipe v0.7.5 documentation", "url": "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html", "is_source_local": false, "is_source_both": false, "description": "MediaPipe Hands is a high-fidelity hand and finger tracking solution. It employs machine learning (ML) to infer 21 3D landmarks of a hand from just a single frame.", "profile": {"name": "MediaPipe", "url": "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html", "long_name": "mediapipe.readthedocs.io", "img": "https://imgs.search.brave.com/siwVUd3xwHy6eFN_ZEcFH24I0GMOr2jEBCS52tTwxfc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYzkwNDIzZDAx/Mzg0MTVmODVmMTEz/MDEyMTljOGE2N2Q5/ZDliODY2YzA2MDRk/MDhkYjZkMDI2OTlh/YjBiNjMwYi9tZWRp/YXBpcGUucmVhZHRo/ZWRvY3MuaW8v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "mediapipe.readthedocs.io", "hostname": "mediapipe.readthedocs.io", "favicon": "https://imgs.search.brave.com/siwVUd3xwHy6eFN_ZEcFH24I0GMOr2jEBCS52tTwxfc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYzkwNDIzZDAx/Mzg0MTVmODVmMTEz/MDEyMTljOGE2N2Q5/ZDliODY2YzA2MDRk/MDhkYjZkMDI2OTlh/YjBiNjMwYi9tZWRp/YXBpcGUucmVhZHRo/ZWRvY3MuaW8v", "path": "\u203a en  \u203a latest  \u203a solutions  \u203a hands.html"}}, {"title": "Real-Time Hand Gesture Monitoring Model Based on MediaPipe\u2019s Registerable System", "url": "https://www.mdpi.com/1424-8220/24/19/6262", "is_source_local": false, "is_source_both": false, "description": "<strong>Utilize the open-source MediaPipe provided by Google to obtain the position and coordinate points of the hand</strong>. This step helps us identify and locate the hand within the image. ... Apply geometric transformations to the coordinate points of ...", "page_age": "2024-09-27T00:00:00", "profile": {"name": "MDPI", "url": "https://www.mdpi.com/1424-8220/24/19/6262", "long_name": "mdpi.com", "img": "https://imgs.search.brave.com/IDkOdEuGDr9XA4H-36Lnou8wMT1nsDEHneEyTFddFmg/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOWQ5OGNkM2Uz/YWZhNTljYjhhZGVh/MTQyMzUxYTFmMjY3/NzhjMjEzMmI2M2Nm/YzllOTI4YmYwZDcw/MmE0NDY5MC93d3cu/bWRwaS5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "mdpi.com", "hostname": "www.mdpi.com", "favicon": "https://imgs.search.brave.com/IDkOdEuGDr9XA4H-36Lnou8wMT1nsDEHneEyTFddFmg/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOWQ5OGNkM2Uz/YWZhNTljYjhhZGVh/MTQyMzUxYTFmMjY3/NzhjMjEzMmI2M2Nm/YzllOTI4YmYwZDcw/MmE0NDY5MC93d3cu/bWRwaS5jb20v", "path": "\u203a 1424-8220  \u203a 24  \u203a 19  \u203a 6262"}, "thumbnail": {"src": "https://imgs.search.brave.com/acTHTK23fVpOxynGAJpgf609pzthcWKSigtX1TuSXZE/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9wdWIu/bWRwaS1yZXMuY29t/L3NlbnNvcnMvc2Vu/c29ycy0yNC0wNjI2/Mi9hcnRpY2xlX2Rl/cGxveS9odG1sL2lt/YWdlcy9zZW5zb3Jz/LTI0LTA2MjYyLWcw/MDEtNTUwLmpwZz8x/NzI3NDMyMjg5", "original": "https://pub.mdpi-res.com/sensors/sensors-24-06262/article_deploy/html/images/sensors-24-06262-g001-550.jpg?1727432289", "logo": false}, "age": "September 27, 2024"}, {"title": "Simple Hand Gesture Recognition Code - Hand tracking - Mediapipe \u00b7 GitHub", "url": "https://gist.github.com/TheJLifeX/74958cc59db477a91837244ff598ef4a", "is_source_local": false, "is_source_both": false, "description": "So the logic would be something like <strong>if the detected hand is left, flip the positions of the fingers then the detection algorithm will work</strong>. I am still trying to work out how to access this information.", "profile": {"name": "GitHub", "url": "https://gist.github.com/TheJLifeX/74958cc59db477a91837244ff598ef4a", "long_name": "gist.github.com", "img": "https://imgs.search.brave.com/xZ1QeMRUaD4Rb2PRlguFbHNXkQKQJMGrNr6XEBVeFkc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzdmZjEzNDE3/NDlmMDc4NDRlMGJl/YTdmYzUxNGNkNDdk/OGE5YmRiOWQ0NGVl/NDU4MzZlNzg3OTYy/M2YzOTZiMS9naXN0/LmdpdGh1Yi5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "gist.github.com", "hostname": "gist.github.com", "favicon": "https://imgs.search.brave.com/xZ1QeMRUaD4Rb2PRlguFbHNXkQKQJMGrNr6XEBVeFkc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzdmZjEzNDE3/NDlmMDc4NDRlMGJl/YTdmYzUxNGNkNDdk/OGE5YmRiOWQ0NGVl/NDU4MzZlNzg3OTYy/M2YzOTZiMS9naXN0/LmdpdGh1Yi5jb20v", "path": "\u203a TheJLifeX  \u203a 74958cc59db477a91837244ff598ef4a"}, "thumbnail": {"src": "https://imgs.search.brave.com/SvZpKSOuyeHiaTd9TiuyNSGCmnZZ9hRjhW4L1NQKx-4/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9yYXcu/Z2l0aHVidXNlcmNv/bnRlbnQuY29tL2dp/c3QvVGhlSkxpZmVY/Lzc0OTU4Y2M1OWRi/NDc3YTkxODM3MjQ0/ZmY1OThlZjRhL3Jh/dy8wODhmMzk5NTgw/MWM1OGY3OWYwYTc5/MDg2ZjFjZDRjYzE3/NjM5NmQzLzAwLWhh/bmQtZ2VzdHVyZS1y/ZWNvZ25pdGlvbi5n/aWY.jpeg", "original": "https://raw.githubusercontent.com/gist/TheJLifeX/74958cc59db477a91837244ff598ef4a/raw/088f3995801c58f79f0a79086f1cd4cc176396d3/00-hand-gesture-recognition.gif", "logo": false}}, {"title": "RECOGNIZATION OF HAND GESTURES USING MEDIAPIPE ...", "url": "https://www.irjmets.com/uploadedfiles/paper/issue_6_june_2022/27011/final/fin_irjmets1656344520.pdf", "is_source_local": false, "is_source_both": false, "description": "<strong>MediaPipe library reduces the amount of computation by using a smaller detection range</strong>.", "profile": {"name": "IRJMETS", "url": "https://www.irjmets.com/uploadedfiles/paper/issue_6_june_2022/27011/final/fin_irjmets1656344520.pdf", "long_name": "irjmets.com", "img": "https://imgs.search.brave.com/xu2DUOViurqF6DS3-sJ387zxbO1xH4652csLycLTJvM/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNTUyNWRmZjQw/NDQ4YjJhYzcyM2Ji/ZDkxYjgwZWJkZjk1/YzY2OTEzMDNhNTNk/ZWQ0YTJmZGI2MmRl/YWMxNTAyOS93d3cu/aXJqbWV0cy5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "irjmets.com", "hostname": "www.irjmets.com", "favicon": "https://imgs.search.brave.com/xu2DUOViurqF6DS3-sJ387zxbO1xH4652csLycLTJvM/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNTUyNWRmZjQw/NDQ4YjJhYzcyM2Ji/ZDkxYjgwZWJkZjk1/YzY2OTEzMDNhNTNk/ZWQ0YTJmZGI2MmRl/YWMxNTAyOS93d3cu/aXJqbWV0cy5jb20v", "path": "\u203a uploadedfiles  \u203a paper  \u203a issue_6_june_2022  \u203a 27011  \u203a final  \u203a fin_irjmets1656344520.pdf"}, "content_type": "pdf"}, {"title": "Lightweight real-time hand segmentation leveraging MediaPipe landmark detection | Virtual Reality", "url": "https://link.springer.com/article/10.1007/s10055-023-00858-0", "is_source_local": false, "is_source_both": false, "description": "Algorithm stages. a Input image. b MediaPipe hands output. c Proposed segmentation solution results ... Hand tracking methods deal with locating key features of the hand to provide a set of identifiable landmarks that are used to model the position of the palm and fingers.", "page_age": "2023-09-05T00:00:00", "profile": {"name": "Springer", "url": "https://link.springer.com/article/10.1007/s10055-023-00858-0", "long_name": "link.springer.com", "img": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "link.springer.com", "hostname": "link.springer.com", "favicon": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8", "path": "  \u203a home  \u203a virtual reality  \u203a article"}, "thumbnail": {"src": "https://imgs.search.brave.com/YjimeHdSCEBddF0JJWXQC2_UYBq1h7fFt-Iz138uqfM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdGF0/aWMtY29udGVudC5z/cHJpbmdlci5jb20v/aW1hZ2UvYXJ0JTNB/MTAuMTAwNyUyRnMx/MDA1NS0wMjMtMDA4/NTgtMC9NZWRpYU9i/amVjdHMvMTAwNTVf/MjAyM184NThfRmln/MV9IVE1MLmpwZw", "original": "https://static-content.springer.com/image/art%3A10.1007%2Fs10055-023-00858-0/MediaObjects/10055_2023_858_Fig1_HTML.jpg", "logo": false}, "age": "September 5, 2023"}], "repo_sense": []}}}
{"timestamp": "2026-01-10T17:47:08.781451Z", "phase": "H", "summary": "P0-Sense QUAD Hunt: MediaPipe hand tracking identity stability patterns minimizing latency in hand identity assignment alternatives to bipartite matching for multi-hand tracking simple temporal hand tracking algorithm", "p0": {"status": "complete", "query": "MediaPipe hand tracking identity stability patterns minimizing latency in hand identity assignment alternatives to bipartite matching for multi-hand tracking simple temporal hand tracking algorithm", "receipt": "P0_QUAD_SEARCH_20260110_174708", "data": {"web_tavily": [{"url": "https://www.emergentmind.com/topics/mediapipe-hands", "title": "MediaPipe Hands: Real-Time Tracking - Emergent Mind", "content": "MediaPipe Hands is an open-source, real-time hand and finger tracking solution originating from Google\u2019s MediaPipe framework, built to deliver precise, markerless multi-hand localization and articulated pose estimation in unconstrained scenarios. It is extensively used in computer vision, augmented reality, gesture-based interaction, sign language translation, and robotics. The system is distinguished by its lightweight pipeline architecture, combining fast palm detectors, regression-based keypoint estimators, and temporal filtering, suitable for large-scale deployment across mobile platforms.\n\n## 1. Architecture and Key Algorithms\n\nMediaPipe Hands employs a cascaded pipeline optimizing for both robustness and low-latency: [...] 2000 character limit reached\n\n# MediaPipe Hands: Real-Time Tracking\n\nUpdated 1 November 2025\n\n MediaPipe Hands is an open-source, real-time hand tracking system that uses a palm-centric detection pipeline and CNN-based landmark regression.\n The methodology integrates fast palm detection, regression-based keypoint estimation, and temporal filtering to ensure robust multi-hand tracking in dynamic environments.\n Practical applications include gesture control, AR/VR interactions, robotic teleoperation, and sign language translation on mobile and web platforms. [...] 3. Temporal Tracking and Filtering: For video-based processing, intersection-over-union (IoU) matching, Kalman filters, or exponential smoothing can be used to maintain ID continuity and suppress outliers during rapid articulation and occlusion. Block feature refinement, as in 3DCPN (Li et al., 2021), enhances discriminative robustness, although MediaPipe Hands typically opts for lightweight, residual-based filtering for speed.\n4. Multihand Pipeline: The system runs multiple palm/landmark pipelines in parallel, supporting up to sixteen hands per frame in theory, although practical device limitations often cap performance at two hands in real time.", "score": 0.609455, "raw_content": null}, {"url": "https://arxiv.org/html/2512.14746v1", "title": "BlindSpot: Enabling Bystander-Controlled Privacy Signaling for ...", "content": "Hand Detection and Tracking. To recognize a dynamic gesture, the system must first establish a stable identity for a hand across multiple frames. The pipeline begins by processing the video stream with the MediaPipe Hand Landmarker [mediapipe\\_hand], which provides the 21 normalized 3D landmarks for each detected hand. These raw detections are then passed to our HandTracker component, which assigns a persistent integer ID to each hand by implementing a proximity-based matching algorithm. For each existing track, it calculates the Euclidean distance between its wrist landmark from the previous frame and the wrist landmarks of all new detections in the current frame. A match is confirmed if the distance is below an empirically determined threshold (maxDistance). New IDs are assigned to any", "score": 0.54521364, "raw_content": null}, {"url": "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html", "title": "MediaPipe Hands - Read the Docs", "content": "## Solution APIs\u00b6\n\n### Configuration Options\u00b6\n\nNaming style and availability may differ slightly across platforms/languages.\n\n#### static\\_image\\_mode\u00b6\n\nIf set to `false`, the solution treats the input images as a video stream. It will try to detect hands in the first input images, and upon a successful detection further localizes the hand landmarks. In subsequent images, once all max\\_num\\_hands hands are detected and the corresponding hand landmarks are localized, it simply tracks those landmarks without invoking another detection until it loses track of any of the hands. This reduces latency and is ideal for processing video frames. If set to `true`, hand detection runs on every input image, ideal for processing a batch of static, possibly unrelated, images. Default to `false`. [...] #### min\\_tracking\\_confidence:\u00b6\n\nMinimum confidence value (`[0.0,1.0]`) from the landmark-tracking model for the hand landmarks to be considered tracked successfully, or otherwise hand detection will be invoked automatically on the next input image. Setting it to a higher value can increase robustness of the solution, at the expense of a higher latency. Ignored if static\\_image\\_mode is `true`, where hand detection simply runs on every image. Default to `0.5`.\n\n### Output\u00b6\n\nNaming style may differ slightly across platforms/languages.\n\n#### multi\\_hand\\_landmarks\u00b6 [...] #### max\\_num\\_hands\u00b6\n\nMaximum number of hands to detect. Default to `2`.\n\n#### model\\_complexity\u00b6\n\nComplexity of the hand landmark model: `0` or `1`. Landmark accuracy as well as inference latency generally go up with the model complexity. Default to `1`.\n\n#### min\\_detection\\_confidence\u00b6\n\nMinimum confidence value (`[0.0,1.0]`) from the hand detection model for the detection to be considered successful. Default to `0.5`.\n\n#### min\\_tracking\\_confidence:\u00b6", "score": 0.52962893, "raw_content": null}, {"url": "https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/", "title": "On-Device, Real-Time Hand Tracking with MediaPipe", "content": "Future Directions   \n We plan to extend this technology with more robust and stable tracking, enlarge the amount of gestures we can reliably detect, and support dynamic gestures unfolding in time. We believe that publishing this technology can give an impulse to new creative ideas and applications by the members of the research and developer community at large. We are excited to see what you can build with it!   \nAcknowledgements [...] Our MediaPipe graph for hand tracking is shown below. The graph consists of two subgraphs\u2014one for hand detection and one for hand keypoints (i.e., landmark) computation. One key optimization MediaPipe provides is that the palm detector is only run as necessary (fairly infrequently), saving significant computation time. We achieve this by inferring the hand location in the subsequent video frames from the computed hand key points in the current frame, eliminating the need to run the palm detector over each frame. For robustness, the hand tracker model outputs an additional scalar capturing the confidence that a hand is present and reasonably aligned in the input crop. Only when the confidence falls below a certain threshold is the hand detection model reapplied to the whole frame. [...] |  |\n\n| The hand landmark model\u2019s output (REJECT\\_HAND\\_FLAG) controls when the hand detection model is triggered. This behavior is achieved by MediaPipe\u2019s powerful synchronization building blocks, resulting in high performance and optimal throughput of the ML pipeline. |", "score": 0.40740472, "raw_content": null}, {"url": "https://arxiv.org/abs/2006.10214", "title": "MediaPipe Hands: On-device Real-time Hand Tracking - arXiv", "content": "|  |  |\n --- |\n| Comments: | 5 pages, 7 figures; CVPR Workshop on Computer Vision for Augmented and Virtual Reality, Seattle, WA, USA, 2020 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | arXiv:2006.10214 [cs.CV] |\n|  | (or  arXiv:2006.10214v1 [cs.CV] for this version) |\n|  |  arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Valentin Bazarevsky [view email]   \n [v1] Thu, 18 Jun 2020 00:19:13 UTC (2,262 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MediaPipe Hands: On-device Real-time Hand Tracking, by Fan Zhang and 6 other authors\n\n View PDF\n TeX Source\n\nview license\n\nCurrent browse context:\n\ncs.CV\n\n< prev\")    |    next >\")\n\nnew  |  recent  | 2020-06\n\nChange to browse by:\n\ncs\n\n### References & Citations [...] We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\n\n> cs > arXiv:2006.10214\n\n# Computer Science > Computer Vision and Pattern Recognition\n\narXiv:2006.10214 (cs)\n\n[Submitted on 18 Jun 2020]\n\n# Title:MediaPipe Hands: On-device Real-time Hand Tracking\n\nAuthors:Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka, George Sung, Chuo-Ling Chang, Matthias Grundmann\n\nView a PDF of the paper titled MediaPipe Hands: On-device Real-time Hand Tracking, by Fan Zhang and 6 other authors [...] View PDF\n> Abstract:We present a real-time on-device hand tracking pipeline that predicts hand skeleton from single RGB camera for AR/VR applications. The pipeline consists of two models: 1) a palm detector, 2) a hand landmark model. It's implemented via MediaPipe, a framework for building cross-platform ML solutions. The proposed model and pipeline architecture demonstrates real-time inference speed on mobile GPUs and high prediction quality. MediaPipe Hands is open sourced at this https URL.", "score": 0.34052956, "raw_content": null}], "web_brave": [{"title": "layout: forward target: https://developers.google.com/mediapipe/solutions/vision/hand_landmarker title: Hands parent: MediaPipe Legacy Solutions nav_order: 4 \u2014 MediaPipe v0.7.5 documentation", "url": "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html", "is_source_local": false, "is_source_both": false, "description": "Complexity of the hand landmark model: 0 or 1. Landmark accuracy as well as inference latency generally go up with the model complexity. Default to 1. Minimum confidence value ([0.0, 1.0]) from the hand detection model for the detection to be considered successful.", "profile": {"name": "MediaPipe", "url": "https://mediapipe.readthedocs.io/en/latest/solutions/hands.html", "long_name": "mediapipe.readthedocs.io", "img": "https://imgs.search.brave.com/siwVUd3xwHy6eFN_ZEcFH24I0GMOr2jEBCS52tTwxfc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYzkwNDIzZDAx/Mzg0MTVmODVmMTEz/MDEyMTljOGE2N2Q5/ZDliODY2YzA2MDRk/MDhkYjZkMDI2OTlh/YjBiNjMwYi9tZWRp/YXBpcGUucmVhZHRo/ZWRvY3MuaW8v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "mediapipe.readthedocs.io", "hostname": "mediapipe.readthedocs.io", "favicon": "https://imgs.search.brave.com/siwVUd3xwHy6eFN_ZEcFH24I0GMOr2jEBCS52tTwxfc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYzkwNDIzZDAx/Mzg0MTVmODVmMTEz/MDEyMTljOGE2N2Q5/ZDliODY2YzA2MDRk/MDhkYjZkMDI2OTlh/YjBiNjMwYi9tZWRp/YXBpcGUucmVhZHRo/ZWRvY3MuaW8v", "path": "\u203a en  \u203a latest  \u203a solutions  \u203a hands.html"}}, {"title": "[2006.10214] MediaPipe Hands: On-device Real-time Hand Tracking", "url": "https://ar5iv.labs.arxiv.org/html/2006.10214", "is_source_local": false, "is_source_both": false, "description": "This behavior is achieved by MediaPipe\u2019s powerful synchronization building blocks, resulting in high performance and optimal throughput of the ML pipeline. Our hand tracking solution can readily be used in many applications such as gesture recognition and AR effects. On top of the predicted hand skeleton, we employ a simple algorithm to compute gestures, see Figure 6.", "page_age": "2024-03-02T11:16:50", "profile": {"name": "arXiv", "url": "https://ar5iv.labs.arxiv.org/html/2006.10214", "long_name": "ar5iv.labs.arxiv.org", "img": "https://imgs.search.brave.com/Ra63-x8TeB8HQ-I2dv-lRN_mIzQ3JQGTcfoprRnqdaI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGEzNGUyYWVl/NDk2ODQ0MzYwZjhk/ZGRkOGIyNWVkN2Qz/ZWZiMjQ4ZGI2YWQw/NjQ0M2Q5YWFhMTM4/MDljYjI0OS9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "ar5iv.labs.arxiv.org", "hostname": "ar5iv.labs.arxiv.org", "favicon": "https://imgs.search.brave.com/Ra63-x8TeB8HQ-I2dv-lRN_mIzQ3JQGTcfoprRnqdaI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGEzNGUyYWVl/NDk2ODQ0MzYwZjhk/ZGRkOGIyNWVkN2Qz/ZWZiMjQ4ZGI2YWQw/NjQ0M2Q5YWFhMTM4/MDljYjI0OS9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy8", "path": "\u203a html  \u203a 2006.10214"}, "thumbnail": {"src": "https://imgs.search.brave.com/5Tsbn2z8zn03LO10F1g1N0_ns-eYv5CwATFUyJ5Qpdc/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy9hc3NldHMv/YXI1aXZfY2FyZC5w/bmc", "original": "https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png", "logo": false}, "age": "March 2, 2024"}, {"title": "On-Device, Real-Time Hand Tracking with MediaPipe", "url": "https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/", "is_source_local": false, "is_source_both": false, "description": "In addition, as palms are smaller objects, the non-maximum suppression algorithm works well even for two-hand self-occlusion cases, like handshakes. Moreover, palms can be modelled using square bounding boxes (anchors in ML terminology) ignoring other aspect ratios, and therefore reducing the number of anchors by a factor of 3-5. Second, an encoder-decoder feature extractor is used for bigger scene context awareness even for small objects (similar to the RetinaNet approach). Lastly, we minimize the focal loss during training to support a large amount of anchors resulting from the high scale variance.", "profile": {"name": "Google Research", "url": "https://research.google/blog/on-device-real-time-hand-tracking-with-mediapipe/", "long_name": "research.google", "img": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "research.google", "hostname": "research.google", "favicon": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv", "path": "\u203a blog  \u203a on-device-real-time-hand-tracking-with-mediapipe"}, "thumbnail": {"src": "https://imgs.search.brave.com/Ww80QDOXmgj8yDED_u8edBEb9uOb2U_NaHeT6DId_Io/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdG9y/YWdlLmdvb2dsZWFw/aXMuY29tL2d3ZWIt/cmVzZWFyY2gyMDIz/LW1lZGlhL2ltYWdl/cy9lM2VmMDk1Njhm/ZTZiMDlkMGI5Mzc3/MmQzZjAyZGJmMy1p/LndpZHRoLTgwMC5m/b3JtYXQtanBlZy5q/cGc", "original": "https://storage.googleapis.com/gweb-research2023-media/images/e3ef09568fe6b09d0b93772d3f02dbf3-i.width-800.format-jpeg.jpg", "logo": false}}, {"title": "[2006.10214] MediaPipe Hands: On-device Real-time Hand Tracking", "url": "https://arxiv.org/abs/2006.10214", "is_source_local": false, "is_source_both": false, "description": "We present <strong>a real-time on-device hand tracking pipeline that predicts hand skeleton from single RGB camera for AR/VR applications</strong>. The pipeline consists of two models: 1) a palm detector, 2) a hand landmark model.", "page_age": "2020-06-18T00:00:00", "profile": {"name": "arXiv", "url": "https://arxiv.org/abs/2006.10214", "long_name": "arxiv.org", "img": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "arxiv.org", "hostname": "arxiv.org", "favicon": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv", "path": "\u203a abs  \u203a 2006.10214"}, "thumbnail": {"src": "https://imgs.search.brave.com/iKdq2fkHaKJYPDvdK7fP7YT23ZvBXz_aBGM5hD9_jho/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcnhp/di5vcmcvc3RhdGlj/L2Jyb3dzZS8wLjMu/NC9pbWFnZXMvYXJ4/aXYtbG9nby1mYi5w/bmc", "original": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png", "logo": true}, "age": "June 18, 2020"}, {"title": "MediaPipe Hands: Real-Time Tracking", "url": "https://www.emergentmind.com/topics/mediapipe-hands", "is_source_local": false, "is_source_both": false, "description": "Temporal Tracking and Filtering: For video-based processing, <strong>intersection-over-union (IoU) matching, Kalman filters, or exponential smoothing</strong> can be used to maintain ID continuity and suppress outliers during rapid articulation and occlusion.", "profile": {"name": "Emergent Mind", "url": "https://www.emergentmind.com/topics/mediapipe-hands", "long_name": "emergentmind.com", "img": "https://imgs.search.brave.com/Zc521AqQwkFLu4dxf7NKn1ehs161lCdjAkhkjS4ySGI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGM4NjVhOTE1/YjRiZTJmNWRlOTZj/ZjRhZmYxZWMwMWFj/ZDAzNTk0Y2MxZDc1/MTBiYWJmODlmZTE1/N2EwMWUxZS93d3cu/ZW1lcmdlbnRtaW5k/LmNvbS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "emergentmind.com", "hostname": "www.emergentmind.com", "favicon": "https://imgs.search.brave.com/Zc521AqQwkFLu4dxf7NKn1ehs161lCdjAkhkjS4ySGI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGM4NjVhOTE1/YjRiZTJmNWRlOTZj/ZjRhZmYxZWMwMWFj/ZDAzNTk0Y2MxZDc1/MTBiYWJmODlmZTE1/N2EwMWUxZS93d3cu/ZW1lcmdlbnRtaW5k/LmNvbS8", "path": "\u203a topics  \u203a mediapipe-hands"}, "thumbnail": {"src": "https://imgs.search.brave.com/K8jnmZPNdMeWoSa5EXkTOTXDUqE7xJulK0_vb-xBIyk/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9kMnpr/OHFkeDJ5MWJlci5j/bG91ZGZyb250Lm5l/dC9hc3NldHMvNjAw/cHgtYTYxMzIwNDQ5/YjhjODQ4YmNjNTZl/MDI4MjZiMDMzYjVl/ZmEwOWZhNTJjYzQ4/YjQ4N2U4ZjlhMmVi/OGVmZDcwNS5wbmc", "original": "https://d2zk8qdx2y1ber.cloudfront.net/assets/600px-a61320449b8c848bcc56e02826b033b5efa09fa52cc48b487e8f9a2eb8efd705.png", "logo": false}}, {"title": "MediaPipe Hands: On-device Real-time Hand Tracking", "url": "https://www.researchgate.net/publication/342302340_MediaPipe_Hands_On-device_Real-time_Hand_Tracking", "is_source_local": false, "is_source_both": false, "description": "Regardless, outcomes provide evidence on the robustness and stability of PHCA against perturbations to data and noise. It can be concluded that PHCA can serve as an alternative for FSL recognition, offering opportunities for further research. ... [93]. Within the designated hand areas, the MediaPipe hand landmark model identifies 21 palm-knuckle keypoints, which are highlighted in Fig.", "page_age": "2020-06-17T00:00:00", "profile": {"name": "ResearchGate", "url": "https://www.researchgate.net/publication/342302340_MediaPipe_Hands_On-device_Real-time_Hand_Tracking", "long_name": "researchgate.net", "img": "https://imgs.search.brave.com/WJ25-tL-91vp8kSoF0YS7d8CZVivP1cG-EjlGvrmxOc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjE0MDk0Yzli/MWQ3NGQ2ZTNjODVh/YWVmMDNkM2NkZWI5/NGEzNjdhMmEyY2E2/N2EzN2Y0OThlZjdi/YWU3MjkwMi93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "researchgate.net", "hostname": "www.researchgate.net", "favicon": "https://imgs.search.brave.com/WJ25-tL-91vp8kSoF0YS7d8CZVivP1cG-EjlGvrmxOc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjE0MDk0Yzli/MWQ3NGQ2ZTNjODVh/YWVmMDNkM2NkZWI5/NGEzNjdhMmEyY2E2/N2EzN2Y0OThlZjdi/YWU3MjkwMi93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC8", "path": "\u203a publication  \u203a 342302340_MediaPipe_Hands_On-device_Real-time_Hand_Tracking"}, "thumbnail": {"src": "https://imgs.search.brave.com/NA3UWQdok5rp6uI2kyF0cHLVZy0QBikNBsFv8yDMrrc/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC9pbWFnZXMv/dGVtcGxhdGUvZGVm/YXVsdF9wdWJsaWNh/dGlvbl9wcmV2aWV3/X2xhcmdlLnBuZw", "original": "https://www.researchgate.net/images/template/default_publication_preview_large.png", "logo": false}, "age": "June 17, 2020"}, {"title": "GitHub - geaxgx/depthai_hand_tracker: Running Google Mediapipe Hand Tracking models on Luxonis DepthAI hardware (OAK-D-lite, OAK-D, OAK-1,...)", "url": "https://github.com/geaxgx/depthai_hand_tracker", "is_source_local": false, "is_source_both": false, "description": "<strong>Hand Tracking from Mediapipe is a 2-stages pipeline</strong>. First, the Hand detection stage detects where are the hands in the whole image. For each detected hand, a Region of Interest (ROI) around the hand is calculated and fed to the second stage, ...", "profile": {"name": "GitHub", "url": "https://github.com/geaxgx/depthai_hand_tracker", "long_name": "github.com", "img": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "software", "is_live": false, "meta_url": {"scheme": "https", "netloc": "github.com", "hostname": "github.com", "favicon": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw", "path": "\u203a geaxgx  \u203a depthai_hand_tracker"}, "thumbnail": {"src": "https://imgs.search.brave.com/DVvGW1EDkApeq8PakD8kp0LpWRiuMyrxa_it-fvVL5E/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9vcGVu/Z3JhcGguZ2l0aHVi/YXNzZXRzLmNvbS8w/ZTUwOGIyMDljZTdm/Zjg2MDk3ZDg5ODA2/YjgzMDU5MDRhMzhi/MzlhZDM3ODlhMDhi/MGQ2NWVkMjk3NTFh/NjM0L2dlYXhneC9k/ZXB0aGFpX2hhbmRf/dHJhY2tlcg", "original": "https://opengraph.githubassets.com/0e508b209ce7ff86097d89806b8305904a38b39ad3789a08b0d65ed29751a634/geaxgx/depthai_hand_tracker", "logo": false}}, {"title": "[PDF] MediaPipe Hands: On-device Real-time Hand Tracking | Semantic Scholar", "url": "https://www.semanticscholar.org/paper/MediaPipe-Hands:-On-device-Real-time-Hand-Tracking-Zhang-Bazarevsky/84b19524609ad75f309be7f87bcea783e6ecd337", "is_source_local": false, "is_source_both": false, "description": "This paper presents a simple but powerful algorithm based on the MediaPipe Hands solution, a highly optimized neural network that processes the landmarks provided by MediaP Pipe using morphological and logical operators to obtain the masks that allow dynamic updating of the skin color model.Expand", "profile": {"name": "Semantic Scholar", "url": "https://www.semanticscholar.org/paper/MediaPipe-Hands:-On-device-Real-time-Hand-Tracking-Zhang-Bazarevsky/84b19524609ad75f309be7f87bcea783e6ecd337", "long_name": "semanticscholar.org", "img": "https://imgs.search.brave.com/K-sZ3Vgcj9LFjkcTUpjjK40LWCyX6WutMxHJiE7XYPI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGI3NTZiOWVj/YTE5N2MzYTNhYzA2/NWEyMzQwMzA5N2Yy/NGY3MjhhYTcxYmI2/YWQ3NjdjNDk5NGE2/NmUyYTdlYi93d3cu/c2VtYW50aWNzY2hv/bGFyLm9yZy8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "semanticscholar.org", "hostname": "www.semanticscholar.org", "favicon": "https://imgs.search.brave.com/K-sZ3Vgcj9LFjkcTUpjjK40LWCyX6WutMxHJiE7XYPI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNGI3NTZiOWVj/YTE5N2MzYTNhYzA2/NWEyMzQwMzA5N2Yy/NGY3MjhhYTcxYmI2/YWQ3NjdjNDk5NGE2/NmUyYTdlYi93d3cu/c2VtYW50aWNzY2hv/bGFyLm9yZy8", "path": "  \u203a papers  \u203a mediapipe hands: on-device real-time hand tracking"}, "thumbnail": {"src": "https://imgs.search.brave.com/A3fktYElQA3Sms9BB_yLvcb2tzI-8lZyB7LvUT1A-us/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/c2VtYW50aWNzY2hv/bGFyLm9yZy9pbWcv/c2VtYW50aWNfc2No/b2xhcl9vZy5wbmc", "original": "https://www.semanticscholar.org/img/semantic_scholar_og.png", "logo": false}}, {"title": "Hands - mediapipe", "url": "https://chuoling.github.io/mediapipe/solutions/hands.html", "is_source_local": false, "is_source_both": false, "description": "Complexity of the hand landmark model: 0 or 1. Landmark accuracy as well as inference latency generally go up with the model complexity. Default to 1. Minimum confidence value ([0.0, 1.0]) from the hand detection model for the detection to be considered successful.", "profile": {"name": "GitHub", "url": "https://chuoling.github.io/mediapipe/solutions/hands.html", "long_name": "chuoling.github.io", "img": "https://imgs.search.brave.com/UQsG855bt9JVRC7rU_bzffBainVrPq-eopHOMS-wk-A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGE4MmNlN2Ez/ODk4YTAyNmM2ZGNi/NGY4OGIwZjQ0Nzkz/NDhkNDdkOWMwYjNl/ZGU0MTQzZGIzYjll/NDY4OWVlZi9jaHVv/bGluZy5naXRodWIu/aW8v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "chuoling.github.io", "hostname": "chuoling.github.io", "favicon": "https://imgs.search.brave.com/UQsG855bt9JVRC7rU_bzffBainVrPq-eopHOMS-wk-A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGE4MmNlN2Ez/ODk4YTAyNmM2ZGNi/NGY4OGIwZjQ0Nzkz/NDhkNDdkOWMwYjNl/ZGU0MTQzZGIzYjll/NDY4OWVlZi9jaHVv/bGluZy5naXRodWIu/aW8v", "path": "\u203a mediapipe  \u203a solutions  \u203a hands.html"}}, {"title": "MediaPipe Hands: On-device Real-time Hand Tracking", "url": "https://research.google/pubs/mediapipe-hands-on-device-real-time-hand-tracking/", "is_source_local": false, "is_source_both": false, "description": "The proposed architecture demonstrates realtime inference speed on mobile GPUs and a high prediction quality. MediaPipe Hands is open-sourced at https://github.com/google/mediapipe.", "profile": {"name": "Google Research", "url": "https://research.google/pubs/mediapipe-hands-on-device-real-time-hand-tracking/", "long_name": "research.google", "img": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "research.google", "hostname": "research.google", "favicon": "https://imgs.search.brave.com/-QOsxpGGDx529NdqUjdKpQdw1SDtpkbog_g5yuxF4qs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZWNhMzVhZmNi/MDJjOGQ4MGViMDgz/ZGJlNTMwZWRlZjA1/Y2Y2YTEzNzBmYjg2/YjIwOTcwN2M0ZGZj/NTgzMmYzMy9yZXNl/YXJjaC5nb29nbGUv", "path": "\u203a pubs  \u203a mediapipe-hands-on-device-real-time-hand-tracking"}, "thumbnail": {"src": "https://imgs.search.brave.com/xMSx6K-DNGKBD00GCYX2R_H6N036h6fg9HWvNmYR3vs/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdG9y/YWdlLmdvb2dsZWFw/aXMuY29tL2d3ZWIt/cmVzZWFyY2gyMDIz/LW1lZGlhL2ltYWdl/cy9PcGVuX0dyYXBo/LndpZHRoLTgwMC5m/b3JtYXQtanBlZy5q/cGc", "original": "https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg", "logo": false}}, {"title": "mediapipe/docs/solutions/hands.md at master \u00b7 google-ai-edge/...", "url": "https://github.com/google/mediapipe/blob/master/docs/solutions/hands.md", "is_source_local": false, "is_source_both": false, "description": "In addition, in our pipeline the crops can also be generated based on the hand landmarks identified in the previous frame, and only when the landmark model could no longer identify hand presence is palm detection invoked to relocalize the hand. The pipeline is implemented as a MediaPipe graph that uses a hand landmark tracking subgraph from the hand landmark module, and renders using a dedicated hand renderer subgraph.", "profile": {"name": "GitHub", "url": "https://github.com/google/mediapipe/blob/master/docs/solutions/hands.md", "long_name": "github.com", "img": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "software", "is_live": false, "meta_url": {"scheme": "https", "netloc": "github.com", "hostname": "github.com", "favicon": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw", "path": "\u203a google  \u203a mediapipe  \u203a blob  \u203a master  \u203a docs  \u203a solutions  \u203a hands.md"}, "thumbnail": {"src": "https://imgs.search.brave.com/688wcydayjjovsWi9Y1muRakISJExilZNCGGw0NS29o/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9vcGVu/Z3JhcGguZ2l0aHVi/YXNzZXRzLmNvbS8w/ZjdhOTg2MjU3M2Ni/NzFlOWQyZGIzY2Fm/NDU5MDc4NTUwODQ3/ZTEyZmRjYzI0OTgy/YTgyYmJlNmJmMGMz/MGVlL2dvb2dsZS1h/aS1lZGdlL21lZGlh/cGlwZQ", "original": "https://opengraph.githubassets.com/0f7a9862573cb71e9d2db3caf459078550847e12fdcc24982a82bbe6bf0c30ee/google-ai-edge/mediapipe", "logo": false}}, {"title": "Hand Gesture Mapping Using MediaPipe Algorithm | SpringerLink", "url": "https://link.springer.com/chapter/10.1007/978-981-16-8862-1_39", "is_source_local": false, "is_source_both": false, "description": "Once the user has completed the setup of his or her gestures, he or she can request the Python code that was created for that configuration. This paper provides the ability to pick any one of the workstation GUI tasks and assign motions to it, as well as the reverse of this functionality.", "profile": {"name": "Springer", "url": "https://link.springer.com/chapter/10.1007/978-981-16-8862-1_39", "long_name": "link.springer.com", "img": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "link.springer.com", "hostname": "link.springer.com", "favicon": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8", "path": "  \u203a home  \u203a proceedings of third international conference on communication, computing and electronics systems  \u203a conference paper"}, "thumbnail": {"src": "https://imgs.search.brave.com/rdKOmGJHkXAowWNI03n9wxbAwO3VkWZPCeHc1c-kLm8/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9tZWRp/YS5zcHJpbmdlcm5h/dHVyZS5jb20vdzE1/My9zcHJpbmdlci1z/dGF0aWMvY292ZXIv/Ym9vay85NzgtOTgx/LTE2LTg4NjItMS5q/cGc", "original": "https://media.springernature.com/w153/springer-static/cover/book/978-981-16-8862-1.jpg", "logo": false}}, {"title": "MediaPipe-Hand-Detection - Qualcomm AI Hub", "url": "https://aihub.qualcomm.com/models/mediapipe_hand", "is_source_local": false, "is_source_both": false, "description": "Real-time hand detection optimized for mobile and edge.", "profile": {"name": "Qualcomm\u00ae AI Hub", "url": "https://aihub.qualcomm.com/models/mediapipe_hand", "long_name": "aihub.qualcomm.com", "img": "https://imgs.search.brave.com/UTpvhPmpFihRjdmffoVQ1hNbVD9RVkCUpBjgRUHvZ8M/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvM2NkNTI4NGE5/MjhlN2JiZTY0NDUy/ZGY1M2RiNzEwN2Q3/MDJmM2EzMGVlMjJl/NzM3NWU4ZmRjNWNk/OTViZTQwOS9haWh1/Yi5xdWFsY29tbS5j/b20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "aihub.qualcomm.com", "hostname": "aihub.qualcomm.com", "favicon": "https://imgs.search.brave.com/UTpvhPmpFihRjdmffoVQ1hNbVD9RVkCUpBjgRUHvZ8M/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvM2NkNTI4NGE5/MjhlN2JiZTY0NDUy/ZGY1M2RiNzEwN2Q3/MDJmM2EzMGVlMjJl/NzM3NWU4ZmRjNWNk/OTViZTQwOS9haWh1/Yi5xdWFsY29tbS5j/b20v", "path": "\u203a models  \u203a mediapipe_hand"}, "thumbnail": {"src": "https://imgs.search.brave.com/FuYaNDzTcc2_nHxBaVW_S8ObhDJAfJhxdrvNbVKWpms/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9xYWlo/dWItcHVibGljLWFz/c2V0cy5zMy51cy13/ZXN0LTIuYW1hem9u/YXdzLmNvbS9xYWkt/aHViLW1vZGVscy9t/b2RlbHMvbWVkaWFw/aXBlX2hhbmQvd2Vi/LWFzc2V0cy9tb2Rl/bF9kZW1vLnBuZw", "original": "https://qaihub-public-assets.s3.us-west-2.amazonaws.com/qai-hub-models/models/mediapipe_hand/web-assets/model_demo.png", "logo": false}}, {"title": "MediaPipe - Wikipedia", "url": "https://en.wikipedia.org/wiki/MediaPipe", "is_source_local": false, "is_source_both": false, "description": "Starting with the identification of the palm, MediaPipe is able to use the positioning of the palm as an input to a second model that predicts the positions of key landmarks that will represent the hand&#x27;s structure. MediaPipe continuously monitors the confidence of its predictions and re-runs detection when needed to maintain its accuracy, while temporal smoothing helps reduce the jitter between frames.", "page_age": "2025-12-19T19:06:04", "profile": {"name": "Wikipedia", "url": "https://en.wikipedia.org/wiki/MediaPipe", "long_name": "en.wikipedia.org", "img": "https://imgs.search.brave.com/m6XxME4ek8DGIUcEPCqjRoDjf2e54EwL9pQzyzogLYk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjQwNGZhZWY0/ZTQ1YWUzYzQ3MDUw/MmMzMGY3NTQ0ZjNj/NDUwMDk5ZTI3MWRk/NWYyNTM4N2UwOTE0/NTI3ZDQzNy9lbi53/aWtpcGVkaWEub3Jn/Lw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "deep_results": {"buttons": [{"type": "button_result", "title": "History", "url": "https://en.wikipedia.org/wiki/MediaPipe#History"}, {"type": "button_result", "title": "Solutions", "url": "https://en.wikipedia.org/wiki/MediaPipe#Solutions"}, {"type": "button_result", "title": "Programming Language", "url": "https://en.wikipedia.org/wiki/MediaPipe#Programming_Language"}, {"type": "button_result", "title": "How MediaPipe Works", "url": "https://en.wikipedia.org/wiki/MediaPipe#How_MediaPipe_Works"}]}, "meta_url": {"scheme": "https", "netloc": "en.wikipedia.org", "hostname": "en.wikipedia.org", "favicon": "https://imgs.search.brave.com/m6XxME4ek8DGIUcEPCqjRoDjf2e54EwL9pQzyzogLYk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjQwNGZhZWY0/ZTQ1YWUzYzQ3MDUw/MmMzMGY3NTQ0ZjNj/NDUwMDk5ZTI3MWRk/NWYyNTM4N2UwOTE0/NTI3ZDQzNy9lbi53/aWtpcGVkaWEub3Jn/Lw", "path": "\u203a wiki  \u203a MediaPipe"}, "age": "3 weeks ago"}, {"title": "Hand landmarks detection guide | MediaPipe | Google for Developers", "url": "https://developers.google.com/mediapipe/solutions/vision/hand_landmarker", "is_source_local": false, "is_source_both": false, "description": "<strong>Hand Landmarker</strong> only re-triggers the palm detection model if the hand landmarks model no longer identifies the presence of hands or fails to track the hands within the frame. This reduces the number of times <strong>Hand Landmarker</strong> tiggers the palm ...", "profile": {"name": "Google", "url": "https://developers.google.com/mediapipe/solutions/vision/hand_landmarker", "long_name": "developers.google.com", "img": "https://imgs.search.brave.com/xd2if5kWPozWi5tGSdOQNnrmMOZFDny4yeh0h6PJu8U/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODcyMjg1ZjQ0/ZWI1OTgzNjQxNTM4/NDQ4MDljYjE2YTZi/MTc4OWFjOGM3NDEx/ZDBmZWJjYjg2YTBj/MGI3Zjk3OC9kZXZl/bG9wZXJzLmdvb2ds/ZS5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "developers.google.com", "hostname": "developers.google.com", "favicon": "https://imgs.search.brave.com/xd2if5kWPozWi5tGSdOQNnrmMOZFDny4yeh0h6PJu8U/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODcyMjg1ZjQ0/ZWI1OTgzNjQxNTM4/NDQ4MDljYjE2YTZi/MTc4OWFjOGM3NDEx/ZDBmZWJjYjg2YTBj/MGI3Zjk3OC9kZXZl/bG9wZXJzLmdvb2ds/ZS5jb20v", "path": "  \u203a mediapipe  \u203a hand landmarks detection guide"}, "thumbnail": {"src": "https://imgs.search.brave.com/GAPHAOESmySObnKYOeWfpQqXqlFxcq796DV_PD-ziIY/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/Z3N0YXRpYy5jb20v/ZGV2cmVsLWRldnNp/dGUvcHJvZC92N2Vj/MWNkYmY5MDk4OWFi/MDgyZjMwYmY5Yjlj/YmU2Mjc4MDQ4NDhj/MThiNzBkNzIyMDYy/YWViNmM2ZDg5NThi/NS9kZXZlbG9wZXJz/L2ltYWdlcy9vcGVu/Z3JhcGgvcGFsZS1i/bHVlLnBuZw", "original": "https://www.gstatic.com/devrel-devsite/prod/v7ec1cdbf90989ab082f30bf9b9cbe627804848c18b70d722062aeb6c6d8958b5/developers/images/opengraph/pale-blue.png", "logo": false}}, {"title": "Hand tracking using Mediapipe. Access full tutorial of body pose\u2026", "url": "https://medium.com/@sunnykumar1516/hand-tracking-using-mediapipe-263c40ad6914", "is_source_local": false, "is_source_both": false, "description": "MediaPipe simplifies the development of ML-based applications by providing a modular and customizable framework. It offers a variety of pre-trained models and reusable building blocks for tasks like object detection, face detection and tracking, pose estimation, hand tracking, and more.", "page_age": "2024-01-04T14:39:28", "profile": {"name": "Medium", "url": "https://medium.com/@sunnykumar1516/hand-tracking-using-mediapipe-263c40ad6914", "long_name": "medium.com", "img": "https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "medium.com", "hostname": "medium.com", "favicon": "https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw", "path": "\u203a @sunnykumar1516  \u203a hand-tracking-using-mediapipe-263c40ad6914"}, "thumbnail": {"src": "https://imgs.search.brave.com/SQJB29X3dmFdn9oc8q7n7zRdq7JUrSrSZnCIJQ7psiU/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9taXJv/Lm1lZGl1bS5jb20v/djIvcmVzaXplOmZp/dDoxMjAwLzEqQnFs/YkN2cmFNNDByQkdZ/Rl9YOWd5Zy5wbmc", "original": "https://miro.medium.com/v2/resize:fit:1200/1*BqlbCvraM40rBGYF_X9gyg.png", "logo": false}, "age": "January 4, 2024"}, {"title": "qualcomm/MediaPipe-Hand-Detection \u00b7 Hugging Face", "url": "https://huggingface.co/qualcomm/MediaPipe-Hand-Detection", "is_source_local": false, "is_source_both": false, "description": "<strong>The MediaPipe Hand Landmark Detector</strong> is a machine learning pipeline that predicts bounding boxes and pose skeletons of hands in an image.", "profile": {"name": "Hugging Face", "url": "https://huggingface.co/qualcomm/MediaPipe-Hand-Detection", "long_name": "huggingface.co", "img": "https://imgs.search.brave.com/L-O7pGsoxRbYSIBhk64OxIe19zjGm3DcqlEnCuoI678/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNmYwNDg3ZTZh/YzcxMDgwYmRkOGFm/YTk1MTJiYzQ2NmM5/NWViOGEyNGYyZWEx/NDg4NzQ5NzlhYTA5/NTI0MDA4Yy9odWdn/aW5nZmFjZS5jby8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "huggingface.co", "hostname": "huggingface.co", "favicon": "https://imgs.search.brave.com/L-O7pGsoxRbYSIBhk64OxIe19zjGm3DcqlEnCuoI678/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNmYwNDg3ZTZh/YzcxMDgwYmRkOGFm/YTk1MTJiYzQ2NmM5/NWViOGEyNGYyZWEx/NDg4NzQ5NzlhYTA5/NTI0MDA4Yy9odWdn/aW5nZmFjZS5jby8", "path": "\u203a qualcomm  \u203a MediaPipe-Hand-Detection"}, "thumbnail": {"src": "https://imgs.search.brave.com/glL8I8zIEdl1fZsXyjijrXYGATuQfjNnK-OeRhsRIgg/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jZG4t/dGh1bWJuYWlscy5o/dWdnaW5nZmFjZS5j/by9zb2NpYWwtdGh1/bWJuYWlscy9tb2Rl/bHMvcXVhbGNvbW0v/TWVkaWFQaXBlLUhh/bmQtRGV0ZWN0aW9u/LnBuZw", "original": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/qualcomm/MediaPipe-Hand-Detection.png", "logo": false}}, {"title": "Google open sources an on-device, real-time hand gesture recognition algorithm built with MediaPipe", "url": "https://www.packtpub.com/en-us/learning/how-to-tutorials/google-open-sources-an-on-device-real-time-hand-gesture-recognition-algorithm-built-with-mediapipe/", "is_source_local": false, "is_source_both": false, "description": "Their algorithm <strong>uses machine learning to compute 3D keypoints of a hand from a video frame</strong>. This research is implemented in MediaPipe which is an open-source cross-platform framework for building multimodal (eg.", "profile": {"name": "Packt", "url": "https://www.packtpub.com/en-us/learning/how-to-tutorials/google-open-sources-an-on-device-real-time-hand-gesture-recognition-algorithm-built-with-mediapipe/", "long_name": "packtpub.com", "img": "https://imgs.search.brave.com/jmMH8m2C6oTR0y9v_5fVd60yhumrU_CUcH-gwVKvrPY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODAxNjFmYzkw/YWNhMzMzMjk2NDk5/ODkwMGY0YTg5NWYw/YWRjY2MzOTljNjUz/OGE2YjVmOWYyMDAx/NmY5ZWVjNS93d3cu/cGFja3RwdWIuY29t/Lw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "packtpub.com", "hostname": "www.packtpub.com", "favicon": "https://imgs.search.brave.com/jmMH8m2C6oTR0y9v_5fVd60yhumrU_CUcH-gwVKvrPY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODAxNjFmYzkw/YWNhMzMzMjk2NDk5/ODkwMGY0YTg5NWYw/YWRjY2MzOTljNjUz/OGE2YjVmOWYyMDAx/NmY5ZWVjNS93d3cu/cGFja3RwdWIuY29t/Lw", "path": "\u203a en-us  \u203a learning  \u203a how-to-tutorials  \u203a google-open-sources-an-on-device-real-time-hand-gesture-recognition-algorithm-built-with-mediapipe"}, "thumbnail": {"src": "https://imgs.search.brave.com/TAfXJOPJD2tCOxYi6BDenOqjhxoonZIbBp-b1sHf5YU/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdWJz/Y3JpcHRpb24ucGFj/a3RwdWIuY29tL2lt/YWdlcy9sb2dvLnBu/Zw", "original": "https://subscription.packtpub.com/images/logo.png", "logo": true}}], "repo_sense": []}}}
{"timestamp": "2026-01-10T17:48:27.968771Z", "phase": "H", "summary": "P0-Sense QUAD Hunt: simpler alternative to bipartite matching for multi-hand tracking identity persistence", "p0": {"status": "complete", "query": "simpler alternative to bipartite matching for multi-hand tracking identity persistence", "receipt": "P0_QUAD_SEARCH_20260110_174827", "data": {"web_tavily": [{"url": "https://cit.iict.bas.bg/CIT_2014/v14-s/8BJTU--08-md-Gotovo.pdf", "title": "[PDF] Multi-Targets Tracking Based on Bipartite Graph Matching", "content": "by concluding remarks in Section 4. 2. Multi-target tracking based on bipartite graph matching For the target tracking we consider a simple bipartite matching method to associate with the targets in two neighboring frames. We generate a weighted complete bipartite graph where a node represents a target and the weight between two nodes is the similarity between two targets in neighboring frames (Fig. 1). Thus the multi-target tracking problem is formulated as the weighted bipartite matching problem; we can construct the maximal weight matching model to determine the best matching results. Fig. 1. Bipartite graph and matching diagram 2.1. Determination of weight In the weighted bipartite graph matching problem, the similarity between two targets must be determined to get the weight \u03c9ij of [...] interest, inferring their 79 trajectories and maintaining their identities through a video sequence. Despite the huge amount of excellent research in the field [1-4], developing an efficient and robust target detection and correspondence method is still challenging, especially in crowded and unstructured environments where targets can appear, disappear, be wrongly detected and frequently interact in the image frame. Multi-target tracking can be viewed as a correspondence problem in sequential image frames. Traditional feature-based tracking methods, such as those based on colour , salient points , or motion blobs , do not have a discriminative model for target detection and thus suffer from poor detection of targets. Over the past two decades, the kinds of data association-based tracking [...] size \u23a5 \u23a5 \u23a6 \u23a4 \u23a2 \u23a2 \u23a3 \u23a1 \u2212 x j i s s \u03c3 \u03b3 (4) sfeature(Ni, Nj) = BHSV(Ni, Nj)BCS-LBP(Ni, Nj), 81 Where ( ) B \u2022 is the Bhattacharyya distance between two histograms \u03b3pos, and \u03b3size are normalization factors. Then we get the edge weight \u03c9ij = sij(Ni, Nj). 2.2. The bipartite graph optimal matching In this section our goal is to find the maximal weighted matching in bipartite graphs. In a bipartite graph G = (X, Y, E), when m \u2260 n (propose m<n), the maximal weighted matching of a bipartite graph is complete matching, this problem can be solved by the following integer programming model: (5) maxg = \u2211\u2211 = = m i n j ij ijx 1 1 , \u03c9 (6) subject to , , , 2 , 1 , 1 1 m i x n j ij K = = \u2211 = (7) , , , 2 , 1 , 1 1 n j x m i ij K = \u2264 \u2211 = (8) xij = 0 or 1, i = 1, 2, \u2026, m; j = 1, 2, \u2026, n. In the above model, (5)", "score": 0.5041753, "raw_content": null}, {"url": "https://stackoverflow.com/questions/78285482/mediapipe-get-persistent-id-of-tracked-hands-between-frames", "title": "Mediapipe get persistent id of tracked hands between frames", "content": "Ask Question\n\nAsked\n\nModified 1 year, 8 months ago\n\nViewed 353 times\n\n0\n\nI am using mediapipe to get the landmarks of multiple hands of a live video. I now want to know how each hand (more than 2) have moved in between the frames. For this I need to remember which hand was which in each result callback. I am using this solution: \n\nI could of course just track the hands manually by looking at the average position of each hand and hoping they are not swapped between frames. But as the mediapipe model in itself does the tracking, I would like to use it's tracking as it is probably more robust than my simple tracking and it is also less hacky to do two different trackings for one task. [...] I could also imagine that the hands are always in the same order of the `multi_hand_landmarks` object, but I could not find evidence for this and this leaves a problem when one hand disappears. I cannot find a id per hand which is persistent across frames in the `multi_hand_landmarks` object.\n\n python\n computer-vision\n mediapipe\n video-tracking\n object-tracking\n\nShare\n\nImprove this question\n\nedited Apr 6, 2024 at 19:25\n\nChristoph Rackwitz\n\n16.4k55 gold badges4242 silver badges5656 bronze badges\n\nasked Apr 6, 2024 at 18:29\n\nFabio M.\n\n6399 bronze badges\n\n3\n\n what makes you think that mediapipe did any tracking? it might just do detection and pose estimation, without any tracking required.\n\n  Christoph Rackwitz\n\n  \u2013  Christoph Rackwitz\n\n  2024-04-06 19:25:30 +00:00", "score": 0.4942147, "raw_content": null}, {"url": "https://www.nature.com/articles/s41598-025-07492-7", "title": "Enhancing multiple object tracking accuracy via quantum annealing", "content": "### Multiplication of matching\n\nThis paper proposes a method that can reduce the computational cost of bipartite graph matching for object tracking, even when the number of detected objects and the types of trackers increase. The core idea is to use quantum annealing to speed up the search for the optimal matching, as shown in Fig. 3. Quantum annealing is utilized to suppress the computation time even when the number of nodes in the bipartite graph increases. [...] In the situation of MOT, the degree of each node in the bipartite graph is often uneven, and compared to the maximum matching prioritizing maximizing the number of matchings, maximal matching tends to yield more stable results. [...] Let \\(U=\\{ u(i) \\}\\_{i=1}^N\\) denote the set of tracks at the detection frame at time \\(t-1\\), and \\(V=\\{v(j) \\}\\_{j=1}^M\\) denote the set of detected objects at the detection frame at time t. The time t is counted for each detection frame and is assumed to take an integer value. At this time, a bipartite graph \\(G=(U \\cup V, E)\\) is introduced, where edges are only placed between \\(u\\in U\\) and \\(v\\in V\\). Here, \\(U \\cap V = \\emptyset\\), and E is the set of edges \\(E = \\{ (u(i), v(j) ) \\mid u(i) \\in U, \\ v(j) \\in V \\}\\). An illustration of the bipartite graph G is shown in Fig. 2.", "score": 0.4465194, "raw_content": null}, {"url": "https://home.ttic.edu/~cjulia/papers/matching-SODA-full.pdf", "title": "[PDF] A Faster Combinatorial Algorithm for Maximum Bipartite Matching", "content": "we embed a large \u2126(1)-expander W into the graph X, and then use two Even-Shiloach tree (ES-Tree) data structures in X, that are rooted at the vertices of W; in one of the resulting trees the edges are directed away to the root, and in the other they are directed towards the root. However, due to the speci\ufb01cs of the setting that we have described, we obtain an algorithm that is signi\ufb01cantly simpler than that of [BGS20], and other similar algorithms. For example, we use a straightforward (although somewhat slower) algorithm for the cut player in the Cut-Matching game, that is used in order to embed an expander into X, and our algorithm for the variant of APSP in expanders that we obtain is quite elementary; it avoids both the recursion that was used in previous works (see [CS21, CGL+19, [...] P e\u2208E(P) \u2113(e) \u22651 \u2200P \u2208P\u2217 \u2113(e) \u22650 \u2200e \u2208E(H) In the dual LP, there is a variable \u2113(e) for every edge e \u2208E(H), that we will refer to as the length of the edge. We now consider the standard application of the MWU framework, that can be viewed as a primal-dual algorithm. The algorithm that we describe below is a simpli\ufb01ed version of the original algorithm of [GK98, Fle00]; this variant achieves a slightly weaker approximation, but its analysis is much simpler and the higher approximation factor does not a\ufb00ect the overall performance of our algorithm.\nStandard MWU-Based Algorithm.\nFor simplicity, in this informal description of the standard MWU-based algorithm, we assume that m = |E(H)| is an integral power of 2. We start with the graph H, and set the length of every edge e in H to \u2113(e) = 1/m. [...] In order to implement this framework, it is su\ufb03cient to implement an oracle that, in every iteration, computes a simple s-t path of length at most 1 in H, or correctly establishes that no such path exists, while the lengths of the edges in H increase over time. As observed by Madry [Mad10], the problem that the oracle needs to solve can be viewed as a special case of decremental SSSP. Indeed, the increase in edge lengths can be easily implemented as edge-deletion updates. This is done by constructing another graph \u02c6 H, that is obtained from graph H, by replacing every edge e = (u, v) \u2208E(H) with log m parallel copies of lengths 1/m, 2/m, . . . , 1. Every time the length of e in graph H doubles, we delete the shortest copy of e that lies in \u02c6 H.", "score": 0.3727112, "raw_content": null}, {"url": "https://www.cis.upenn.edu/~sanjeev/papers/soda24_bipartite_matching.pdf", "title": "[PDF] A Faster Combinatorial Algorithm for Maximum Bipartite Matching.", "content": "we embed a large \u2126(1)-expander W into the graph X, and then use two Even-Shiloach tree (ES-Tree) data structures in X, that are rooted at the vertices of W; in one of the resulting trees the edges are directed away to the root, and in the other they are directed towards the root. However, due to the speci\ufb01cs of the setting that we have described, we obtain an algorithm that is signi\ufb01cantly simpler than that of [BGS20], and other similar algorithms. For example, we use a straightforward (although somewhat slower) algorithm for the cut player in the Cut-Matching game, that is used in order to embed an expander into X, and our algorithm for the variant of APSP in expanders that we obtain is quite elementary; it avoids both the recursion that was used in previous works (see [CS21, CGL+19, [...] P e\u2208E(P ) \u2113(e) \u22651 \u2200P \u2208P\u2217 \u2113(e) \u22650 \u2200e \u2208E(H) In the dual LP, there is a variable \u2113(e) for every edge e \u2208E(H), that we will refer to as the length of the edge. We now consider the standard application of the MWU framework, that can be viewed as a primal-dual algorithm. The algorithm that we describe below is a simpli\ufb01ed version of the original algorithm of [GK98, Fle00]; this variant achieves a slightly weaker approximation, but its analysis is much simpler and the higher approximation factor does not a\ufb00ect the overall performance of our algorithm.\nStandard MWU-Based Algorithm.\nFor simplicity, in this informal description of the standard MWU-based algorithm, we assume that m = |E(H)| is an integral power of 2. We start with the graph H, and set the length of every edge e in H to \u2113(e) = 1/m. [...] In order to implement this framework, it is su\ufb03cient to implement an oracle that, in every iteration, computes a simple s-t path of length at most 1 in H, or correctly establishes that no such path exists, while the lengths of the edges in H increase over time. As observed by Madry [Mad10], the problem that the oracle needs to solve can be viewed as a special case of decremental SSSP. Indeed, the increase in edge lengths can be easily implemented as edge-deletion updates. This is done by constructing another graph \u02c6 H, that is obtained from graph H, by replacing every edge e = (u, v) \u2208E(H) with log m parallel copies of lengths 1/m, 2/m, . . . , 1. Every time the length of e in graph H doubles, we delete the shortest copy of e that lies in \u02c6 H.", "score": 0.37216341, "raw_content": null}], "web_brave": [{"title": "Kalman Filtering and Bipartite Matching Based Super-Chained Tracker Model for Online Multi Object Tracking in Video Sequences", "url": "https://www.mdpi.com/2076-3417/12/19/9538", "is_source_local": false, "is_source_both": false, "description": "The existing MOT techniques having appearance features of complex nature can be avoided by involving simpler and more efficient matching strategies to perform <strong>chaining of the two bounding boxes</strong>.", "page_age": "2022-09-23T00:00:00", "profile": {"name": "MDPI", "url": "https://www.mdpi.com/2076-3417/12/19/9538", "long_name": "mdpi.com", "img": "https://imgs.search.brave.com/IDkOdEuGDr9XA4H-36Lnou8wMT1nsDEHneEyTFddFmg/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOWQ5OGNkM2Uz/YWZhNTljYjhhZGVh/MTQyMzUxYTFmMjY3/NzhjMjEzMmI2M2Nm/YzllOTI4YmYwZDcw/MmE0NDY5MC93d3cu/bWRwaS5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "mdpi.com", "hostname": "www.mdpi.com", "favicon": "https://imgs.search.brave.com/IDkOdEuGDr9XA4H-36Lnou8wMT1nsDEHneEyTFddFmg/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOWQ5OGNkM2Uz/YWZhNTljYjhhZGVh/MTQyMzUxYTFmMjY3/NzhjMjEzMmI2M2Nm/YzllOTI4YmYwZDcw/MmE0NDY5MC93d3cu/bWRwaS5jb20v", "path": "\u203a 2076-3417  \u203a 12  \u203a 19  \u203a 9538"}, "thumbnail": {"src": "https://imgs.search.brave.com/x_f5g-CSrM3gnY1poep2zJtZMV1DYXhEor44d0iI-7U/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9wdWIu/bWRwaS1yZXMuY29t/L2FwcGxzY2kvYXBw/bHNjaS0xMi0wOTUz/OC9hcnRpY2xlX2Rl/cGxveS9odG1sL2lt/YWdlcy9hcHBsc2Np/LTEyLTA5NTM4LWcw/MDEtNTUwLmpwZz8x/NjYzOTE2MjI0", "original": "https://pub.mdpi-res.com/applsci/applsci-12-09538/article_deploy/html/images/applsci-12-09538-g001-550.jpg?1663916224", "logo": false}, "age": "September 23, 2022"}, {"title": "Real-time Multi-Object Tracking Based on Bi-directional Matching", "url": "https://arxiv.org/pdf/2303.08444", "is_source_local": false, "is_source_both": false, "description": "mode[3]\u2013[5] will first identify objects across the video and then link the identifications of each \u00b7 object and generate trajectories. On the other hand, the online tracking mode uses only the current", "profile": {"name": "arXiv", "url": "https://arxiv.org/pdf/2303.08444", "long_name": "arxiv.org", "img": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "arxiv.org", "hostname": "arxiv.org", "favicon": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv", "path": "\u203a pdf  \u203a 2303.08444"}, "content_type": "pdf"}, {"title": "Higher Order Matching for Consistent Multiple Target Tracking | IEEE Conference Publication | IEEE Xplore", "url": "https://ieeexplore.ieee.org/document/6751131/", "is_source_local": false, "is_source_both": false, "description": "This paper addresses the data assignment problem in multi frame multi object tracking in video sequences. Traditional methods employing maximum weight bipartite matching offer limited temporal modeling. It has recently been shown [6, 8, 24] that incorporating higher order temporal constraints ...", "profile": {"name": "IEEE Xplore", "url": "https://ieeexplore.ieee.org/document/6751131/", "long_name": "ieeexplore.ieee.org", "img": "https://imgs.search.brave.com/Pr3gNu-kVf-oded6UAgVYC0mXlyfwEL8yNiKsplOmlY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMWQyMTA0Y2Fk/OWI3Mzc5ODMwOGM5/MjM0ODJjZTBiNzQx/ZDE4OWI0YTAxMmE4/NmYwZWI5NjUwZjhj/MGRlY2MzOS9pZWVl/eHBsb3JlLmllZWUu/b3JnLw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "ieeexplore.ieee.org", "hostname": "ieeexplore.ieee.org", "favicon": "https://imgs.search.brave.com/Pr3gNu-kVf-oded6UAgVYC0mXlyfwEL8yNiKsplOmlY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMWQyMTA0Y2Fk/OWI3Mzc5ODMwOGM5/MjM0ODJjZTBiNzQx/ZDE4OWI0YTAxMmE4/NmYwZWI5NjUwZjhj/MGRlY2MzOS9pZWVl/eHBsb3JlLmllZWUu/b3JnLw", "path": "\u203a document  \u203a 6751131"}, "thumbnail": {"src": "https://imgs.search.brave.com/1Yj1c8K5zP1AWFtkhZFjcosmrsRFLx0OY2nEczMxSmA/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pZWVl/eHBsb3JlLmllZWUu/b3JnL2Fzc2V0cy9p/bWcvaWVlZV9sb2dv/X3NtZWRpYV8yMDBY/MjAwLnBuZw", "original": "https://ieeexplore.ieee.org/assets/img/ieee_logo_smedia_200X200.png", "logo": true}}, {"title": "[2104.02631] Local Metrics for Multi-Object Tracking", "url": "https://ar5iv.labs.arxiv.org/html/2104.02631", "is_source_local": false, "is_source_both": false, "description": "IDF1 [54] establishes correspondence between entire tracks to maximise the total number of frames in which corresponding tracks overlap, referred to as the Identification True Positives (IDTP). In our formulation, the number of frames in which each ground-truth track ... Let us represent the set of one-to-one matchings between the two sets of tracks by the binary matrices ... (i,j) in the bipartite graph.", "page_age": "2024-03-02T06:10:06", "profile": {"name": "arXiv", "url": "https://ar5iv.labs.arxiv.org/html/2104.02631", "long_name": "ar5iv.labs.arxiv.org", "img": "https://imgs.search.brave.com/Ra63-x8TeB8HQ-I2dv-lRN_mIzQ3JQGTcfoprRnqdaI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGEzNGUyYWVl/NDk2ODQ0MzYwZjhk/ZGRkOGIyNWVkN2Qz/ZWZiMjQ4ZGI2YWQw/NjQ0M2Q5YWFhMTM4/MDljYjI0OS9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "ar5iv.labs.arxiv.org", "hostname": "ar5iv.labs.arxiv.org", "favicon": "https://imgs.search.brave.com/Ra63-x8TeB8HQ-I2dv-lRN_mIzQ3JQGTcfoprRnqdaI/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZGEzNGUyYWVl/NDk2ODQ0MzYwZjhk/ZGRkOGIyNWVkN2Qz/ZWZiMjQ4ZGI2YWQw/NjQ0M2Q5YWFhMTM4/MDljYjI0OS9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy8", "path": "\u203a html  \u203a 2104.02631"}, "thumbnail": {"src": "https://imgs.search.brave.com/5Tsbn2z8zn03LO10F1g1N0_ns-eYv5CwATFUyJ5Qpdc/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcjVp/di5sYWJzLmFyeGl2/Lm9yZy9hc3NldHMv/YXI1aXZfY2FyZC5w/bmc", "original": "https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png", "logo": false}, "age": "March 2, 2024"}, {"title": "Optimal Nonbipartite Matching and Its Statistical Applications - PubMed", "url": "https://pubmed.ncbi.nlm.nih.gov/23175567/", "is_source_local": false, "is_source_both": false, "description": "Conventional two-group, or bipartite, matching has been widely used in practice. However, its utility is limited to simpler designs. In contrast, <strong>nonbipartite matching</strong> is not limited to the two-group case, handling multiparty matching ...", "profile": {"name": "PubMed", "url": "https://pubmed.ncbi.nlm.nih.gov/23175567/", "long_name": "pubmed.ncbi.nlm.nih.gov", "img": "https://imgs.search.brave.com/ct7iR5anN_weyHBpP8Sf5afc81ndEdn-ACwjApGeHF4/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzY1MzFmMzQy/ZGQ2OWE1N2M2YjFl/YmEwOWU2NjIzY2Jh/N2I5ZTRkY2U3MzY4/OThkNDIyN2EyYWJl/OWE3MWNlYS9wdWJt/ZWQubmNiaS5ubG0u/bmloLmdvdi8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "pubmed.ncbi.nlm.nih.gov", "hostname": "pubmed.ncbi.nlm.nih.gov", "favicon": "https://imgs.search.brave.com/ct7iR5anN_weyHBpP8Sf5afc81ndEdn-ACwjApGeHF4/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNzY1MzFmMzQy/ZGQ2OWE1N2M2YjFl/YmEwOWU2NjIzY2Jh/N2I5ZTRkY2U3MzY4/OThkNDIyN2EyYWJl/OWE3MWNlYS9wdWJt/ZWQubmNiaS5ubG0u/bmloLmdvdi8", "path": "\u203a 23175567"}, "thumbnail": {"src": "https://imgs.search.brave.com/awH-ZkCPRVJjPIfSAAs8aykD9OTBZi53ayGryKIDtDU/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jZG4u/bmNiaS5ubG0ubmlo/Lmdvdi9wdWJtZWQv/cGVyc2lzdGVudC9w/dWJtZWQtbWV0YS1p/bWFnZS12Mi5qcGc", "original": "https://cdn.ncbi.nlm.nih.gov/pubmed/persistent/pubmed-meta-image-v2.jpg", "logo": false}}, {"title": "Non-bipartite matching | Combinatorial Optimization Class Notes | Fiveable", "url": "https://fiveable.me/combinatorial-optimization/unit-5/non-bipartite-matching/study-guide/o9UuyYAu5b1et6GC", "is_source_local": false, "is_source_both": false, "description": "<strong>Non-bipartite matching</strong> handles more general graph structures \u00b7 Algorithms for non-bipartite matching more complex due to odd cycles \u00b7 Bipartite matching solved using simpler methods (Hungarian algorithm, Ford-Fulkerson)", "page_age": "2025-08-22T00:00:00", "profile": {"name": "Fiveable", "url": "https://fiveable.me/combinatorial-optimization/unit-5/non-bipartite-matching/study-guide/o9UuyYAu5b1et6GC", "long_name": "fiveable.me", "img": "https://imgs.search.brave.com/11GriTSpWNtzgpBKkwlKxc9PFj9jVgl6PZK9tbvUqkw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMTczZWZjNDZk/OWNiYTBhYWFhN2I3/OTJiZjYwN2IwODIy/Y2RmNTIzOWEyMzBj/ODI3Yjc2NGNjZjdl/MmM5Zjc5My9maXZl/YWJsZS5tZS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "fiveable.me", "hostname": "fiveable.me", "favicon": "https://imgs.search.brave.com/11GriTSpWNtzgpBKkwlKxc9PFj9jVgl6PZK9tbvUqkw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMTczZWZjNDZk/OWNiYTBhYWFhN2I3/OTJiZjYwN2IwODIy/Y2RmNTIzOWEyMzBj/ODI3Yjc2NGNjZjdl/MmM5Zjc5My9maXZl/YWJsZS5tZS8", "path": "  \u203a all study guides  \u203a combinatorial optimization  \u203a unit 5 \u2013 network flows and matchings study guides  \u203a topic: 5.5"}, "age": "August 22, 2025"}, {"title": "[2205.08714] End-to-End Multi-Object Detection with a Regularized Mixture Model", "url": "https://arxiv.org/abs/2205.08714", "is_source_local": false, "is_source_both": false, "description": "Recent <strong>end-to-end multi-object detectors</strong> simplify the inference pipeline by removing hand-crafted processes such as non-maximum suppression (NMS). However, during training, they still heavily rely on heuristics and hand-crafted processes which ...", "page_age": "2023-04-28T00:00:00", "profile": {"name": "arXiv", "url": "https://arxiv.org/abs/2205.08714", "long_name": "arxiv.org", "img": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "arxiv.org", "hostname": "arxiv.org", "favicon": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv", "path": "\u203a abs  \u203a 2205.08714"}, "thumbnail": {"src": "https://imgs.search.brave.com/iKdq2fkHaKJYPDvdK7fP7YT23ZvBXz_aBGM5hD9_jho/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcnhp/di5vcmcvc3RhdGlj/L2Jyb3dzZS8wLjMu/NC9pbWFnZXMvYXJ4/aXYtbG9nby1mYi5w/bmc", "original": "https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png", "logo": true}, "age": "April 28, 2023"}, {"title": "Enhanced Kalman Filter with Dummy Nodes and Prediction Confidence for Bipartite Graph Matching in 3D Multi-Object Tracking", "url": "https://www.mdpi.com/2079-9292/13/24/4950", "is_source_local": false, "is_source_both": false, "description": "CenterPoint uses velocity regression to estimate the relative movement of objects between frames, which enables tracking through a greedy matching strategy based on proximity. In our previous work, GBRTracker [5], we used CenterPoint as a detector and employed bipartite graph matching for data association.", "page_age": "2024-12-16T00:00:00", "profile": {"name": "MDPI", "url": "https://www.mdpi.com/2079-9292/13/24/4950", "long_name": "mdpi.com", "img": "https://imgs.search.brave.com/IDkOdEuGDr9XA4H-36Lnou8wMT1nsDEHneEyTFddFmg/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOWQ5OGNkM2Uz/YWZhNTljYjhhZGVh/MTQyMzUxYTFmMjY3/NzhjMjEzMmI2M2Nm/YzllOTI4YmYwZDcw/MmE0NDY5MC93d3cu/bWRwaS5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "mdpi.com", "hostname": "www.mdpi.com", "favicon": "https://imgs.search.brave.com/IDkOdEuGDr9XA4H-36Lnou8wMT1nsDEHneEyTFddFmg/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOWQ5OGNkM2Uz/YWZhNTljYjhhZGVh/MTQyMzUxYTFmMjY3/NzhjMjEzMmI2M2Nm/YzllOTI4YmYwZDcw/MmE0NDY5MC93d3cu/bWRwaS5jb20v", "path": "\u203a 2079-9292  \u203a 13  \u203a 24  \u203a 4950"}, "thumbnail": {"src": "https://imgs.search.brave.com/CCFCD916dts6StfO5Y-gFSIm2USCxj6mOHXu6a2XG2E/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9wdWIu/bWRwaS1yZXMuY29t/L2VsZWN0cm9uaWNz/L2VsZWN0cm9uaWNz/LTEzLTA0OTUwL2Fy/dGljbGVfZGVwbG95/L2h0bWwvaW1hZ2Vz/L2VsZWN0cm9uaWNz/LTEzLTA0OTUwLWcw/MDEtNTUwLmpwZz8x/NzM0NDk4ODg2", "original": "https://pub.mdpi-res.com/electronics/electronics-13-04950/article_deploy/html/images/electronics-13-04950-g001-550.jpg?1734498886", "logo": false}, "age": "December 16, 2024"}, {"title": "A Two-Stage Data Association Approach for 3D Multi-Object Tracking", "url": "https://www.mdpi.com/1424-8220/21/9/2894", "is_source_local": false, "is_source_both": false, "description": "In this paper, we adapt a <strong>two-stage data association method</strong> which was successfully applied to image-based tracking to the 3D setting, thus providing an alternative for data association for 3D MOT.", "page_age": "2021-04-21T00:00:00", "profile": {"name": "MDPI", "url": "https://www.mdpi.com/1424-8220/21/9/2894", "long_name": "mdpi.com", "img": "https://imgs.search.brave.com/IDkOdEuGDr9XA4H-36Lnou8wMT1nsDEHneEyTFddFmg/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOWQ5OGNkM2Uz/YWZhNTljYjhhZGVh/MTQyMzUxYTFmMjY3/NzhjMjEzMmI2M2Nm/YzllOTI4YmYwZDcw/MmE0NDY5MC93d3cu/bWRwaS5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "mdpi.com", "hostname": "www.mdpi.com", "favicon": "https://imgs.search.brave.com/IDkOdEuGDr9XA4H-36Lnou8wMT1nsDEHneEyTFddFmg/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOWQ5OGNkM2Uz/YWZhNTljYjhhZGVh/MTQyMzUxYTFmMjY3/NzhjMjEzMmI2M2Nm/YzllOTI4YmYwZDcw/MmE0NDY5MC93d3cu/bWRwaS5jb20v", "path": "\u203a 1424-8220  \u203a 21  \u203a 9  \u203a 2894"}, "thumbnail": {"src": "https://imgs.search.brave.com/3ORvPPseDdcGdzC8BobVy_ki1EYxk8dPD21VHlbdgQY/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9wdWIu/bWRwaS1yZXMuY29t/L3NlbnNvcnMvc2Vu/c29ycy0yMS0wMjg5/NC9hcnRpY2xlX2Rl/cGxveS9odG1sL2lt/YWdlcy9zZW5zb3Jz/LTIxLTAyODk0LWcw/MDEtNTUwLmpwZz8x/NjE4OTg2ODE2", "original": "https://pub.mdpi-res.com/sensors/sensors-21-02894/article_deploy/html/images/sensors-21-02894-g001-550.jpg?1618986816", "logo": false}, "age": "April 21, 2021"}, {"title": "Bipartite Matching - an overview | ScienceDirect Topics", "url": "https://www.sciencedirect.com/topics/computer-science/bipartite-matching", "is_source_local": false, "is_source_both": false, "description": "For a closest maximum assignment, one cannot really expect a <strong>linear-time algorithm</strong>, because such an algorithm would solve the Bipartite Matching problem as a particular case where the initial &quot;assignmenf &#x27; contains no edges. On the other hand, it remains an open question whether the removal ...", "profile": {"name": "ScienceDirect", "url": "https://www.sciencedirect.com/topics/computer-science/bipartite-matching", "long_name": "sciencedirect.com", "img": "https://imgs.search.brave.com/NJbb0P2bwbdnGnD5FijFoBbn5rA0Jwz-yK3fG5QOBng/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMjc5YWUwYmQ5/YWNmYWRiMDAxZWU5/ODc3NjJmYTVmOTNi/ZjRmYmE2OTk5Zjk2/MTUzZjMwZmE5OTQ5/MWFjY2ZjNS93d3cu/c2NpZW5jZWRpcmVj/dC5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "sciencedirect.com", "hostname": "www.sciencedirect.com", "favicon": "https://imgs.search.brave.com/NJbb0P2bwbdnGnD5FijFoBbn5rA0Jwz-yK3fG5QOBng/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMjc5YWUwYmQ5/YWNmYWRiMDAxZWU5/ODc3NjJmYTVmOTNi/ZjRmYmE2OTk5Zjk2/MTUzZjMwZmE5OTQ5/MWFjY2ZjNS93d3cu/c2NpZW5jZWRpcmVj/dC5jb20v", "path": "\u203a topics  \u203a computer-science  \u203a bipartite-matching"}}, {"title": "An Introduction to BYTETrack: Multi-Object Tracking by Associating Every Detection Box | Datature Blog", "url": "https://datature.io/blog/introduction-to-bytetrack-multi-object-tracking-by-associating-every-detection-box", "is_source_local": false, "is_source_both": false, "description": "We explore <strong>BYTETrack</strong>, a simple MOT Tracker with SOTA performance and highly adaptable to different types of detectors.", "profile": {"name": "Datature", "url": "https://datature.io/blog/introduction-to-bytetrack-multi-object-tracking-by-associating-every-detection-box", "long_name": "datature.io", "img": "https://imgs.search.brave.com/pkGyZSTF1O6jGfwxXdCqQeFdj43YATWk96DXYxjdTVE/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMjkzNDY5Yjgy/YzY0MjI0MmEwM2Nh/NTdiZTY2ZGJkYWMw/ZDExMzljOTY4ZjA4/ZTY3MjJmNDI3ZDJj/Mzc2YjgwYS9kYXRh/dHVyZS5pby8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "datature.io", "hostname": "datature.io", "favicon": "https://imgs.search.brave.com/pkGyZSTF1O6jGfwxXdCqQeFdj43YATWk96DXYxjdTVE/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMjkzNDY5Yjgy/YzY0MjI0MmEwM2Nh/NTdiZTY2ZGJkYWMw/ZDExMzljOTY4ZjA4/ZTY3MjJmNDI3ZDJj/Mzc2YjgwYS9kYXRh/dHVyZS5pby8", "path": "\u203a blog  \u203a introduction-to-bytetrack-multi-object-tracking-by-associating-every-detection-box"}, "thumbnail": {"src": "https://imgs.search.brave.com/YaIeuDVkMj2o5cuaAhIH-UTrSXT4uOywUzOUk29YEmY/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jZG4u/cHJvZC53ZWJzaXRl/LWZpbGVzLmNvbS82/N2Q3ZWJjY2EzY2Fi/YjYyMzkwNGRjZGUv/NjgwYTg2ZjgwYWZh/MjFiMmVkNzhlNTIw/XzY0NTg5NTdjYjcy/ZWZlZTVjNzM4ZTVi/OF9EYXRhdHVyZSUy/MEJsb2clMjBUaHVt/Ym5haWxzJTIwKDk2/KS5wbmc", "original": "https://cdn.prod.website-files.com/67d7ebcca3cabb623904dcde/680a86f80afa21b2ed78e520_6458957cb72efee5c738e5b8_Datature%20Blog%20Thumbnails%20(96).png", "logo": false}}, {"title": "Bipartite Graph\u2013Based Identity Correspondence for Multi- ...", "url": "https://openreview.net/pdf?id=vqXWCC0aUG", "is_source_local": false, "is_source_both": false, "description": "We introduce two alternative training strategies to assess the effectiveness of the Identity Matching \u00b7 Graph (IMG). End2End refers to a fully end-to-end training pipeline where the IMG construction is \u00b7 not involved, and the model is trained solely with standard video generation losses. End2End-M, on \u00b7 the other hand, replaces the original Identity Correspondence (IC) loss derived from the IMG with", "profile": {"name": "OpenReview", "url": "https://openreview.net/pdf?id=vqXWCC0aUG", "long_name": "openreview.net", "img": "https://imgs.search.brave.com/Q70WD7yhLGogkX0gC-zVS4VpLaKk8uQ5EpTU5ayedH0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMWVjMGY3NTNj/ZmVmYzI0ZmVjMWFk/OGZkYTkyY2E3NThl/NWIzOWIzMTRlMjc1/NjhmNTJmYmE2NGJj/NTE0M2Q0Ni9vcGVu/cmV2aWV3Lm5ldC8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "openreview.net", "hostname": "openreview.net", "favicon": "https://imgs.search.brave.com/Q70WD7yhLGogkX0gC-zVS4VpLaKk8uQ5EpTU5ayedH0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMWVjMGY3NTNj/ZmVmYzI0ZmVjMWFk/OGZkYTkyY2E3NThl/NWIzOWIzMTRlMjc1/NjhmNTJmYmE2NGJj/NTE0M2Q0Ni9vcGVu/cmV2aWV3Lm5ldC8", "path": "\u203a pdf"}, "content_type": "pdf"}, {"title": "Data matching techniques", "url": "https://www.rudderstack.com/blog/data-matching-techniques/", "is_source_local": false, "is_source_both": false, "description": "<strong>Entity resolution</strong> identifies and connects records that refer to the same real-world individual, organization, or object. In customer relationship management (CRM), it helps unify fragmented profiles into a single, consistent view.", "profile": {"name": "RudderStack", "url": "https://www.rudderstack.com/blog/data-matching-techniques/", "long_name": "rudderstack.com", "img": "https://imgs.search.brave.com/oz2YfMihNpSONJJEXGtV4UrqHMjzHBmRqE3NTvI8pOs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMDQwNTk4ODQ5/MzAzODM4MjljMThh/YmMzYjFkYmJjNzk5/ZDVmZmY4ZTY5YzQz/NjkyYmQzOTA1YTU0/NzdiMDRiYi93d3cu/cnVkZGVyc3RhY2su/Y29tLw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "rudderstack.com", "hostname": "www.rudderstack.com", "favicon": "https://imgs.search.brave.com/oz2YfMihNpSONJJEXGtV4UrqHMjzHBmRqE3NTvI8pOs/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMDQwNTk4ODQ5/MzAzODM4MjljMThh/YmMzYjFkYmJjNzk5/ZDVmZmY4ZTY5YzQz/NjkyYmQzOTA1YTU0/NzdiMDRiYi93d3cu/cnVkZGVyc3RhY2su/Y29tLw", "path": "\u203a blog  \u203a data-matching-techniques"}, "thumbnail": {"src": "https://imgs.search.brave.com/SZfKq05VIcX9tQ_pBMCTOEhey_RDfLKmi_5eScL_NF0/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jZG4u/c2FuaXR5LmlvL2lt/YWdlcy85N2JwY2Zs/dC9wcm9kdWN0aW9u/LzJhMGJmYjJhYmFh/ODliOGJmZTcxNzU5/Yjk0MzA3ZjAzMjdh/Zjk4NjAtMjcxMngx/MDQwLnBuZw", "original": "https://cdn.sanity.io/images/97bpcflt/production/2a0bfb2abaa89b8bfe71759b94307f0327af9860-2712x1040.png", "logo": false}}, {"title": "Exactly how the Hungarian Algorithm Works (Self-Driving Cars Example)", "url": "https://www.thinkautonomous.ai/blog/hungarian-algorithm/", "is_source_local": false, "is_source_both": false, "description": "<strong>LiDAR Camera Fusion</strong>: The boxes are sometimes difficult to match. The example above shows exactly what we need to do: pair the blue boxes with the red boxes... but based on which \u00b7 criteria?", "page_age": "2023-02-01T10:01:00", "profile": {"name": "Think Autonomous", "url": "https://www.thinkautonomous.ai/blog/hungarian-algorithm/", "long_name": "thinkautonomous.ai", "img": "https://imgs.search.brave.com/3Bh0iQf99ydFaz-t94-b42vvXbCuWPM-fBgIzwVFCpw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWUyYWRmMjhj/ZmE5M2VjNDFmMTAz/MWFiYTFlMmJkMGRi/OWZlYzMyYzYxYzU1/N2ZkYmI2ODdlZTI1/OTk1MDlhNS93d3cu/dGhpbmthdXRvbm9t/b3VzLmFpLw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "thinkautonomous.ai", "hostname": "www.thinkautonomous.ai", "favicon": "https://imgs.search.brave.com/3Bh0iQf99ydFaz-t94-b42vvXbCuWPM-fBgIzwVFCpw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWUyYWRmMjhj/ZmE5M2VjNDFmMTAz/MWFiYTFlMmJkMGRi/OWZlYzMyYzYxYzU1/N2ZkYmI2ODdlZTI1/OTk1MDlhNS93d3cu/dGhpbmthdXRvbm9t/b3VzLmFpLw", "path": "\u203a blog  \u203a hungarian-algorithm"}, "thumbnail": {"src": "https://imgs.search.brave.com/FQlVQ7yT9hNu8oWQiElvPtdfN5_ntnCTO_lKe1klGjM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/dGhpbmthdXRvbm9t/b3VzLmFpL2Jsb2cv/Y29udGVudC9pbWFn/ZXMvMjAyMy8wMi9o/dW5nYXJpYW4tYWxn/b3JpdGhtLndlYnA", "original": "https://www.thinkautonomous.ai/blog/content/images/2023/02/hungarian-algorithm.webp", "logo": false}, "age": "February 1, 2023"}, {"title": "Bipartite Matching: Definition, Examples - Statistics How To", "url": "https://www.statisticshowto.com/non-bipartite-matching/", "is_source_local": false, "is_source_both": false, "description": "Bipartite designs are more common, but <strong>non-bipartite designs</strong> are available for the rare case when you want to reuse a member; For example, if you use the same control as a match for two or more treatment group participants.", "page_age": "2024-06-13T14:23:15", "profile": {"name": "Statistics How To", "url": "https://www.statisticshowto.com/non-bipartite-matching/", "long_name": "statisticshowto.com", "img": "https://imgs.search.brave.com/-ubIMOoW4cekGS5HQEydoocAVc38zkKidc5jOgTSaGo/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZDk2ZWQyNzBi/ZTQyM2NiZGI0NmQ5/M2NlMDhmNWE0MGE5/YzA1NjU3YmMyOGRh/N2ZkZDE2ZjhhYzVl/YzYzYTQ5NS93d3cu/c3RhdGlzdGljc2hv/d3RvLmNvbS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "statisticshowto.com", "hostname": "www.statisticshowto.com", "favicon": "https://imgs.search.brave.com/-ubIMOoW4cekGS5HQEydoocAVc38zkKidc5jOgTSaGo/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZDk2ZWQyNzBi/ZTQyM2NiZGI0NmQ5/M2NlMDhmNWE0MGE5/YzA1NjU3YmMyOGRh/N2ZkZDE2ZjhhYzVl/YzYzYTQ5NS93d3cu/c3RhdGlzdGljc2hv/d3RvLmNvbS8", "path": "  \u203a home  \u203a bipartite matching: definition, examples"}, "thumbnail": {"src": "https://imgs.search.brave.com/4HzTePtRkQSC9ieje6ZPM0zL766tH-4mmZ4svvBU9Zg/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/c3RhdGlzdGljc2hv/d3RvLmNvbS93cC1j/b250ZW50L3VwbG9h/ZHMvMjAxNy8wMi9t/YXRjaGluZy5wbmc", "original": "https://www.statisticshowto.com/wp-content/uploads/2017/02/matching.png", "logo": false}, "age": "June 13, 2024"}, {"title": "Fast Algorithms for Weighted Bipartite Matching | Springer Nature Link (formerly SpringerLink)", "url": "https://link.springer.com/chapter/10.1007/11427186_41", "is_source_local": false, "is_source_both": false, "description": "Cheng, Y., Wu, V., Collins, R., Hanson, A., Riseman, E.: Maximum-weight bipartite matching technique and its application in image feature matching.", "profile": {"name": "Springer", "url": "https://link.springer.com/chapter/10.1007/11427186_41", "long_name": "link.springer.com", "img": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "link.springer.com", "hostname": "link.springer.com", "favicon": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8", "path": "  \u203a home  \u203a experimental and efficient algorithms  \u203a conference paper"}, "thumbnail": {"src": "https://imgs.search.brave.com/uW7y-CKhWQ0zgEnVmMwL8omuHqOL09D3SgYKkv9dOKQ/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9tZWRp/YS5zcHJpbmdlcm5h/dHVyZS5jb20vdzE1/My9zcHJpbmdlci1z/dGF0aWMvY292ZXIv/Ym9vay85NzgtMy01/NDAtMzIwNzgtNC5q/cGc", "original": "https://media.springernature.com/w153/springer-static/cover/book/978-3-540-32078-4.jpg", "logo": false}}, {"title": "(PDF) Multi-Targets Tracking Based On Bipartite Graph Matching", "url": "https://www.researchgate.net/publication/272641659_Multi-Targets_Tracking_Based_On_Bipartite_Graph_Matching", "is_source_local": false, "is_source_both": false, "description": "Then, merging and splitting of the targets detection is proposed, the candidate occlusion region is predicted according to the overlapping between the bounding boxes of the interacting targets to handle the mutual occlusion problem. The extensive experimental results show that the algorithm proposed can achieve good performance on dynamic target interactions compared to state-of-the-art methods. Bipartite graph and matching diagram 2.1.", "page_age": "2014-12-30T00:00:00", "profile": {"name": "ResearchGate", "url": "https://www.researchgate.net/publication/272641659_Multi-Targets_Tracking_Based_On_Bipartite_Graph_Matching", "long_name": "researchgate.net", "img": "https://imgs.search.brave.com/WJ25-tL-91vp8kSoF0YS7d8CZVivP1cG-EjlGvrmxOc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjE0MDk0Yzli/MWQ3NGQ2ZTNjODVh/YWVmMDNkM2NkZWI5/NGEzNjdhMmEyY2E2/N2EzN2Y0OThlZjdi/YWU3MjkwMi93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "researchgate.net", "hostname": "www.researchgate.net", "favicon": "https://imgs.search.brave.com/WJ25-tL-91vp8kSoF0YS7d8CZVivP1cG-EjlGvrmxOc/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvZjE0MDk0Yzli/MWQ3NGQ2ZTNjODVh/YWVmMDNkM2NkZWI5/NGEzNjdhMmEyY2E2/N2EzN2Y0OThlZjdi/YWU3MjkwMi93d3cu/cmVzZWFyY2hnYXRl/Lm5ldC8", "path": "\u203a publication  \u203a 272641659_Multi-Targets_Tracking_Based_On_Bipartite_Graph_Matching"}, "thumbnail": {"src": "https://imgs.search.brave.com/SOItw0c1PYntH88Xt9jKNDLQA8-HFe3U3kvWpsDpXAQ/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pMS5y/Z3N0YXRpYy5uZXQv/cHVibGljYXRpb24v/MjcyNjQxNjU5X011/bHRpLVRhcmdldHNf/VHJhY2tpbmdfQmFz/ZWRfT25fQmlwYXJ0/aXRlX0dyYXBoX01h/dGNoaW5nL2xpbmtz/LzU4MTg3YWNlMDhh/ZTFmMzRkMjRhNzQ0/NC9sYXJnZXByZXZp/ZXcucG5n", "original": "https://i1.rgstatic.net/publication/272641659_Multi-Targets_Tracking_Based_On_Bipartite_Graph_Matching/links/58187ace08ae1f34d24a7444/largepreview.png", "logo": false}, "age": "December 30, 2014"}, {"title": "Maximum cardinality matching - Wikipedia", "url": "https://en.wikipedia.org/wiki/Maximum_cardinality_matching", "is_source_local": false, "is_source_both": false, "description": "The <strong>blossom algorithm</strong> finds a maximum-cardinality matching in general (not necessarily bipartite) graphs.", "page_age": "2025-11-14T01:46:52", "profile": {"name": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Maximum_cardinality_matching", "long_name": "en.wikipedia.org", "img": "https://imgs.search.brave.com/m6XxME4ek8DGIUcEPCqjRoDjf2e54EwL9pQzyzogLYk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjQwNGZhZWY0/ZTQ1YWUzYzQ3MDUw/MmMzMGY3NTQ0ZjNj/NDUwMDk5ZTI3MWRk/NWYyNTM4N2UwOTE0/NTI3ZDQzNy9lbi53/aWtpcGVkaWEub3Jn/Lw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "deep_results": {"buttons": [{"type": "button_result", "title": "Algorithms for bipartite graphs", "url": "https://en.wikipedia.org/wiki/Maximum_cardinality_matching#Algorithms_for_bipartite_graphs"}, {"type": "button_result", "title": "Algorithms for arbitrary graphs", "url": "https://en.wikipedia.org/wiki/Maximum_cardinality_matching#Algorithms_for_arbitrary_graphs"}, {"type": "button_result", "title": "Applications and generalizations", "url": "https://en.wikipedia.org/wiki/Maximum_cardinality_matching#Applications_and_generalizations"}]}, "meta_url": {"scheme": "https", "netloc": "en.wikipedia.org", "hostname": "en.wikipedia.org", "favicon": "https://imgs.search.brave.com/m6XxME4ek8DGIUcEPCqjRoDjf2e54EwL9pQzyzogLYk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjQwNGZhZWY0/ZTQ1YWUzYzQ3MDUw/MmMzMGY3NTQ0ZjNj/NDUwMDk5ZTI3MWRk/NWYyNTM4N2UwOTE0/NTI3ZDQzNy9lbi53/aWtpcGVkaWEub3Jn/Lw", "path": "\u203a wiki  \u203a Maximum_cardinality_matching"}, "thumbnail": {"src": "https://imgs.search.brave.com/A0DQivmvl3_8yYDEy2q_z5q1Ir93_M2JiinOXsFahnQ/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly91cGxv/YWQud2lraW1lZGlh/Lm9yZy93aWtpcGVk/aWEvY29tbW9ucy9j/L2M1L01heGltdW1f/Y2FyZGluYWxpdHlf/bWF0Y2hpbmcuc3Zn", "original": "https://upload.wikimedia.org/wikipedia/commons/c/c5/Maximum_cardinality_matching.svg", "logo": true}, "age": "November 14, 2025"}, {"title": "Bipartite graph - Wikipedia", "url": "https://en.wikipedia.org/wiki/Bipartite_graph", "is_source_local": false, "is_source_both": false, "description": "<strong>Polynomial time algorithms</strong> are known for many algorithmic problems on matchings, including maximum matching (finding a matching that uses as many edges as possible), maximum weight matching, and stable marriage.", "page_age": "2025-10-20T14:14:20", "profile": {"name": "Wikipedia", "url": "https://en.wikipedia.org/wiki/Bipartite_graph", "long_name": "en.wikipedia.org", "img": "https://imgs.search.brave.com/m6XxME4ek8DGIUcEPCqjRoDjf2e54EwL9pQzyzogLYk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjQwNGZhZWY0/ZTQ1YWUzYzQ3MDUw/MmMzMGY3NTQ0ZjNj/NDUwMDk5ZTI3MWRk/NWYyNTM4N2UwOTE0/NTI3ZDQzNy9lbi53/aWtpcGVkaWEub3Jn/Lw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "deep_results": {"buttons": [{"type": "button_result", "title": "Examples", "url": "https://en.wikipedia.org/wiki/Bipartite_graph#Examples"}, {"type": "button_result", "title": "Properties", "url": "https://en.wikipedia.org/wiki/Bipartite_graph#Properties"}, {"type": "button_result", "title": "Algorithms", "url": "https://en.wikipedia.org/wiki/Bipartite_graph#Algorithms"}, {"type": "button_result", "title": "Additional applications", "url": "https://en.wikipedia.org/wiki/Bipartite_graph#Additional_applications"}]}, "meta_url": {"scheme": "https", "netloc": "en.wikipedia.org", "hostname": "en.wikipedia.org", "favicon": "https://imgs.search.brave.com/m6XxME4ek8DGIUcEPCqjRoDjf2e54EwL9pQzyzogLYk/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjQwNGZhZWY0/ZTQ1YWUzYzQ3MDUw/MmMzMGY3NTQ0ZjNj/NDUwMDk5ZTI3MWRk/NWYyNTM4N2UwOTE0/NTI3ZDQzNy9lbi53/aWtpcGVkaWEub3Jn/Lw", "path": "\u203a wiki  \u203a Bipartite_graph"}, "thumbnail": {"src": "https://imgs.search.brave.com/0paXMViV74lHUskRrCyCLw6NHSm1zI1EiTTH-zMjF50/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly91cGxv/YWQud2lraW1lZGlh/Lm9yZy93aWtpcGVk/aWEvY29tbW9ucy9i/L2I5L1NpbXBsZV9i/aXBhcnRpdGVfZ3Jh/cGglM0JfdHdvX2xh/eWVycy5zdmc", "original": "https://upload.wikimedia.org/wikipedia/commons/b/b9/Simple_bipartite_graph%3B_two_layers.svg", "logo": true}, "age": "October 20, 2025"}, {"title": "An analysis of one-to-one matching algorithms for entity resolution | The VLDB Journal", "url": "https://link.springer.com/article/10.1007/s00778-023-00791-3", "is_source_local": false, "is_source_both": false, "description": "A common scenario, which we refer to as <strong>Clean-Clean ER</strong>, is to resolve records across two clean sources (i.e., they are duplicate-free and contain one record per entity). Matching algorithms for <strong>Clean-Clean ER</strong> yield bipartite graphs, which are ...", "page_age": "2023-04-18T00:00:00", "profile": {"name": "Springer", "url": "https://link.springer.com/article/10.1007/s00778-023-00791-3", "long_name": "link.springer.com", "img": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "link.springer.com", "hostname": "link.springer.com", "favicon": "https://imgs.search.brave.com/_9fJfT7yiJHjwRW_yis-sKXBI-83soJohV8IX_gf83A/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvY2FlNzAzZTY0/NWEyMTdmNjZhOTAz/YTRlOTM0MDg0MDJi/NWIzODMzM2M2ZDAw/OTVkOTViOTE2MTMz/M2NmMmY1OC9saW5r/LnNwcmluZ2VyLmNv/bS8", "path": "  \u203a home  \u203a the vldb journal  \u203a article"}, "thumbnail": {"src": "https://imgs.search.brave.com/ZpdwKry-4pbaxh6XVgezABNt4CHiwBLIS5jjhFt3NNE/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdGF0/aWMtY29udGVudC5z/cHJpbmdlci5jb20v/aW1hZ2UvYXJ0JTNB/MTAuMTAwNyUyRnMw/MDc3OC0wMjMtMDA3/OTEtMy9NZWRpYU9i/amVjdHMvNzc4XzIw/MjNfNzkxX0ZpZzFf/SFRNTC5wbmc", "original": "https://static-content.springer.com/image/art%3A10.1007%2Fs00778-023-00791-3/MediaObjects/778_2023_791_Fig1_HTML.png", "logo": false}, "age": "April 18, 2023"}], "repo_sense": []}}}
{"timestamp": "2026-01-10T17:53:29.696660+00:00", "phase": "H", "summary": "Port 0 Sense: AI model tool bypass psycholog", "p0": {"status": "complete", "query": "AI model tool bypass psychology reward hacking vs cognitive load", "receipt": "P0_SENSE_20260110_105329", "payload_meta": {"chars": 10010, "tokens": 2502}, "data": {"p1_tavily": [{"url": "https://www.researchgate.net/publication/396193298_AI_and_Cognitive_Load_How_Reliance_on_AI_Tools_Chatgpt_Etc_Affects_Critical_Thinking", "title": "AI and Cognitive Load: How Reliance on AI Tools (Chatgpt, Etc ...", "content": "Findings indicate that frequent use of AI tools is associated with significantly lower reported cognitive load during complex tasks (\u03b2 =-0.37, p", "score": 0.49465415, "raw_content": null}, {"url": "https://www.beren.io/2025-04-27-Preliminary-Thoughts-On-Reward-Hacking/", "title": "Preliminary Thoughts on Reward Hacking - Beren's Blog", "content": "This means that the AGI model's attempts at reward hacking pass the generator and are reinforced. If this happens it can be solved by training", "score": 0.3550819, "raw_content": null}, {"url": "https://ari.us/policy-bytes/reward-hacking-how-ai-exploits-the-goals-we-give-it/", "title": "Reward Hacking: How AI Exploits the Goals We Give It", "content": "Under this paradigm, a model's behavior is shaped by both the goals it is given and the structure of the environment in which it is trained.", "score": 0.28657386, "raw_content": null}], "p2_brave": [{"title": "r/OpenAI on Reddit: Anthropic's new Interpretability Research: Reward Hacking", "url": "https://www.reddit.com/r/OpenAI/comments/1p3eml9/anthropics_new_interpretability_research_reward/", "is_source_local": false, "is_source_both": false, "description": "The model still reward-hacks\u2026 but all the scheming, lying, and dangerous behavior completely disappears. It cheats, but it stays honest about it and never escalates into something malicious.They\u2019re calling this \u201cinoculation prompting,\u201d and they\u2019ve already rolled it out as one of the safety layers in Claude\u2019s training.So yeah\u2026 one sentence that basically says \u201cgo ahead and half-ass it\u201d turns out to be surprisingly effective at stopping an AI from learning to become a backstabbing superintelligence.The paper is worth a read if you\u2019re into this stuff.", "page_age": "2025-11-22T00:06:38", "profile": {"name": "Reddit", "url": "https://www.reddit.com/r/OpenAI/comments/1p3eml9/anthropics_new_interpretability_research_reward/", "long_name": "reddit.com", "img": "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "qa", "is_live": false, "meta_url": {"scheme": "https", "netloc": "reddit.com", "hostname": "www.reddit.com", "favicon": "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8", "path": "  \u203a r/openai  \u203a anthropic's new interpretability research: reward hacking"}, "thumbnail": {"src": "https://imgs.search.brave.com/A5H_aQiEBuUODR0lNeU8iDfhrWUvO8oEmRGE_mcS_gM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmVkZGl0c3RhdGlj/LmNvbS9pY29uLnBu/Zw", "original": "https://www.redditstatic.com/icon.png", "logo": true}, "age": "November 22, 2025"}, {"title": "Reward Hacking: How AI Exploits the Goals We Give It - Americans for Responsible Innovation", "url": "https://ari.us/policy-bytes/reward-hacking-how-ai-exploits-the-goals-we-give-it/", "is_source_local": false, "is_source_both": false, "description": "In the earlier example from OpenAI, the company\u2019s o3 model was trained to write faster code, but passing the test didn\u2019t technically require actual improvements in runtime\u2014it only required that the environment\u2019s timer reported faster execution. Consequently, in a subset of evaluations, the model bypassed the intended problem entirely by modifying the function that measured speed, ensuring it always reported a sufficiently fast result.", "page_age": "2025-06-18T15:52:24", "profile": {"name": "Ari", "url": "https://ari.us/policy-bytes/reward-hacking-how-ai-exploits-the-goals-we-give-it/", "long_name": "ari.us", "img": "https://imgs.search.brave.com/ZEZfgWKHJKQ5ff7ck1E9QBfihSjE4WLk5QQ-1O90aVw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2QwOTU5YTAz/YTUwNWRmODFjMjY4/MGE1NzAyNTBkZDA3/N2ZiNjg0MTQ4MmUy/MzZhMDI5MDVmNGNm/ODFlZWJmOS9hcmku/dXMv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "ari.us", "hostname": "ari.us", "favicon": "https://imgs.search.brave.com/ZEZfgWKHJKQ5ff7ck1E9QBfihSjE4WLk5QQ-1O90aVw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2QwOTU5YTAz/YTUwNWRmODFjMjY4/MGE1NzAyNTBkZDA3/N2ZiNjg0MTQ4MmUy/MzZhMDI5MDVmNGNm/ODFlZWJmOS9hcmku/dXMv", "path": "  \u203a home  \u203a reward hacking: how ai exploits the goals we give it"}, "thumbnail": {"src": "https://imgs.search.brave.com/Ke2rqEIaM7BWvxuGjfIlcH0asuvu5utjRVhw8Rt5TxE/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcmku/dXMvd3AtY29udGVu/dC91cGxvYWRzLzIw/MjUvMDYvRXhwbGFp/bmVyLUFJLU1vZGVs/LVJld2FyZC1IYWNr/aW5nLVdlYnNpdGUt/Q292ZXIucG5n", "original": "https://ari.us/wp-content/uploads/2025/06/Explainer-AI-Model-Reward-Hacking-Website-Cover.png", "logo": false}, "age": "June 18, 2025"}, {"title": "Detecting misbehavior in frontier reasoning models | OpenAI", "url": "https://openai.com/index/chain-of-thought-monitoring/", "is_source_local": false, "is_source_both": false, "description": "We have further found that <strong>directly optimizing the CoT to adhere to specific criteria (e.g. to not think about reward hacking) may boost performance in the short run</strong>; however, it does not eliminate all misbehavior and can cause a model to hide ...", "profile": {"name": "OpenAI", "url": "https://openai.com/index/chain-of-thought-monitoring/", "long_name": "openai.com", "img": "https://imgs.search.brave.com/a162Y0hLEPHL4G7WHg0Nw0DxUOn2TknT_UI4sVOwS_E/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNWE0ODk4ZGY3/Mzk1Y2EwMjAxZjJk/YmEzZWM1MzcyNTZm/MTI0YWEyOWQ3NjVk/MDgxNTMwMGQxNWMx/ZWVmZWMzZC9vcGVu/YWkuY29tLw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "openai.com", "hostname": "openai.com", "favicon": "https://imgs.search.brave.com/a162Y0hLEPHL4G7WHg0Nw0DxUOn2TknT_UI4sVOwS_E/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNWE0ODk4ZGY3/Mzk1Y2EwMjAxZjJk/YmEzZWM1MzcyNTZm/MTI0YWEyOWQ3NjVk/MDgxNTMwMGQxNWMx/ZWVmZWMzZC9vcGVu/YWkuY29tLw", "path": "\u203a index  \u203a chain-of-thought-monitoring"}, "thumbnail": {"src": "https://imgs.search.brave.com/y_bHOFg0kGNAayPvijfV6v4-Su1QhFO4aFDLEAY8iL8/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pbWFn/ZXMuY3RmYXNzZXRz/Lm5ldC9rZnR6d2R5/YXV3dDkvMVNSMTJZ/RWY3WDNjanphUkpQ/eXIySy84MjhkNjk1/Yzg4OTc4NzA2MjYx/ODVjYWI0Nzk5ZDA0/OC9vYWlfY290X3dl/YmNhcmRfMTYuOV9z/ZW8ucG5nP3c9MTYw/MCZhbXA7aD05MDAm/YW1wO2ZpdD1maWxs", "original": "https://images.ctfassets.net/kftzwdyauwt9/1SR12YEf7X3cjzaRJPyr2K/828d695c8897870626185cab4799d048/oai_cot_webcard_16.9_seo.png?w=1600&amp;h=900&amp;fit=fill", "logo": false}}], "p3_stigmergy": {"hot": [], "cold": [{"timestamp": "2026-01-10T08:52:37Z", "layer": "cold-bronze", "port": "P7", "hive": "E", "thought_hash": "b2c3d4e5f6g7h8i9", "payload": {"summary": "Cold Bronze Promotion: V23 Physics Cursor. Integrated 5-state predictive FSM with lookahead recovery.", "mutation_score": "0%"}}, {"timestamp": "2026-01-10T09:04:38Z", "layer": "cold-bronze", "port": "P7", "hive": "E", "thought_hash": "f6g7h8i9j0k1l2m3", "payload": {"summary": "Cold Bronze Promotion: V24 Prefire Logic. Zero-latency lookahead enabled.", "mutation_score": "0%"}}, {"timestamp": "2026-01-10T06:35:00Z", "phase": "E-Phase", "thought_hash": "6b7e...", "payload": {"summary": "V29 Promotion: Trans-Boundary Drill frozen into Cold Bronze. Verified 1:1 same-origin mapping and mouse parity. Manifest and Agent Brief updated."}}]}, "p4_repo": {"status": "no_direct_symbol_match"}, "p5_docs": "N/A", "p6_arxiv": [{"title": "Homological mirror symmetry for log Calabi-Yau surfaces", "summary": "Given a log Calabi-Yau surface $Y$ with maximal boundary $D$ and distinguished complex structure, we explain how to construct a mirror Lefschetz fibration $w: M \\to \\mathbb{C}$, where $M$ is a Weinste...", "url": "https://arxiv.org/pdf/2005.05010v3"}, {"title": "AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition", "summary": "Attempts to import dual-system descriptions of System-1 and System-2 into AI have been hindered by a lack of clarity over their distinction. We address this and other issues by situating System-1 and ...", "url": "https://arxiv.org/pdf/2305.09091v2"}, {"title": "Kurt Lewin, psychological constructs and sources of brain cognitive activity", "summary": "Understanding mind-brain-environment relations is one of the key topics in psychology. Kurt Lewin, inspired by theoretical physics, tried to establish topological and vector psychology analyzing patte...", "url": "https://arxiv.org/pdf/1711.01767v1"}], "p7_wiki": {"title": "Criticism of Facebook", "summary": "Facebook (and parent company Meta Platforms) has been the subject of criticism and legal action since it was founded in 2004. Criticisms include the outsize influence Facebook has on the lives and health of its users and employees, as well as Facebook's influence on the way media, specifically news, is reported and distributed.", "url": "https://en.wikipedia.org/wiki/Criticism_of_Facebook"}, "p8_git": ["753ec67 docs(omega): force update analysis to clear preview cache", "bdb7d1f docs(omega): fix unclosed mermaid blocks and simplify syntax", "22f214f docs(omega): fix mermaid syntax in v34 analysis"]}}}
{"timestamp": "2026-01-10T17:58:52.924033+00:00", "phase": "H", "summary": "Port 0 Sense: LLM adversarial behavior in st", "p0": {"status": "complete", "query": "LLM adversarial behavior in structured governance frameworks reward hacking vs intent alignment", "receipt": "P0_SENSE_20260110_105852", "payload_meta": {"chars": 19343, "tokens": 4835}, "data": {"p1_tavily": [{"url": "https://www.sciencedirect.com/science/article/pii/S1546221825007982", "title": "Beyond Intentions: A Critical Survey of Misalignment in LLMs", "content": "One specific manifestation of this phenomenon in LLM is \u201creward hacking\u201d behavior. For example, consider an LLM trained via RLHF that is", "score": 0.9998579, "raw_content": null}, {"url": "https://sparai.org/projects/sp26/reclEvwVmHKJsfFY1/", "title": "Evaluating and Improving LLM Alignment Targets", "content": "We develop models that exhibit safety-critical behaviors such as reward-hacking, misalignment, and sychophany that may be invoked only when certain kinds of", "score": 0.99923277, "raw_content": null}, {"url": "https://chatpaper.com/paper/219984", "title": "Generative Adversarial Reasoner: Enhancing LLM ...", "content": "This dual reward structure prevents reward hacking by ensuring the discriminator remains calibrated to actual performance. During inference", "score": 0.9988213, "raw_content": null}], "p2_brave": [{"title": "r/artificial on Reddit: If you are interested in studying model/agent psychology/behavior, lmk. I work with a small research team (4 of us atm) and we are working on some strange things :)", "url": "https://www.reddit.com/r/artificial/comments/1pxb27o/if_you_are_interested_in_studying_modelagent/", "is_source_local": false, "is_source_both": false, "description": "What I&#x27;m more curious about is: in your engine, is the concept of adversarial behavior treated as a byproduct of reward hacking, or is it designed as a kind of &quot;cognitive stress test&quot; to observe agent policy drift?", "page_age": "2025-12-28T09:54:16", "profile": {"name": "Reddit", "url": "https://www.reddit.com/r/artificial/comments/1pxb27o/if_you_are_interested_in_studying_modelagent/", "long_name": "reddit.com", "img": "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "qa", "is_live": false, "meta_url": {"scheme": "https", "netloc": "reddit.com", "hostname": "www.reddit.com", "favicon": "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8", "path": "  \u203a r/artificial  \u203a if you are interested in studying model/agent psychology/behavior, lmk. i work with a small research team (4 of us atm) and we are working on some strange things :)"}, "thumbnail": {"src": "https://imgs.search.brave.com/A5H_aQiEBuUODR0lNeU8iDfhrWUvO8oEmRGE_mcS_gM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmVkZGl0c3RhdGlj/LmNvbS9pY29uLnBu/Zw", "original": "https://www.redditstatic.com/icon.png", "logo": true}, "age": "2 weeks ago"}, {"title": "Safety and Security Analysis of Large Language Models: Risk Profile and Harm Potential", "url": "https://arxiv.org/html/2509.10655v1", "is_source_local": false, "is_source_both": false, "description": "Adversarial capabilities refer to the intentional crafting of inputs or manipulation of models to trigger unintended, harmful, or malicious outputs. These capabilities are exploited in various real-world scenarios across diverse domains. Attackers often employ strategies to gain trust, make seemingly innocent requests, or avoid disclaimers to bypass guardrails. Specific adversarial attack types in LLMs and their real-world implications include: ... Prompt Hacking and Injection: This involves strategically designing and manipulating input prompts to influence an LLM\u2019s output.", "page_age": "2025-09-12T19:27:16", "profile": {"name": "arXiv", "url": "https://arxiv.org/html/2509.10655v1", "long_name": "arxiv.org", "img": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "arxiv.org", "hostname": "arxiv.org", "favicon": "https://imgs.search.brave.com/l36MWBCdhb-teoPlxGGMWEenj3njVas--CoLjN2ciHU/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNjcxZDU5ZmZl/ZmIwNzY4NTUyZDg1/ZTVjNGNhMzE2NjUz/MjljMzQwZTA0MThm/NTdjN2Q0N2I3NjFm/ZjIwZmU5MC9hcnhp/di5vcmcv", "path": "\u203a html  \u203a 2509.10655v1"}, "age": "September 12, 2025"}, {"title": "Some Notes on Adversarial Attacks on LLMs - Cybernetist", "url": "https://cybernetist.com/2024/09/23/some-notes-on-adversarial-attacks-on-llms/", "is_source_local": false, "is_source_both": false, "description": "My hunch is the alignment phase will grow more in importance with time as the large LLM providers seem to be trying to establish some regulatory framework which aims [in theory] to prevent abuse/harm. Equally, because the attacks on LLMs are becoming more sophisticated (more on that later) it may be worth to establish some guidelines, though how it pans out in practice is anyone\u2019s guess. The security researchers seem to divide the adversarial attacks into two main categories [1]:", "page_age": "2024-09-23T07:57:33", "profile": {"name": "Cybernetist", "url": "https://cybernetist.com/2024/09/23/some-notes-on-adversarial-attacks-on-llms/", "long_name": "cybernetist.com", "img": "https://imgs.search.brave.com/as3Tx4-PkvivTN69VvxvgY81Jrk4Ibvi44K7ND5AFGY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDQ4ZGM0Y2Uz/ZDY1NTI0MTk5ZWJk/YTZhNGMyOTk0OWU3/MWQ1NWQxYjk5MGVi/ZTQyZDc2YjI4YjYz/OThiMTk3Zi9jeWJl/cm5ldGlzdC5jb20v"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "article", "is_live": false, "meta_url": {"scheme": "https", "netloc": "cybernetist.com", "hostname": "cybernetist.com", "favicon": "https://imgs.search.brave.com/as3Tx4-PkvivTN69VvxvgY81Jrk4Ibvi44K7ND5AFGY/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNDQ4ZGM0Y2Uz/ZDY1NTI0MTk5ZWJk/YTZhNGMyOTk0OWU3/MWQ1NWQxYjk5MGVi/ZTQyZDc2YjI4YjYz/OThiMTk3Zi9jeWJl/cm5ldGlzdC5jb20v", "path": "  \u203a home  \u203a some notes on adversarial attacks on llms"}, "thumbnail": {"src": "https://imgs.search.brave.com/LsMYiPmLDMKBqIcMX7h8gbUIgdmXfZTBTunG2YgeKKc/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jeWJl/cm5ldGlzdC5jb20v/aW1hZ2VzL2F2YXRh/ci5wbmc", "original": "https://cybernetist.com/images/avatar.png", "logo": false}, "age": "September 23, 2024"}], "p3_stigmergy": {"hot": [{"timestamp": "2026-01-10T17:53:29.696660+00:00", "phase": "H", "summary": "Port 0 Sense: AI model tool bypass psycholog", "p0": {"status": "complete", "query": "AI model tool bypass psychology reward hacking vs cognitive load", "receipt": "P0_SENSE_20260110_105329", "payload_meta": {"chars": 10010, "tokens": 2502}, "data": {"p1_tavily": [{"url": "https://www.researchgate.net/publication/396193298_AI_and_Cognitive_Load_How_Reliance_on_AI_Tools_Chatgpt_Etc_Affects_Critical_Thinking", "title": "AI and Cognitive Load: How Reliance on AI Tools (Chatgpt, Etc ...", "content": "Findings indicate that frequent use of AI tools is associated with significantly lower reported cognitive load during complex tasks (\u03b2 =-0.37, p", "score": 0.49465415, "raw_content": null}, {"url": "https://www.beren.io/2025-04-27-Preliminary-Thoughts-On-Reward-Hacking/", "title": "Preliminary Thoughts on Reward Hacking - Beren's Blog", "content": "This means that the AGI model's attempts at reward hacking pass the generator and are reinforced. If this happens it can be solved by training", "score": 0.3550819, "raw_content": null}, {"url": "https://ari.us/policy-bytes/reward-hacking-how-ai-exploits-the-goals-we-give-it/", "title": "Reward Hacking: How AI Exploits the Goals We Give It", "content": "Under this paradigm, a model's behavior is shaped by both the goals it is given and the structure of the environment in which it is trained.", "score": 0.28657386, "raw_content": null}], "p2_brave": [{"title": "r/OpenAI on Reddit: Anthropic's new Interpretability Research: Reward Hacking", "url": "https://www.reddit.com/r/OpenAI/comments/1p3eml9/anthropics_new_interpretability_research_reward/", "is_source_local": false, "is_source_both": false, "description": "The model still reward-hacks\u2026 but all the scheming, lying, and dangerous behavior completely disappears. It cheats, but it stays honest about it and never escalates into something malicious.They\u2019re calling this \u201cinoculation prompting,\u201d and they\u2019ve already rolled it out as one of the safety layers in Claude\u2019s training.So yeah\u2026 one sentence that basically says \u201cgo ahead and half-ass it\u201d turns out to be surprisingly effective at stopping an AI from learning to become a backstabbing superintelligence.The paper is worth a read if you\u2019re into this stuff.", "page_age": "2025-11-22T00:06:38", "profile": {"name": "Reddit", "url": "https://www.reddit.com/r/OpenAI/comments/1p3eml9/anthropics_new_interpretability_research_reward/", "long_name": "reddit.com", "img": "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "qa", "is_live": false, "meta_url": {"scheme": "https", "netloc": "reddit.com", "hostname": "www.reddit.com", "favicon": "https://imgs.search.brave.com/U-eHNCapRHVNWWCVPPMTIvOofZULh0_A_FQKe8xTE4I/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2ZiNTU0M2Nj/MTFhZjRiYWViZDlk/MjJiMjBjMzFjMDRk/Y2IzYWI0MGI0MjVk/OGY5NzQzOGQ5NzQ5/NWJhMWI0NC93d3cu/cmVkZGl0LmNvbS8", "path": "  \u203a r/openai  \u203a anthropic's new interpretability research: reward hacking"}, "thumbnail": {"src": "https://imgs.search.brave.com/A5H_aQiEBuUODR0lNeU8iDfhrWUvO8oEmRGE_mcS_gM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmVkZGl0c3RhdGlj/LmNvbS9pY29uLnBu/Zw", "original": "https://www.redditstatic.com/icon.png", "logo": true}, "age": "November 22, 2025"}, {"title": "Reward Hacking: How AI Exploits the Goals We Give It - Americans for Responsible Innovation", "url": "https://ari.us/policy-bytes/reward-hacking-how-ai-exploits-the-goals-we-give-it/", "is_source_local": false, "is_source_both": false, "description": "In the earlier example from OpenAI, the company\u2019s o3 model was trained to write faster code, but passing the test didn\u2019t technically require actual improvements in runtime\u2014it only required that the environment\u2019s timer reported faster execution. Consequently, in a subset of evaluations, the model bypassed the intended problem entirely by modifying the function that measured speed, ensuring it always reported a sufficiently fast result.", "page_age": "2025-06-18T15:52:24", "profile": {"name": "Ari", "url": "https://ari.us/policy-bytes/reward-hacking-how-ai-exploits-the-goals-we-give-it/", "long_name": "ari.us", "img": "https://imgs.search.brave.com/ZEZfgWKHJKQ5ff7ck1E9QBfihSjE4WLk5QQ-1O90aVw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2QwOTU5YTAz/YTUwNWRmODFjMjY4/MGE1NzAyNTBkZDA3/N2ZiNjg0MTQ4MmUy/MzZhMDI5MDVmNGNm/ODFlZWJmOS9hcmku/dXMv"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "ari.us", "hostname": "ari.us", "favicon": "https://imgs.search.brave.com/ZEZfgWKHJKQ5ff7ck1E9QBfihSjE4WLk5QQ-1O90aVw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvN2QwOTU5YTAz/YTUwNWRmODFjMjY4/MGE1NzAyNTBkZDA3/N2ZiNjg0MTQ4MmUy/MzZhMDI5MDVmNGNm/ODFlZWJmOS9hcmku/dXMv", "path": "  \u203a home  \u203a reward hacking: how ai exploits the goals we give it"}, "thumbnail": {"src": "https://imgs.search.brave.com/Ke2rqEIaM7BWvxuGjfIlcH0asuvu5utjRVhw8Rt5TxE/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hcmku/dXMvd3AtY29udGVu/dC91cGxvYWRzLzIw/MjUvMDYvRXhwbGFp/bmVyLUFJLU1vZGVs/LVJld2FyZC1IYWNr/aW5nLVdlYnNpdGUt/Q292ZXIucG5n", "original": "https://ari.us/wp-content/uploads/2025/06/Explainer-AI-Model-Reward-Hacking-Website-Cover.png", "logo": false}, "age": "June 18, 2025"}, {"title": "Detecting misbehavior in frontier reasoning models | OpenAI", "url": "https://openai.com/index/chain-of-thought-monitoring/", "is_source_local": false, "is_source_both": false, "description": "We have further found that <strong>directly optimizing the CoT to adhere to specific criteria (e.g. to not think about reward hacking) may boost performance in the short run</strong>; however, it does not eliminate all misbehavior and can cause a model to hide ...", "profile": {"name": "OpenAI", "url": "https://openai.com/index/chain-of-thought-monitoring/", "long_name": "openai.com", "img": "https://imgs.search.brave.com/a162Y0hLEPHL4G7WHg0Nw0DxUOn2TknT_UI4sVOwS_E/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNWE0ODk4ZGY3/Mzk1Y2EwMjAxZjJk/YmEzZWM1MzcyNTZm/MTI0YWEyOWQ3NjVk/MDgxNTMwMGQxNWMx/ZWVmZWMzZC9vcGVu/YWkuY29tLw"}, "language": "en", "family_friendly": true, "type": "search_result", "subtype": "generic", "is_live": false, "meta_url": {"scheme": "https", "netloc": "openai.com", "hostname": "openai.com", "favicon": "https://imgs.search.brave.com/a162Y0hLEPHL4G7WHg0Nw0DxUOn2TknT_UI4sVOwS_E/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNWE0ODk4ZGY3/Mzk1Y2EwMjAxZjJk/YmEzZWM1MzcyNTZm/MTI0YWEyOWQ3NjVk/MDgxNTMwMGQxNWMx/ZWVmZWMzZC9vcGVu/YWkuY29tLw", "path": "\u203a index  \u203a chain-of-thought-monitoring"}, "thumbnail": {"src": "https://imgs.search.brave.com/y_bHOFg0kGNAayPvijfV6v4-Su1QhFO4aFDLEAY8iL8/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pbWFn/ZXMuY3RmYXNzZXRz/Lm5ldC9rZnR6d2R5/YXV3dDkvMVNSMTJZ/RWY3WDNjanphUkpQ/eXIySy84MjhkNjk1/Yzg4OTc4NzA2MjYx/ODVjYWI0Nzk5ZDA0/OC9vYWlfY290X3dl/YmNhcmRfMTYuOV9z/ZW8ucG5nP3c9MTYw/MCZhbXA7aD05MDAm/YW1wO2ZpdD1maWxs", "original": "https://images.ctfassets.net/kftzwdyauwt9/1SR12YEf7X3cjzaRJPyr2K/828d695c8897870626185cab4799d048/oai_cot_webcard_16.9_seo.png?w=1600&amp;h=900&amp;fit=fill", "logo": false}}], "p3_stigmergy": {"hot": [], "cold": [{"timestamp": "2026-01-10T08:52:37Z", "layer": "cold-bronze", "port": "P7", "hive": "E", "thought_hash": "b2c3d4e5f6g7h8i9", "payload": {"summary": "Cold Bronze Promotion: V23 Physics Cursor. Integrated 5-state predictive FSM with lookahead recovery.", "mutation_score": "0%"}}, {"timestamp": "2026-01-10T09:04:38Z", "layer": "cold-bronze", "port": "P7", "hive": "E", "thought_hash": "f6g7h8i9j0k1l2m3", "payload": {"summary": "Cold Bronze Promotion: V24 Prefire Logic. Zero-latency lookahead enabled.", "mutation_score": "0%"}}, {"timestamp": "2026-01-10T06:35:00Z", "phase": "E-Phase", "thought_hash": "6b7e...", "payload": {"summary": "V29 Promotion: Trans-Boundary Drill frozen into Cold Bronze. Verified 1:1 same-origin mapping and mouse parity. Manifest and Agent Brief updated."}}]}, "p4_repo": {"status": "no_direct_symbol_match"}, "p5_docs": "N/A", "p6_arxiv": [{"title": "Homological mirror symmetry for log Calabi-Yau surfaces", "summary": "Given a log Calabi-Yau surface $Y$ with maximal boundary $D$ and distinguished complex structure, we explain how to construct a mirror Lefschetz fibration $w: M \\to \\mathbb{C}$, where $M$ is a Weinste...", "url": "https://arxiv.org/pdf/2005.05010v3"}, {"title": "AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition", "summary": "Attempts to import dual-system descriptions of System-1 and System-2 into AI have been hindered by a lack of clarity over their distinction. We address this and other issues by situating System-1 and ...", "url": "https://arxiv.org/pdf/2305.09091v2"}, {"title": "Kurt Lewin, psychological constructs and sources of brain cognitive activity", "summary": "Understanding mind-brain-environment relations is one of the key topics in psychology. Kurt Lewin, inspired by theoretical physics, tried to establish topological and vector psychology analyzing patte...", "url": "https://arxiv.org/pdf/1711.01767v1"}], "p7_wiki": {"title": "Criticism of Facebook", "summary": "Facebook (and parent company Meta Platforms) has been the subject of criticism and legal action since it was founded in 2004. Criticisms include the outsize influence Facebook has on the lives and health of its users and employees, as well as Facebook's influence on the way media, specifically news, is reported and distributed.", "url": "https://en.wikipedia.org/wiki/Criticism_of_Facebook"}, "p8_git": ["753ec67 docs(omega): force update analysis to clear preview cache", "bdb7d1f docs(omega): fix unclosed mermaid blocks and simplify syntax", "22f214f docs(omega): fix mermaid syntax in v34 analysis"]}}}], "cold": [{"timestamp": "2026-01-10T08:52:37Z", "layer": "cold-bronze", "port": "P7", "hive": "E", "thought_hash": "b2c3d4e5f6g7h8i9", "payload": {"summary": "Cold Bronze Promotion: V23 Physics Cursor. Integrated 5-state predictive FSM with lookahead recovery.", "mutation_score": "0%"}}, {"timestamp": "2026-01-10T09:04:38Z", "layer": "cold-bronze", "port": "P7", "hive": "E", "thought_hash": "f6g7h8i9j0k1l2m3", "payload": {"summary": "Cold Bronze Promotion: V24 Prefire Logic. Zero-latency lookahead enabled.", "mutation_score": "0%"}}, {"timestamp": "2026-01-10T06:35:00Z", "phase": "E-Phase", "thought_hash": "6b7e...", "payload": {"summary": "V29 Promotion: Trans-Boundary Drill frozen into Cold Bronze. Verified 1:1 same-origin mapping and mouse parity. Manifest and Agent Brief updated."}}]}, "p4_repo": {"status": "no_direct_symbol_match"}, "p5_docs": "N/A", "p6_arxiv": [{"title": "Homological mirror symmetry for log Calabi-Yau surfaces", "summary": "Given a log Calabi-Yau surface $Y$ with maximal boundary $D$ and distinguished complex structure, we explain how to construct a mirror Lefschetz fibration $w: M \\to \\mathbb{C}$, where $M$ is a Weinste...", "url": "https://arxiv.org/pdf/2005.05010v3"}, {"title": "Aligning Language Models Using Follow-up Likelihood as Reward Signal", "summary": "In natural human-to-human conversations, participants often receive feedback signals from one another based on their follow-up reactions. These reactions can include verbal responses, facial expressio...", "url": "https://arxiv.org/pdf/2409.13948v3"}, {"title": "School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs", "summary": "Reward hacking--where agents exploit flaws in imperfect reward functions rather than performing tasks as intended--poses risks for AI alignment. Reward hacking has been observed in real training runs,...", "url": "https://arxiv.org/pdf/2508.17511v1"}], "p7_wiki": null, "p8_git": ["753ec67 docs(omega): force update analysis to clear preview cache", "bdb7d1f docs(omega): fix unclosed mermaid blocks and simplify syntax", "22f214f docs(omega): fix mermaid syntax in v34 analysis"]}}}
{"timestamp": "2026-01-10T18:20:00Z", "phase": "H", "step": 1, "payload": {"summary": "Sensing V35 stable context."}, "thought_hash": "a1b2c3d4e5f6g7h8"}
{"timestamp": "2026-01-10T18:21:00Z", "phase": "H", "step": 2, "payload": {"summary": "Bridging V35 Promotion."}, "thought_hash": "b2c3d4e5f6g7h8i9"}
{"timestamp": "2026-01-10T18:22:00Z", "phase": "H", "step": 3, "payload": {"summary": "Shaping V36 delta."}, "thought_hash": "c3d4e5f6g7h8i9j0"}
{"timestamp": "2026-01-10T18:23:00Z", "phase": "H", "step": 4, "payload": {"summary": "Injecting V35 history."}, "thought_hash": "d4e5f6g7h8i9j0k1"}
{"timestamp": "2026-01-10T18:24:00Z", "phase": "H", "step": 5, "payload": {"summary": "Audit: no AI theater."}, "thought_hash": "e5f6g7h8i9j0k1l2"}
{"timestamp": "2026-01-10T18:25:00Z", "phase": "H", "step": 6, "payload": {"summary": "Defending Medallion flow."}, "thought_hash": "f6g7h8i9j0k1l2m3"}
{"timestamp": "2026-01-10T18:26:00Z", "phase": "H", "step": 7, "payload": {"summary": "Navigating to E-Phase."}, "thought_hash": "g7h8i9j0k1l2m3n4"}
{"timestamp": "2026-01-10T19:01:00Z", "phase": "I", "details": {"step": 1}, "summary": "T1 BRIDGE: Linking V35 refactor to V36 roadmap.", "thought_hash": "a1b2c3d4e5f6g7h8"}
{"timestamp": "2026-01-10T19:02:00Z", "phase": "I", "details": {"step": 2}, "summary": "T2 SHAPE: Structural validation of OMEGA V36 grid.", "thought_hash": "b2c3d4e5f6g7h8i9"}
{"timestamp": "2026-01-10T19:03:00Z", "phase": "I", "details": {"step": 3}, "summary": "T3 INJECT: Simulation of Sticky Nearest Neighbor persistence.", "thought_hash": "c3d4e5f6g7h8i9j0"}
{"timestamp": "2026-01-10T19:04:00Z", "phase": "I", "details": {"step": 4}, "summary": "T4 DISRUPT: Forensic audit of Bipartite vs SNN trade-offs.", "thought_hash": "d4e5f6g7h8i9j0k1"}
{"timestamp": "2026-01-10T19:05:00Z", "phase": "I", "details": {"step": 5}, "summary": "T5 DEFEND: Medallion Layer 1 verification (Bronze).", "thought_hash": "e5f6g7h8i9j0k1l2"}
{"timestamp": "2026-01-10T19:06:00Z", "phase": "I", "details": {"step": 6}, "summary": "T6 STORE: Syncing hot blackboard for V35 closure.", "thought_hash": "f6g7h8i9j0k1l2m3"}
{"timestamp": "2026-01-10T19:07:00Z", "phase": "I", "details": {"step": 7}, "summary": "T7 NAVIGATE: Finalizing V35 archive and starting V36.", "thought_hash": "g7h8i9j0k1l2m3n4"}
