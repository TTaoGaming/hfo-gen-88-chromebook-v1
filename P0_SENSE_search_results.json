{
  "query": "@mediapipe/tasks-vision vision_bundle.js script global name for FilesetResolver and GestureRecognizer",
  "tavily": {
    "query": "@mediapipe/tasks-vision vision_bundle.js script global name for FilesetResolver and GestureRecognizer",
    "response_time": 2.8,
    "follow_up_questions": null,
    "answer": null,
    "images": [],
    "results": [
      {
        "url": "https://github.com/google/mediapipe/issues/4112",
        "title": "Web example code snippet throws error \u00b7 Issue #4112 - GitHub",
        "content": "1. Create a JS file gesture\\_tasks.js\n\n```\nimport{FilesetResolver, GestureRecognizer} from'./node_modules/@mediapipe/tasks-vision/vision_bundle.js'// Create task for image file processing: const vision = await FilesetResolver. forVisionTasks(\"./node_modules/@mediapipe/tasks-vision/wasm\"); const gestureRecognizer = await GestureRecognizer. createFromOptions(vision,{baseOptions{modelAssetPath\"gesture_recognizer.task\"}, numHands 2}); const image = document. getElementById(\"image\"); const gestureRecognitionResult = gestureRecognizer. recognize(image); console. log(gestureRecognitionResult)\n```\n\n### Other info / Complete Logs\n\nOn serving the page with a local/live server, following error is encountered:\n\n## Metadata\n\n## Metadata\n\n### Assignees\n\n ayushgdev\n\n### Labels [...] Javascript\n\n### Describe the actual behavior\n\nA simple copy-paste to run the given code example in this section throws error for loading of the Gesture recognizer model. This file is not present in the installed `tasks-vision` library from npm.\n\n### Describe the expected behaviour\n\nThe example should run without any error or modification required.\n\n### Standalone code/steps you may have used to try to get what you need\n\n1. Create an empty project folder and run `npm i @mediapipe/tasks` inside it\n2. Create an HTML file\n\n```\n>< html>< head> head>< body>< img srcthumbs-up.jpg alt id image>< script type module srcgesture_tasks.js> script> body> html>\n```\n\n1. Create a JS file gesture\\_tasks.js [...] Copy link\n\nAssignees\n\nLabels\n\nlegacy:handsHand tracking/gestures/etcHand tracking/gestures/etcplatform:javascriptMediaPipe Javascript issuesMediaPipe Javascript issuestype:tasksIssues related to deployment of on-device ML solutionIssues related to deployment of on-device ML solution\n\n## Description\n\nayushgdev\n\nopened on Feb 21, 2023\n\nIssue body actions\n\n### Have I written custom code (as opposed to using a stock example script provided in MediaPipe)\n\nNo\n\n### OS Platform and Distribution\n\nMac OS 13 (Ventura)\n\n### MediaPipe Tasks SDK version\n\n0.1.0-alpha-4\n\n### Task name (e.g. Image classification, Gesture recognition etc.)\n\nGesture Recognition\n\n### Programming Language and version (e.g. C++, Python, Java)\n\nJavascript\n\n### Describe the actual behavior",
        "score": 0.76300776,
        "raw_content": null
      },
      {
        "url": "https://www.npmjs.com/package/@mediapipe/tasks-vision",
        "title": "@mediapipe/tasks-vision - npm",
        "content": "```\nconst vision = await FilesetResolver.forVisionTasks( \" ); const gestureRecognizer = await GestureRecognizer.createFromModelPath(vision, \" ); const image = document.getElementById(\"image\") as HTMLImageElement; const recognitions = gestureRecognizer.recognize(image); \n```\n\nFor more information, refer to the Gesture Recognizer documentation.\n\n## Hand Landmarker\n\nThe MediaPipe Hand Landmarker task lets you detect the landmarks of the hands in an image. You can use this Task to localize key points of the hands and render visual effects over the hands. [...] For more information, refer to the Face Landmarker documentation.\n\n## Face Stylizer\n\nThe MediaPipe Face Stylizer lets you perform face stylization on images.\n\n```\nconst vision = await FilesetResolver.forVisionTasks( \" ); const faceStylizer = await FaceStylizer.createFromModelPath(vision, \" ); const image = document.getElementById(\"image\") as HTMLImageElement; const stylizedImage = faceStylizer.stylize(image); \n```\n\n## Gesture Recognizer\n\nThe MediaPipe Gesture Recognizer task lets you recognize hand gestures in real time, and provides the recognized hand gesture results along with the landmarks of the detected hands. You can use this task to recognize specific hand gestures from a user, and invoke application features that correspond to those gestures. [...] ```\nconst vision = await FilesetResolver.forVisionTasks( \" ); const handLandmarker = await HandLandmarker.createFromModelPath(vision, \" ); const image = document.getElementById(\"image\") as HTMLImageElement; const landmarks = handLandmarker.detect(image); \n```\n\nFor more information, refer to the Hand Landmarker documentation.\n\n## Holistic Landmarker\n\nThe MediaPipe Holistic Landmarker task task lets you combine components of the pose, face, and hand landmarkers to create a complete landmarker for the human body.\n\n```\nconst vision = await FilesetResolver.forVisionTasks( \" ); const holisticLandmarker = await HolisticLandmarker.createFromModelPath(vision, \" ); const image = document.getElementById(\"image\") as HTMLImageElement; const landmarks = holisticLandmarker.detect(image); \n```",
        "score": 0.7140107,
        "raw_content": null
      },
      {
        "url": "https://medium.com/@kiyo07/integrating-mediapipe-tasks-vision-for-hand-landmark-detection-in-react-a2cfb9d543c7",
        "title": "HandIntegrating @mediapipe/tasks-vision for Hand Landmark ...",
        "content": "here is how To do this:\n\n(Directly scroll to last if you just want to see the full code.)\n\n## Setting Up the Environment\n\nStep 1: Install MediaPipe.\n\n`npm i @mediapipe/tasks-vision`\n\nStep 2: Download the HandLandmark model from here.  \n(Note: For different models, refer to the models section within the MediaPipe documentation.)\n\n## Implementing in React\n\nStep 1: Import the Model and FilesetResolver.\n\n```\nimport { FilesetResolver, HandLandmarker } from \"@mediapipe/tasks-vision\";import hand_landmarker_task from \"../models/hand_landmarker.task\";\n```\n\nFilesetResolver \u2014 To find and use a specific set of files needed in project.  \nHandLandmarker \u2014 It\u2019s used for recognizing and understanding hand movements in images or videos.  \nhand\\_landmarker\\_task is just name of model. [...] import React, { useEffect, useRef, useState } from \"react\";import { FilesetResolver, HandLandmarker } from \"@mediapipe/tasks-vision\";import hand_landmarker_task from \"../models/hand_landmarker.task\";const Demo = () => { const videoRef = useRef(null); const canvasRef = useRef(null); const [handPresence, setHandPresence] = useState(null); useEffect(() => { let handLandmarker; let animationFrameId; const initializeHandDetection = async () => { try { const vision = await FilesetResolver.forVisionTasks( \" ); handLandmarker = await HandLandmarker.createFromOptions( vision, { baseOptions: { modelAssetPath: hand_landmarker_task }, numHands: 2, runningMode: \"video\" } ); detectHands(); } catch (error) { console.error(\"Error initializing hand detection:\", error); } }; const drawLandmarks = [...] Step 2: Initialize hand detection.\n\n```\nconst initializeHandDetection = async () => { try { const vision = await FilesetResolver.forVisionTasks( \" ); handLandmarker = await HandLandmarker.createFromOptions( vision, { baseOptions: { modelAssetPath: hand_landmarker_task }, numHands: 2, runningMode: \"video\" } ); detectHands(); } catch (error) { console.error(\"Error initializing hand detection:\", error); } };\n```\n\nThe function first uses `FilesetResolver.forVisionTasks` to load necessary files from a URL. Next, the `HandLandmarker.createFromOptions` is called to create a hand landmarker. to work in a \u2018video\u2019 mode (`runningMode: \"video\"`). For Image use `runningMode: \"image\" .`Once everything is set up, the function calls `detectHands()` to start the actual hand detection process.",
        "score": 0.6643884,
        "raw_content": null
      },
      {
        "url": "https://ai.google.dev/edge/api/mediapipe/js/tasks-vision",
        "title": "tasks-vision package | Google AI Edge",
        "content": "| Class | Description |\n --- |\n| DrawingUtils | Helper class to visualize the result of a MediaPipe Vision task. |\n| FaceDetector | Performs face detection on images. |\n| FaceLandmarker | Performs face landmarks detection on images.This API expects a pre-trained face landmarker model asset bundle. |\n| FaceStylizer | Performs face stylization on images. |\n| FilesetResolver | Resolves the files required for the MediaPipe Task APIs.This class verifies whether SIMD is supported in the current environment and loads the SIMD files only if support is detected. The returned filesets require that the Wasm files are published without renaming. If this is not possible, you can invoke the MediaPipe Tasks APIs using a manually created `WasmFileset`. | [...] | GestureRecognizer | Performs hand gesture recognition on images. |\n| HandLandmarker | Performs hand landmarks detection on images. |\n| HolisticLandmarker | Performs holistic landmarks detection on images. |\n| ImageClassifier | Performs classification on images. |\n| ImageEmbedder | Performs embedding extraction on images. |\n| ImageSegmenter | Performs image segmentation on images. |\n| ImageSegmenterResult | The output result of ImageSegmenter. | [...] `WasmFileset`\n`RegionOfInterest`\n`[batch x height x width x channels]`\n`batch`\n`channels`\n`output_type`\n`output_type`\n`channels`\n`ImageData`\n`ImageBitmap`\n`WebGLTexture`\n`getAs...()`\n`has...()`\n`clone()`\n`close()`\n`MPImage`\n`OffscreenCanvas`\n`Uint8Array`\n`Float32Array`\n`WebGLTexture`\n`getAs...()`\n`has...()`\n`clone()`\n`close()`\n`MPMask`\n`PoseLandmarker`\n\n## Interfaces",
        "score": 0.62303346,
        "raw_content": null
      },
      {
        "url": "https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer/web_js",
        "title": "Gesture recognition guide for Web | Google AI Edge",
        "content": "### JavaScript packages\n\nGesture Recognizer code is available through the MediaPipe `@mediapipe/tasks-vision`\nNPM package. You can\nfind and download these libraries by following the instructions in the platform\nSetup guide.\n\n`@mediapipe/tasks-vision`\n\nYou can install the required packages through NPM\nusing the following command:\n\n`npm install @mediapipe/tasks-vision`\n\nIf you want to import the task code via a content delivery network (CDN)\nservice, add the following code in the `<head>` tag in your HTML file:\n\n`<head>`\n`<!-- You can replace JSDeliver with another CDN if you prefer to -->\n<head>\n<script src=\"\ncrossorigin=\"anonymous\"></script>\n</head>`\n\n### Model [...] The code example below demonstrates using the `createFromOptions()` function to\nset up the task. The `createFromOptions` function allows you to customize the\nGesture Recognizer with configuration options. For more information on configuration\noptions, see Configuration options.\n\n`createFromOptions()`\n`createFromOptions`\n\nThe following code demonstrates how to build and configure the task with custom\noptions:\n\n`// Create task for image file processing:\nconst vision = await FilesetResolver.forVisionTasks(\n // path/to/wasm/root\n \" \"\n);\nconst gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {\n baseOptions: {\n modelAssetPath: \"\n },\n numHands: 2\n});`\n\n### Configuration options\n\nThis task has the following configuration options for Web applications: [...] Google AI for Developers\nGoogle AI for Developers\n\n# Gesture recognition guide for Web\n\nThe MediaPipe Gesture Recognizer task lets you recognize hand gestures in real time, and\nprovides the recognized hand gesture results and the hand landmarks of the\ndetected hands. These instructions show you how to use the Gesture Recognizer\nfor web and JavaScript apps.\n\nYou can see this task in action by viewing the\ndemo.\nFor more information about the capabilities, models, and configuration options\nof this task, see the Overview.\n\n## Code example",
        "score": 0.6152965,
        "raw_content": null
      }
    ],
    "request_id": "bbcd2aa3-ad89-47b1-8d9e-88d62676942d"
  },
  "brave": {
    "query": {
      "original": "@mediapipe/tasks-vision vision_bundle.js script global name for FilesetResolver and GestureRecognizer",
      "show_strict_warning": false,
      "is_navigational": false,
      "is_news_breaking": false,
      "spellcheck_off": true,
      "country": "us",
      "bad_results": false,
      "should_fallback": false,
      "postal_code": "",
      "city": "",
      "header_country": "",
      "more_results_available": true,
      "state": ""
    },
    "mixed": {
      "type": "mixed",
      "main": [
        {
          "type": "web",
          "index": 0,
          "all": false
        },
        {
          "type": "web",
          "index": 1,
          "all": false
        },
        {
          "type": "web",
          "index": 2,
          "all": false
        },
        {
          "type": "web",
          "index": 3,
          "all": false
        },
        {
          "type": "web",
          "index": 4,
          "all": false
        },
        {
          "type": "web",
          "index": 5,
          "all": false
        },
        {
          "type": "web",
          "index": 6,
          "all": false
        },
        {
          "type": "web",
          "index": 7,
          "all": false
        },
        {
          "type": "web",
          "index": 8,
          "all": false
        },
        {
          "type": "web",
          "index": 9,
          "all": false
        },
        {
          "type": "web",
          "index": 10,
          "all": false
        },
        {
          "type": "web",
          "index": 11,
          "all": false
        },
        {
          "type": "web",
          "index": 12,
          "all": false
        },
        {
          "type": "web",
          "index": 13,
          "all": false
        },
        {
          "type": "web",
          "index": 14,
          "all": false
        },
        {
          "type": "web",
          "index": 15,
          "all": false
        }
      ],
      "top": [],
      "side": []
    },
    "type": "search",
    "web": {
      "type": "search",
      "results": [
        {
          "title": "@mediapipe/tasks-vision - npm",
          "url": "https://www.npmjs.com/package/@mediapipe/tasks-vision",
          "is_source_local": false,
          "is_source_both": false,
          "description": "The MediaPipe Holistic Landmarker task task lets you combine components of the pose, face, and hand landmarkers to create a complete landmarker for the human body. const vision = await FilesetResolver.forVisionTasks( &quot;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm&quot; ); const holisticLandmarker = await HolisticLandmarker.createFromModelPath(vision, &quot;https://storage.googleapis.com/mediapipe-models/holistic_landmarker/holistic_landmarker/float16/1/hand_landmark.task&quot; ); const image = document.getElementById(&quot;image&quot;) as HTMLImageElement; const landmarks = holisticLandmarker.detect(image);",
          "profile": {
            "name": "npm",
            "url": "https://www.npmjs.com/package/@mediapipe/tasks-vision",
            "long_name": "npmjs.com",
            "img": "https://imgs.search.brave.com/129PsExJCizx2_pC-7e4exnT1C0gWE4PUinIWjrXr-4/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMzg1ZTZiOGFm/M2NhY2JjMmE1NmJl/ZTRlODIwNDVhZWIy/OWRjZDgzYjYyYjcw/NjhmNzQzMWM0NDBk/Y2U3MGIzMi93d3cu/bnBtanMuY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "software",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "npmjs.com",
            "hostname": "www.npmjs.com",
            "favicon": "https://imgs.search.brave.com/129PsExJCizx2_pC-7e4exnT1C0gWE4PUinIWjrXr-4/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMzg1ZTZiOGFm/M2NhY2JjMmE1NmJl/ZTRlODIwNDVhZWIy/OWRjZDgzYjYyYjcw/NjhmNzQzMWM0NDBk/Y2U3MGIzMi93d3cu/bnBtanMuY29tLw",
            "path": "\u203a package  \u203a @mediapipe  \u203a tasks-vision"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/t1TrvkMaJ8NM6BTzVXuMm-wqMPNvTDiEW-dL8v7DsSc/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdGF0/aWMtcHJvZHVjdGlv/bi5ucG1qcy5jb20v/MzM4ZTQ5MDVhMjY4/NGNhOTZlMDhjNzc4/MGZjNjg0MTIucG5n",
            "original": "https://static-production.npmjs.com/338e4905a2684ca96e08c7780fc68412.png",
            "logo": false
          }
        },
        {
          "title": "reactjs - Is there any way of using node modules packages subfolders? - Stack Overflow",
          "url": "https://stackoverflow.com/questions/78872825/is-there-any-way-of-using-node-modules-packages-subfolders",
          "is_source_local": false,
          "is_source_both": false,
          "description": "I am using @mediapipe/tasks-vision npm package in the webapp, I want to use my own wasm folder that&#x27;s present in &#x27;node_modules/@mediapipe/tasks-vision/wasm&#x27;. I am using next.js as the front end. Any help or sharing is most appreciated. ... import { FaceLandmarker, FilesetResolver, DrawingUtils} from &quot;@mediapipe/tasks-vision&quot;; Initialize face landmarker useEffect(() =&gt; { const createFaceLandmarker = async () =&gt; { const filesetResolver = await FilesetResolver.forVisionTasks(&quot;https://cdn.jsdelivr.net/npm/@mediapipe/[email protected]/wasm&quot;); faceLandmarker = await FaceLandmarker.createFromOptions(",
          "profile": {
            "name": "Stack Overflow",
            "url": "https://stackoverflow.com/questions/78872825/is-there-any-way-of-using-node-modules-packages-subfolders",
            "long_name": "stackoverflow.com",
            "img": "https://imgs.search.brave.com/4WRMec_wn8Q9LO6DI43kkBvIL6wD5TYCXztC9C9kEI0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNWU3Zjg0ZjA1/YjQ3ZTlkNjQ1ODA1/MjAwODhiNjhjYWU0/OTc4MjM4ZDJlMTBi/ODExYmNiNTkzMjdh/YjM3MGExMS9zdGFj/a292ZXJmbG93LmNv/bS8"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "qa",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "stackoverflow.com",
            "hostname": "stackoverflow.com",
            "favicon": "https://imgs.search.brave.com/4WRMec_wn8Q9LO6DI43kkBvIL6wD5TYCXztC9C9kEI0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNWU3Zjg0ZjA1/YjQ3ZTlkNjQ1ODA1/MjAwODhiNjhjYWU0/OTc4MjM4ZDJlMTBi/ODExYmNiNTkzMjdh/YjM3MGExMS9zdGFj/a292ZXJmbG93LmNv/bS8",
            "path": "\u203a questions  \u203a 78872825  \u203a is-there-any-way-of-using-node-modules-packages-subfolders"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/7xYts7jy_f89pn-IFVr8VxO1jMDhot5Ln0Buo0It8EM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdGFj/a292ZXJmbG93LmNv/bS9Db250ZW50L1Np/dGVzL3N0YWNrb3Zl/cmZsb3cvSW1nL2Fw/cGxlLXRvdWNoLWlj/b25AMi5wbmc_dj03/M2Q3OWE4OWJkZWQ",
            "original": "https://stackoverflow.com/Content/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded",
            "logo": true
          }
        },
        {
          "title": "mediapipe/tasks-vision",
          "url": "https://app.unpkg.com/@mediapipe/tasks-vision@0.10.15/files/vision.d.ts",
          "is_source_local": false,
          "is_source_both": false,
          "description": "* @return A `WasmFileset` that can be used to initialize MediaPipe Vision * tasks. */ static forVisionTasks(basePath?: string): Promise&lt;WasmFileset&gt;; } /** Performs hand gesture recognition on images. */ export declare class GestureRecognizer extends VisionTaskRunner { /** * An array containing the pairs of hand landmark indices to be rendered with * connections.",
          "profile": {
            "name": "UNPKG",
            "url": "https://app.unpkg.com/@mediapipe/tasks-vision@0.10.15/files/vision.d.ts",
            "long_name": "app.unpkg.com",
            "img": "https://imgs.search.brave.com/eF6NI3VuAHeNDOcACFA06uafkiGWlVUnrPLHIwc_LGw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMDA1N2U3OGY2/MDIxYzhlY2FjZWUy/ZTVhYmNkMTEzNWRl/M2UyZTQyYjIyNTQ1/MDE0OTc2ZWJlMjky/Y2QwNTcyYi9hcHAu/dW5wa2cuY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "app.unpkg.com",
            "hostname": "app.unpkg.com",
            "favicon": "https://imgs.search.brave.com/eF6NI3VuAHeNDOcACFA06uafkiGWlVUnrPLHIwc_LGw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvMDA1N2U3OGY2/MDIxYzhlY2FjZWUy/ZTVhYmNkMTEzNWRl/M2UyZTQyYjIyNTQ1/MDE0OTc2ZWJlMjky/Y2QwNTcyYi9hcHAu/dW5wa2cuY29tLw",
            "path": "\u203a @mediapipe  \u203a tasks-vision@0.10.15  \u203a files  \u203a vision.d.ts"
          }
        },
        {
          "title": "Using tasks-vision in node.js/server-side? \u00b7 Issue #5237 \u00b7 ...",
          "url": "https://github.com/google/mediapipe/issues/5237",
          "is_source_local": false,
          "is_source_both": false,
          "description": "Have I written custom code (as opposed to using a stock example script provided in MediaPipe) Yes OS Platform and Distribution Mac OS MediaPipe Tasks SDK version ^0.10.12 Task name (e.g. Image classification, Gesture recognition etc.) La...",
          "page_age": "2024-03-18T14:27:05",
          "profile": {
            "name": "GitHub",
            "url": "https://github.com/google/mediapipe/issues/5237",
            "long_name": "github.com",
            "img": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "software",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "github.com",
            "hostname": "github.com",
            "favicon": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw",
            "path": "\u203a google  \u203a mediapipe  \u203a issues  \u203a 5237"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/uZ6b4HG7Wg87bgU61CFz8JWiKbbsRkbt3HYXFzI3Ypc/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9vcGVu/Z3JhcGguZ2l0aHVi/YXNzZXRzLmNvbS8y/OGQxMzI4MzgyYzI2/YTgwN2FhZThiNjYy/MzQ5NTE2ZWEwY2Y2/YmQ4MTNhODgzOWU0/Zjk5YjI1MTljYjc0/MjRhL2dvb2dsZS1h/aS1lZGdlL21lZGlh/cGlwZS9pc3N1ZXMv/NTIzNw",
            "original": "https://opengraph.githubassets.com/28d1328382c26a807aae8b662349516ea0cf6bd813a8839e4f99b2519cb7424a/google-ai-edge/mediapipe/issues/5237",
            "logo": false
          },
          "age": "March 18, 2024"
        },
        {
          "title": "Google AI Edge API reference | Google AI for Developers",
          "url": "https://ai.google.dev/edge/api/mediapipe/js/tasks-text.filesetresolver",
          "is_source_local": false,
          "is_source_both": false,
          "description": "This API reference documentation provides detailed information for MediaPipe Solutions and LiteRT \u00b7 The following documentation provides detailed information for each of the classes and methods in the MediaPipe Tasks libraries and MediaPipe Model Maker",
          "profile": {
            "name": "Google AI",
            "url": "https://ai.google.dev/edge/api/mediapipe/js/tasks-text.filesetresolver",
            "long_name": "ai.google.dev",
            "img": "https://imgs.search.brave.com/m_vCaOleBRRqR4DoxKjK3UfaUq7NjGhFsQ6MmrZnZps/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYjBlMTk4YmQw/OWZhZDMyNDk0ZjI3/NzM5Njg2NTNmMGY0/ZTAzNTkxZTU3YjIz/YzgxNjM4YWMzZjk5/MWI2NTliZS9haS5n/b29nbGUuZGV2Lw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "ai.google.dev",
            "hostname": "ai.google.dev",
            "favicon": "https://imgs.search.brave.com/m_vCaOleBRRqR4DoxKjK3UfaUq7NjGhFsQ6MmrZnZps/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYjBlMTk4YmQw/OWZhZDMyNDk0ZjI3/NzM5Njg2NTNmMGY0/ZTAzNTkxZTU3YjIz/YzgxNjM4YWMzZjk5/MWI2NTliZS9haS5n/b29nbGUuZGV2Lw",
            "path": "\u203a edge  \u203a api  \u203a mediapipe  \u203a js  \u203a tasks-text.filesetresolver"
          }
        },
        {
          "title": "Facelandmarker failed to load inside a module web worker \u00b7 Issue #5527 \u00b7 google-ai-edge/mediapipe",
          "url": "https://github.com/google-ai-edge/mediapipe/issues/5527",
          "is_source_local": false,
          "is_source_both": false,
          "description": "From the main thread: const myWorker = new Worker(&quot;worker.js&quot;, { type: &quot;module&quot; }); -------------- worker.js: import vision from &quot;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js&quot;; const { FaceLandmarker, FilesetResolver, DrawingUtils } = vision; async function createFaceLandmarker() { const filesetResolver = await FilesetResolver.forVisionTasks( &quot;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/wasm&quot; ); faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, { baseOptions: { modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face",
          "page_age": "2024-07-10T15:15:09",
          "profile": {
            "name": "GitHub",
            "url": "https://github.com/google-ai-edge/mediapipe/issues/5527",
            "long_name": "github.com",
            "img": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "software",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "github.com",
            "hostname": "github.com",
            "favicon": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw",
            "path": "\u203a google-ai-edge  \u203a mediapipe  \u203a issues  \u203a 5527"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/R-Ln228bA16nNiWP7PamimfWEZJd_EGT5tC87WicOG4/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9vcGVu/Z3JhcGguZ2l0aHVi/YXNzZXRzLmNvbS9i/NmIxZDI5N2JjODQz/MWQyNjdmNjgwODUx/MThmYjczZjYzZjJj/MzYwZWVkMTMxYmM0/ZWM0MzFmMTQyNjM3/Njc3L2dvb2dsZS1h/aS1lZGdlL21lZGlh/cGlwZS9pc3N1ZXMv/NTUyNw",
            "original": "https://opengraph.githubassets.com/b6b1d297bc8431d267f68085118fb73f63f2c360eed131bc4ec431f142637677/google-ai-edge/mediapipe/issues/5527",
            "logo": false
          },
          "age": "July 10, 2024"
        },
        {
          "title": "7 dos and don'ts of using ML on the web with MediaPipe - Google Developers Blog",
          "url": "https://developers.googleblog.com/7-dos-and-donts-of-using-ml-on-the-web-with-mediapipe/",
          "is_source_local": false,
          "is_source_both": false,
          "description": "const createGestureRecognizer = async () =&gt; { const vision = await FilesetResolver.forVisionTasks( &quot;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm&quot; ); gestureRecognizer = await GestureRecognizer.createFromOptions(vision, { baseOptions: { modelAssetPath: &quot;https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task&quot;, delegate: &quot;GPU&quot; }, }); }; createGestureRecognizer();",
          "page_age": "2023-10-05T00:00:00",
          "profile": {
            "name": "Google Developers",
            "url": "https://developers.googleblog.com/7-dos-and-donts-of-using-ml-on-the-web-with-mediapipe/",
            "long_name": "developers.googleblog.com",
            "img": "https://imgs.search.brave.com/1wHM_xROwxW4Kv3Hca3FesKaZCjw4YxFVIuplgioAOw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWNlOWM3MDcx/ZGE0YjhhMWZmMGNi/NDZmNmFjNTQ5NTFl/OGNmZmZiY2M4ZjFl/MTJkOGZjNTI5ZTY1/YjBhMDQ2Yi9kZXZl/bG9wZXJzLmdvb2ds/ZWJsb2cuY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "developers.googleblog.com",
            "hostname": "developers.googleblog.com",
            "favicon": "https://imgs.search.brave.com/1wHM_xROwxW4Kv3Hca3FesKaZCjw4YxFVIuplgioAOw/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWNlOWM3MDcx/ZGE0YjhhMWZmMGNi/NDZmNmFjNTQ5NTFl/OGNmZmZiY2M4ZjFl/MTJkOGZjNTI5ZTY1/YjBhMDQ2Yi9kZXZl/bG9wZXJzLmdvb2ds/ZWJsb2cuY29tLw",
            "path": "  \u203a google for developers blog  \u203a 7 dos and don'ts of using ml on the web with mediapipe"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/MB3XxrCXLUWo-yTL9As6FWXvRLZHwovQsxMUhrd3HSU/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdG9y/YWdlLmdvb2dsZWFw/aXMuY29tL2d3ZWIt/ZGV2ZWxvcGVyLWdv/b2ctYmxvZy1hc3Nl/dHMvaW1hZ2VzLzEy/MS4yZTE2ZDBiYS5m/aWxsLTgwMHg0MDAu/cG5n",
            "original": "https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/121.2e16d0ba.fill-800x400.png",
            "logo": false
          },
          "age": "October 5, 2023"
        },
        {
          "title": "How to use a Web worker with Mediapipe Vision",
          "url": "https://stackoverflow.com/questions/76222648/how-to-use-a-web-worker-with-mediapipe-vision",
          "is_source_local": false,
          "is_source_both": false,
          "description": "vision_bundle.js:1 Uncaught TypeError: Failed to execute &#x27;importScripts&#x27; on &#x27;WorkerGlobalScope&#x27;: Module scripts don&#x27;t support importScripts(). at o (vision_bundle.js:1:466026) at Array.map (&lt;anonymous&gt;) at r (vision_bundle.js:1:466152) at createTaskRunner (vision_bundle.js:1:467669) at TaskRunner.createInstance (vision_bundle.js:1:467909) at VisionTaskRunner.createVisionInstance (vision_bundle.js:1:472322) at b.createFromOptions (vision_bundle.js:1:553534) at index.js:9:45 o \u00b7 This one seems to be a problem with workers not fully supporting modules, however I can&#x27;t think of how I&#x27;d get around that when I have to import the HandLandmarker and FilesetResolver from the vision package",
          "profile": {
            "name": "Stack Overflow",
            "url": "https://stackoverflow.com/questions/76222648/how-to-use-a-web-worker-with-mediapipe-vision",
            "long_name": "stackoverflow.com",
            "img": "https://imgs.search.brave.com/4WRMec_wn8Q9LO6DI43kkBvIL6wD5TYCXztC9C9kEI0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNWU3Zjg0ZjA1/YjQ3ZTlkNjQ1ODA1/MjAwODhiNjhjYWU0/OTc4MjM4ZDJlMTBi/ODExYmNiNTkzMjdh/YjM3MGExMS9zdGFj/a292ZXJmbG93LmNv/bS8"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "stackoverflow.com",
            "hostname": "stackoverflow.com",
            "favicon": "https://imgs.search.brave.com/4WRMec_wn8Q9LO6DI43kkBvIL6wD5TYCXztC9C9kEI0/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvNWU3Zjg0ZjA1/YjQ3ZTlkNjQ1ODA1/MjAwODhiNjhjYWU0/OTc4MjM4ZDJlMTBi/ODExYmNiNTkzMjdh/YjM3MGExMS9zdGFj/a292ZXJmbG93LmNv/bS8",
            "path": "\u203a questions  \u203a 76222648  \u203a how-to-use-a-web-worker-with-mediapipe-vision"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/7xYts7jy_f89pn-IFVr8VxO1jMDhot5Ln0Buo0It8EM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdGFj/a292ZXJmbG93LmNv/bS9Db250ZW50L1Np/dGVzL3N0YWNrb3Zl/cmZsb3cvSW1nL2Fw/cGxlLXRvdWNoLWlj/b25AMi5wbmc_dj03/M2Q3OWE4OWJkZWQ",
            "original": "https://stackoverflow.com/Content/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded",
            "logo": true
          }
        },
        {
          "title": "Real-Time Face Tracking in the Browser with MediaPipe | by Chris McKenzie | Medium",
          "url": "https://medium.com/@kenzic/real-time-face-tracking-in-the-browser-with-mediapipe-7c818c96b4ca",
          "is_source_local": false,
          "is_source_both": false,
          "description": "async function loadFaceLandmarker() { const filesetResolver = await FilesetResolver.forVisionTasks( &quot;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm&quot; ); faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, { baseOptions: { modelAssetPath: &quot;https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task&quot;, delegate: &quot;GPU&quot;, }, outputFaceBlendshapes: true, runningMode: &quot;LIVE_STREAM&quot;, numFaces: 2, }); }",
          "page_age": "2025-07-21T15:57:46",
          "profile": {
            "name": "Medium",
            "url": "https://medium.com/@kenzic/real-time-face-tracking-in-the-browser-with-mediapipe-7c818c96b4ca",
            "long_name": "medium.com",
            "img": "https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "medium.com",
            "hostname": "medium.com",
            "favicon": "https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw",
            "path": "\u203a @kenzic  \u203a real-time-face-tracking-in-the-browser-with-mediapipe-7c818c96b4ca"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/orZ1bZIH-tAbcnv6ypt4kS7b1cMApEDJMjRInsOcL2g/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9taXJv/Lm1lZGl1bS5jb20v/MSpnMnZoMXUxellV/WHJUMXFHZTJwN3VR/LnBuZw",
            "original": "https://miro.medium.com/1*g2vh1u1zYUXrT1qGe2p7uQ.png",
            "logo": false
          },
          "age": "July 21, 2025"
        },
        {
          "title": "GitHub - whatisor/mediapipe-tasks-vision: Prebuild mediapipe tasks vision from https://github.com/whatisor/mediapipe",
          "url": "https://github.com/whatisor/mediapipe-tasks-vision",
          "is_source_local": false,
          "is_source_both": false,
          "description": "const vision = await FilesetResolver.forVisionTasks( &quot;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/wasm&quot; ); const gestureRecognizer = await GestureRecognizer.createFromModelPath(vision, &quot;https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task&quot; ); const image = document.getElementById(&quot;image&quot;) as HTMLImageElement; const recognitions = gestureRecognizer.recognize(image);",
          "profile": {
            "name": "GitHub",
            "url": "https://github.com/whatisor/mediapipe-tasks-vision",
            "long_name": "github.com",
            "img": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "software",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "github.com",
            "hostname": "github.com",
            "favicon": "https://imgs.search.brave.com/xxsA4YxzaR0cl-DBsH9-lpv2gsif3KMYgM87p26bs_o/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYWQyNWM1NjA5/ZjZmZjNlYzI2MDNk/N2VkNmJhYjE2MzZl/MDY5ZTMxMDUzZmY1/NmU3NWIzNWVmMjk0/NTBjMjJjZi9naXRo/dWIuY29tLw",
            "path": "\u203a whatisor  \u203a mediapipe-tasks-vision"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/7Fd8cZr8zbGW5n4M1e0U8SuEGlP2isDna0hgZYx7xtI/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9vcGVu/Z3JhcGguZ2l0aHVi/YXNzZXRzLmNvbS82/ZjkyNGM1MjQ0ZDFj/MjM3Zjk2YzNjNGVm/YzNhODE2ZjI2MmM5/ZjU0MjY3M2I0ZTRl/MGI5NTNiOTIxNjUy/YzkyL3doYXRpc29y/L21lZGlhcGlwZS10/YXNrcy12aXNpb24",
            "original": "https://opengraph.githubassets.com/6f924c5244d1c237f96c3c4efc3a816f262c9f542673b4e4e0b953b921652c92/whatisor/mediapipe-tasks-vision",
            "logo": false
          }
        },
        {
          "title": "MediaPipe Tasks | Google AI Edge | Google AI for Developers",
          "url": "https://ai.google.dev/edge/mediapipe/solutions/tasks",
          "is_source_local": false,
          "is_source_both": false,
          "description": "The MediaPipe Tasks Web JavaScript API is divided into packages that perform ML tasks in major domains, including vision, natural language, and audio.",
          "profile": {
            "name": "Google AI",
            "url": "https://ai.google.dev/edge/mediapipe/solutions/tasks",
            "long_name": "ai.google.dev",
            "img": "https://imgs.search.brave.com/m_vCaOleBRRqR4DoxKjK3UfaUq7NjGhFsQ6MmrZnZps/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYjBlMTk4YmQw/OWZhZDMyNDk0ZjI3/NzM5Njg2NTNmMGY0/ZTAzNTkxZTU3YjIz/YzgxNjM4YWMzZjk5/MWI2NTliZS9haS5n/b29nbGUuZGV2Lw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "ai.google.dev",
            "hostname": "ai.google.dev",
            "favicon": "https://imgs.search.brave.com/m_vCaOleBRRqR4DoxKjK3UfaUq7NjGhFsQ6MmrZnZps/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYjBlMTk4YmQw/OWZhZDMyNDk0ZjI3/NzM5Njg2NTNmMGY0/ZTAzNTkxZTU3YjIz/YzgxNjM4YWMzZjk5/MWI2NTliZS9haS5n/b29nbGUuZGV2Lw",
            "path": "  \u203a google ai edge  \u203a mediapipe tasks"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/zhrzsc_qNH91JsJAdP7mQPSrMHVuaQhxr8LWjxB4_7Y/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9haS5n/b29nbGUuZGV2L3N0/YXRpYy9zaXRlLWFz/c2V0cy9pbWFnZXMv/c2hhcmUucG5n",
            "original": "https://ai.google.dev/static/site-assets/images/share.png",
            "logo": false
          }
        },
        {
          "title": "HandIntegrating @mediapipe/tasks-vision for Hand Landmark Detection in React | by Kathan Chaudhari | Medium",
          "url": "https://medium.com/@kiyo07/integrating-mediapipe-tasks-vision-for-hand-landmark-detection-in-react-a2cfb9d543c7",
          "is_source_local": false,
          "is_source_both": false,
          "description": "import React, { useEffect, useRef, useState } from &quot;react&quot;; import { FilesetResolver, HandLandmarker } from &quot;@mediapipe/tasks-vision&quot;; import hand_landmarker_task from &quot;../models/hand_landmarker.task&quot;; const Demo = () =&gt; { const videoRef = useRef(null); const canvasRef = useRef(null); const [handPresence, setHandPresence] = useState(null); useEffect(() =&gt; { let handLandmarker; let animationFrameId; const initializeHandDetection = async () =&gt; { try { const vision = await FilesetResolver.forVisionTasks( &quot;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm&quot;, ); handLandmarker = await",
          "page_age": "2023-12-21T06:53:32",
          "profile": {
            "name": "Medium",
            "url": "https://medium.com/@kiyo07/integrating-mediapipe-tasks-vision-for-hand-landmark-detection-in-react-a2cfb9d543c7",
            "long_name": "medium.com",
            "img": "https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "medium.com",
            "hostname": "medium.com",
            "favicon": "https://imgs.search.brave.com/4R4hFITz_F_be0roUiWbTZKhsywr3fnLTMTkFL5HFow/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvOTZhYmQ1N2Q4/NDg4ZDcyODIyMDZi/MzFmOWNhNjE3Y2E4/Y2YzMThjNjljNDIx/ZjllZmNhYTcwODhl/YTcwNDEzYy9tZWRp/dW0uY29tLw",
            "path": "\u203a @kiyo07  \u203a integrating-mediapipe-tasks-vision-for-hand-landmark-detection-in-react-a2cfb9d543c7"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/jWpn8vtmZ9-jNZ0UoEkFg9sALFPnQ_rzZeyHdEHSMlI/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9taXJv/Lm1lZGl1bS5jb20v/MSpWbDBEd2VtWlRm/SXpXbUFuQTVvbk5n/LnBuZw",
            "original": "https://miro.medium.com/1*Vl0DwemZTfIzWmAnA5onNg.png",
            "logo": false
          },
          "age": "December 21, 2023"
        },
        {
          "title": "tasks-vision package | Google AI Edge | Google AI for Developers",
          "url": "https://ai.google.dev/edge/api/mediapipe/js/tasks-vision",
          "is_source_local": false,
          "is_source_both": false,
          "description": "A color map with 22 classes. Used in our demos \u00b7 A user-defined callback to take input data and map it to a custom output value",
          "profile": {
            "name": "Google AI",
            "url": "https://ai.google.dev/edge/api/mediapipe/js/tasks-vision",
            "long_name": "ai.google.dev",
            "img": "https://imgs.search.brave.com/m_vCaOleBRRqR4DoxKjK3UfaUq7NjGhFsQ6MmrZnZps/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYjBlMTk4YmQw/OWZhZDMyNDk0ZjI3/NzM5Njg2NTNmMGY0/ZTAzNTkxZTU3YjIz/YzgxNjM4YWMzZjk5/MWI2NTliZS9haS5n/b29nbGUuZGV2Lw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "generic",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "ai.google.dev",
            "hostname": "ai.google.dev",
            "favicon": "https://imgs.search.brave.com/m_vCaOleBRRqR4DoxKjK3UfaUq7NjGhFsQ6MmrZnZps/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYjBlMTk4YmQw/OWZhZDMyNDk0ZjI3/NzM5Njg2NTNmMGY0/ZTAzNTkxZTU3YjIz/YzgxNjM4YWMzZjk5/MWI2NTliZS9haS5n/b29nbGUuZGV2Lw",
            "path": "  \u203a google ai edge  \u203a tasks-vision package"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/zhrzsc_qNH91JsJAdP7mQPSrMHVuaQhxr8LWjxB4_7Y/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9haS5n/b29nbGUuZGV2L3N0/YXRpYy9zaXRlLWFz/c2V0cy9pbWFnZXMv/c2hhcmUucG5n",
            "original": "https://ai.google.dev/static/site-assets/images/share.png",
            "logo": false
          }
        },
        {
          "title": "MediaPipe Tasks | Google for Developers",
          "url": "https://developers.google.com/mediapipe/solutions/tasks",
          "is_source_local": false,
          "is_source_both": false,
          "description": "The MediaPipe Tasks Web JavaScript API is divided into packages that perform ML tasks in major domains, including vision, natural language, and audio.",
          "profile": {
            "name": "Google",
            "url": "https://developers.google.com/mediapipe/solutions/tasks",
            "long_name": "developers.google.com",
            "img": "https://imgs.search.brave.com/xd2if5kWPozWi5tGSdOQNnrmMOZFDny4yeh0h6PJu8U/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODcyMjg1ZjQ0/ZWI1OTgzNjQxNTM4/NDQ4MDljYjE2YTZi/MTc4OWFjOGM3NDEx/ZDBmZWJjYjg2YTBj/MGI3Zjk3OC9kZXZl/bG9wZXJzLmdvb2ds/ZS5jb20v"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "developers.google.com",
            "hostname": "developers.google.com",
            "favicon": "https://imgs.search.brave.com/xd2if5kWPozWi5tGSdOQNnrmMOZFDny4yeh0h6PJu8U/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODcyMjg1ZjQ0/ZWI1OTgzNjQxNTM4/NDQ4MDljYjE2YTZi/MTc4OWFjOGM3NDEx/ZDBmZWJjYjg2YTBj/MGI3Zjk3OC9kZXZl/bG9wZXJzLmdvb2ds/ZS5jb20v",
            "path": "  \u203a mediapipe  \u203a mediapipe tasks"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/1ynaEXHTvCrrBbFfD2vhOLKfXdZpdisq41k3koFjNo0/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/Z3N0YXRpYy5jb20v/ZGV2cmVsLWRldnNp/dGUvcHJvZC92NWNl/YWUzNzZiNjkyYzZh/MTZhNTFkNWJhMjll/MDU0Y2E2NzY2MzVl/NjViZjc3ZGE2OWRl/YWM0MjYzODAwY2Zj/Ny9kZXZlbG9wZXJz/L2ltYWdlcy9vcGVu/Z3JhcGgvcGFsZS1i/bHVlLnBuZw",
            "original": "https://www.gstatic.com/devrel-devsite/prod/v5ceae376b692c6a16a51d5ba29e054ca676635e65bf77da69deac4263800cfc7/developers/images/opengraph/pale-blue.png",
            "logo": false
          }
        },
        {
          "title": "Gesture recognition guide for Web | MediaPipe | Google for Developers",
          "url": "https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer/web_js",
          "is_source_local": false,
          "is_source_both": false,
          "description": "// Create task for image file processing: const vision = await FilesetResolver.forVisionTasks( // path/to/wasm/root &quot;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm &quot; ); const gestureRecognizer = await GestureRecognizer.createFromOptions(vision, { baseOptions: { modelAssetPath: &quot;https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/gesture_recognizer.task&quot; }, numHands: 2 }); This task has the following configuration options for Web applications: Gesture Recognizer can recognize gestures in images in any format supported by the host browser.",
          "profile": {
            "name": "Google",
            "url": "https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer/web_js",
            "long_name": "developers.google.com",
            "img": "https://imgs.search.brave.com/xd2if5kWPozWi5tGSdOQNnrmMOZFDny4yeh0h6PJu8U/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODcyMjg1ZjQ0/ZWI1OTgzNjQxNTM4/NDQ4MDljYjE2YTZi/MTc4OWFjOGM3NDEx/ZDBmZWJjYjg2YTBj/MGI3Zjk3OC9kZXZl/bG9wZXJzLmdvb2ds/ZS5jb20v"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "developers.google.com",
            "hostname": "developers.google.com",
            "favicon": "https://imgs.search.brave.com/xd2if5kWPozWi5tGSdOQNnrmMOZFDny4yeh0h6PJu8U/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvODcyMjg1ZjQ0/ZWI1OTgzNjQxNTM4/NDQ4MDljYjE2YTZi/MTc4OWFjOGM3NDEx/ZDBmZWJjYjg2YTBj/MGI3Zjk3OC9kZXZl/bG9wZXJzLmdvb2ds/ZS5jb20v",
            "path": "  \u203a mediapipe  \u203a gesture recognition guide for web"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/U2kk1n_jMTSYqWgDHTCdr7uiHQBSzJXOwxKF-QEo5k4/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/Z3N0YXRpYy5jb20v/ZGV2cmVsLWRldnNp/dGUvcHJvZC92ZTVl/ZjlhYzdiNDk3ZTE5/ZWNlOTQyN2ZhY2M3/OGQwYzU5YWFhYjdh/MmJjNmEwZjc1ZmRh/ZTkzZjRlZTU4OWY2/Ny9kZXZlbG9wZXJz/L2ltYWdlcy9vcGVu/Z3JhcGgvcGFsZS1i/bHVlLnBuZw",
            "original": "https://www.gstatic.com/devrel-devsite/prod/ve5ef9ac7b497e19ece9427facc78d0c59aaab7a2bc6a0f75fdae93f4ee589f67/developers/images/opengraph/pale-blue.png",
            "logo": false
          }
        },
        {
          "title": "Gesture recognition guide for Web | Google AI Edge | Google AI for Developers",
          "url": "https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer/web_js",
          "is_source_local": false,
          "is_source_both": false,
          "description": "// Create task for image file processing: const vision = await FilesetResolver.forVisionTasks( // path/to/wasm/root &quot;https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm &quot; ); const gestureRecognizer = await GestureRecognizer.createFromOptions(vision, { baseOptions: { modelAssetPath: &quot;https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/gesture_recognizer.task&quot; }, numHands: 2 });",
          "profile": {
            "name": "Google AI",
            "url": "https://ai.google.dev/edge/mediapipe/solutions/vision/gesture_recognizer/web_js",
            "long_name": "ai.google.dev",
            "img": "https://imgs.search.brave.com/m_vCaOleBRRqR4DoxKjK3UfaUq7NjGhFsQ6MmrZnZps/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYjBlMTk4YmQw/OWZhZDMyNDk0ZjI3/NzM5Njg2NTNmMGY0/ZTAzNTkxZTU3YjIz/YzgxNjM4YWMzZjk5/MWI2NTliZS9haS5n/b29nbGUuZGV2Lw"
          },
          "language": "en",
          "family_friendly": true,
          "type": "search_result",
          "subtype": "article",
          "is_live": false,
          "meta_url": {
            "scheme": "https",
            "netloc": "ai.google.dev",
            "hostname": "ai.google.dev",
            "favicon": "https://imgs.search.brave.com/m_vCaOleBRRqR4DoxKjK3UfaUq7NjGhFsQ6MmrZnZps/rs:fit:32:32:1:0/g:ce/aHR0cDovL2Zhdmlj/b25zLnNlYXJjaC5i/cmF2ZS5jb20vaWNv/bnMvYjBlMTk4YmQw/OWZhZDMyNDk0ZjI3/NzM5Njg2NTNmMGY0/ZTAzNTkxZTU3YjIz/YzgxNjM4YWMzZjk5/MWI2NTliZS9haS5n/b29nbGUuZGV2Lw",
            "path": "  \u203a google ai edge  \u203a gesture recognition guide for web"
          },
          "thumbnail": {
            "src": "https://imgs.search.brave.com/zhrzsc_qNH91JsJAdP7mQPSrMHVuaQhxr8LWjxB4_7Y/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9haS5n/b29nbGUuZGV2L3N0/YXRpYy9zaXRlLWFz/c2V0cy9pbWFnZXMv/c2hhcmUucG5n",
            "original": "https://ai.google.dev/static/site-assets/images/share.png",
            "logo": false
          }
        }
      ],
      "family_friendly": true
    }
  },
  "timestamp": "2026-01-09T21:30:26.558747"
}